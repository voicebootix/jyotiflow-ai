
#!/usr/bin/env python3
"""
Comprehensive Test Suite Generator for JyotiFlow AI Platform
Generates automated tests for critical spiritual services functionality ðŸ™
"""

import os
import json
import uuid
import asyncio
import asyncpg
import secrets
import string
import time
from datetime import datetime, timezone, timedelta
from typing import Dict, Any, Union
import logging


# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Custom Exception Classes
class DatabaseConnectionError(Exception):
Â    """Raised when database connection fails"""
Â    pass

class TestGenerationError(Exception):
Â    """Raised when test generation fails"""
Â    pass

DATABASE_URL = os.getenv("DATABASE_URL")
API_BASE_URL = os.getenv("API_BASE_URL", "https://jyotiflow-ai.onrender.com")

class TestStorageError(Exception):
Â    """Raised when test storage fails."""
Â    pass

# Try to import monitoring and service modules with error handling
try:
Â    from monitoring.dashboard import monitoring_dashboard
Â    MONITORING_AVAILABLE = True
except ImportError:
Â    MONITORING_AVAILABLE = False
Â    logger.debug("Monitoring dashboard not available")

try:
Â    from social_media_marketing_automation import SocialMediaMarketingEngine
Â    SOCIAL_MEDIA_ENGINE_AVAILABLE = True
except ImportError:
Â    SOCIAL_MEDIA_ENGINE_AVAILABLE = False
Â    logger.debug("Social media marketing engine not available")

try:
Â    from agora_service import AgoraService
Â    AGORA_SERVICE_AVAILABLE = True
except ImportError:
Â    AGORA_SERVICE_AVAILABLE = False
Â    logger.debug("Agora service not available")

try:
Â    from database_self_healing_system import DatabaseSelfHealingSystem
Â    HEALING_SYSTEM_AVAILABLE = True
except ImportError:
Â    HEALING_SYSTEM_AVAILABLE = False
Â    logger.debug("Database self-healing system not available")

try:
Â    from validators.social_media_validator import SocialMediaValidator
Â    SOCIAL_MEDIA_VALIDATOR_AVAILABLE = True
except ImportError:
Â    SOCIAL_MEDIA_VALIDATOR_AVAILABLE = False
Â    logger.debug("Social media validator not available")

try:
Â    from services.credit_package_service import CreditPackageService
Â    CREDIT_SERVICE_AVAILABLE = True
except ImportError:
Â    CREDIT_SERVICE_AVAILABLE = False
Â    logger.debug("Credit package service not available")

def generate_secure_test_password() -> str:
Â    """
Â    Generate a cryptographically secure random password for test purposes.
Â 
Â    Returns:
Â        A secure random password string
Â    """
Â    alphabet = string.ascii_letters + string.digits + "!@#$%^&*"
Â    return ''.join(secrets.choice(alphabet) for _ in range(16))

def generate_test_email() -> str:
Â    """
Â    Generate a unique test email address.
Â 
Â    Returns:
Â        A unique test email address
Â    """
Â    return f"test_{uuid.uuid4()}@example.com"

class TestSuiteGenerator:
Â    """
Â    Generates comprehensive test suites for the spiritual AI platform
Â    covering database operations, API endpoints, and business logic
Â    """
Â 
Â    def __init__(self):
Â        self.database_url = DATABASE_URL
Â        self.api_base_url = API_BASE_URL
Â        self.test_suites = {}
Â 
Â    def get_api_url(self, endpoint: str) -> str:
Â        """
Â        Get full API URL for testing endpoints.
Â 
Â        Args:
Â            endpoint: API endpoint path (e.g., '/api/test')
Â 
Â        Returns:
Â            Full URL for the endpoint
Â        """
Â        return f"{self.api_base_url}{endpoint}"
Â 
Â    async def generate_all_test_suites(self) -> Dict[str, Any]:
Â        """
Â        Generate all test suites for the platform.
Â 
Â        Returns:
Â            Dict containing all generated test suites with comprehensive coverage
Â 
Â        Raises:
Â            DatabaseConnectionError: If unable to connect to database
Â            TestGenerationError: If test suite generation fails
Â        """
Â        try:
Â            logger.info("Starting test suite generation")
Â 
Â            # Test database connection first
Â            try:
Â                if hasattr(self, 'database_url') and self.database_url:
Â                    conn = await asyncpg.connect(self.database_url)
Â                    await conn.execute("SELECT 1")
Â                    await conn.close()
Â                    logger.info("Database connection verified")
Â            except (asyncpg.PostgresConnectionError, asyncpg.PostgresError) as db_error:
Â                logger.error(f"Database connection failed: {db_error}")
Â                raise DatabaseConnectionError(f"Unable to connect to database: {db_error}") from db_error
Â            except Exception as conn_error:
Â                logger.error(f"Unexpected database connection error: {conn_error}")
Â                raise DatabaseConnectionError(f"Database connection error: {conn_error}") from conn_error
Â 
Â            # Generate all test suites with proper error handling
Â            test_suites = {}
Â 
Â            try:
Â                test_suites["database_tests"] = await self.generate_database_tests()
Â            except Exception as e:
Â                logger.warning(f"Database tests generation failed: {e}")
Â                test_suites["database_tests"] = {"error": str(e)}
Â 
Â            try:
Â                test_suites["api_tests"] = await self.generate_api_endpoint_tests()
Â            except Exception as e:
Â                logger.warning(f"API tests generation failed: {e}")
Â                test_suites["api_tests"] = {"error": str(e)}
Â 
Â            try:
Â                test_suites["spiritual_services_tests"] = await self.generate_spiritual_services_tests()
Â            except Exception as e:
Â                logger.warning(f"Spiritual services tests generation failed: {e}")
Â                test_suites["spiritual_services_tests"] = {"error": str(e)}
Â 
Â            try:
Â                test_suites["integration_tests"] = await self.generate_integration_tests()
Â            except Exception as e:
Â                logger.warning(f"Integration tests generation failed: {e}")
Â                test_suites["integration_tests"] = {"error": str(e)}
Â 
Â            try:
Â                test_suites["performance_tests"] = await self.generate_performance_tests()
Â            except Exception as e:
Â                logger.warning(f"Performance tests generation failed: {e}")
Â                test_suites["performance_tests"] = {"error": str(e)}
Â 
Â            try:
Â                test_suites["security_tests"] = await self.generate_security_tests()
Â            except Exception as e:
Â                logger.warning(f"Security tests generation failed: {e}")
Â                test_suites["security_tests"] = {"error": str(e)}
Â 
Â            try:
Â                test_suites["auto_healing_tests"] = await self.generate_auto_healing_tests()
Â            except Exception as e:
Â                logger.warning(f"Auto healing tests generation failed: {e}")
Â                test_suites["auto_healing_tests"] = {"error": str(e)}
Â 
Â            try:
Â                test_suites["live_audio_video_tests"] = await self.generate_live_audio_video_tests()
Â            except Exception as e:
Â                logger.warning(f"Live audio/video tests generation failed: {e}")
Â                test_suites["live_audio_video_tests"] = {"error": str(e)}
Â 
Â            try:
Â                test_suites["social_media_tests"] = await self.generate_social_media_tests()
Â            except Exception as e:
Â                logger.warning(f"Social media tests generation failed: {e}")
Â                test_suites["social_media_tests"] = {"error": str(e)}
Â 
Â            try:
Â                test_suites["avatar_generation_tests"] = await self.generate_avatar_generation_tests()
Â            except Exception as e:
Â                logger.warning(f"Avatar generation tests failed: {e}")
Â                test_suites["avatar_generation_tests"] = {"error": str(e)}
Â 
Â            try:
Â                test_suites["credit_payment_tests"] = await self.generate_credit_payment_tests()
Â            except Exception as e:
Â                logger.warning(f"Credit payment tests generation failed: {e}")
Â                test_suites["credit_payment_tests"] = {"error": str(e)}
Â 
Â            try:
Â                test_suites["user_management_tests"] = await self.generate_user_management_tests()
Â            except Exception as e:
Â                logger.warning(f"User management tests generation failed: {e}")
Â                test_suites["user_management_tests"] = {"error": str(e)}
Â 
Â            try:
Â                test_suites["admin_services_tests"] = await self.generate_admin_services_tests()
Â            except Exception as e:
Â                logger.warning(f"Admin services tests generation failed: {e}")
Â                test_suites["admin_services_tests"] = {"error": str(e)}
Â 
Â            try:
Â                test_suites["community_services_tests"] = await self.generate_community_services_tests()
Â            except Exception as e:
Â                logger.warning(f"Community services tests generation failed: {e}")
Â                test_suites["community_services_tests"] = {"error": str(e)}
Â 
Â            try:
Â                test_suites["notification_services_tests"] = await self.generate_notification_services_tests()
Â            except Exception as e:
Â                logger.warning(f"Notification services tests generation failed: {e}")
Â                test_suites["notification_services_tests"] = {"error": str(e)}
Â 
Â            try:
Â                test_suites["analytics_monitoring_tests"] = await self.generate_analytics_monitoring_tests()
Â            except Exception as e:
Â                logger.warning(f"Analytics monitoring tests generation failed: {e}")
Â                test_suites["analytics_monitoring_tests"] = {"error": str(e)}
Â 
Â            logger.info("Test suite generation completed")
Â            return test_suites
Â 
Â        except DatabaseConnectionError:
Â            raise  # Re-raise database connection errors
Â        except Exception as generation_error:
Â            logger.error(f"Failed to generate test suites: {generation_error}")
Â            raise TestGenerationError(f"Test suite generation failed: {generation_error}") from generation_error
Â 
Â    async def generate_database_tests(self) -> Dict[str, Any]:
Â        """Generate database-specific tests"""
Â        return {
Â            "test_suite_name": "Database Operations",
Â            "test_category": "database",
Â            "description": "Tests for core database operations and spiritual data integrity",
Â            "test_cases": [
Â                {
Â                    "test_name": "test_user_creation_and_credits",
Â                    "description": "Test user account creation with initial credits",
Â                    "test_type": "unit",
Â                    "priority": "critical",
Â                    "test_code": """
async def test_user_creation_and_credits():
Â    conn = await asyncpg.connect(DATABASE_URL)
Â    try:
Â        # Create test user
Â        user_id = str(uuid.uuid4())
Â        await conn.execute(
Â            "INSERT INTO users (id, email, credits) VALUES ($1, $2, $3)",
Â            user_id, f"test_{user_id}@example.com", 100
Â        )
Â 
Â        # Verify user exists with correct credits
Â        result = await conn.fetchrow(
Â            "SELECT credits FROM users WHERE id = $1", user_id
Â        )
Â        assert result is not None, "User should exist"
Â        assert result['credits'] == 100, "User should have 100 initial credits"
Â 
Â        # Cleanup
Â        await conn.execute("DELETE FROM users WHERE id = $1", user_id)
Â        return {"status": "passed", "message": "User creation test passed"}
Â    except Exception as e:
Â        return {"status": "failed", "error": str(e)}
Â    finally:
Â        await conn.close()
""",
Â                    "expected_result": "User created with 100 credits",
Â                    "timeout_seconds": 30
Â                },
Â                {
Â                    "test_name": "test_spiritual_session_creation",
Â                    "description": "Test spiritual guidance session creation and storage",
Â                    "test_type": "unit",
Â                    "priority": "critical",
Â                    "test_code": """
async def test_spiritual_session_creation():
Â    conn = await asyncpg.connect(DATABASE_URL)
Â    try:
Â        # Create test session
Â        session_id = str(uuid.uuid4())
Â        user_id = str(uuid.uuid4())
Â 
Â        # First create a user
Â        await conn.execute(
Â            "INSERT INTO users (id, email, credits) VALUES ($1, $2, $3)",
Â            user_id, f"test_{user_id}@example.com", 50
Â        )
Â 
Â        # Create spiritual session
Â        await conn.execute('''
Â            INSERT INTO sessions (id, user_id, service_type, status, created_at)
Â            VALUES ($1, $2, $3, $4, $5)
Â        ''', session_id, user_id, "spiritual_guidance", "active", datetime.now(timezone.utc))
Â 
Â        # Verify session exists
Â        result = await conn.fetchrow(
Â            "SELECT status, service_type FROM sessions WHERE id = $1", session_id
Â        )
Â        assert result is not None, "Session should exist"
Â        assert result['status'] == 'active', "Session should be active"
Â        assert result['service_type'] == 'spiritual_guidance', "Correct service type"
Â 
Â        # Cleanup
Â        await conn.execute("DELETE FROM sessions WHERE id = $1", session_id)
Â        await conn.execute("DELETE FROM users WHERE id = $1", user_id)
Â        return {"status": "passed", "message": "Spiritual session creation test passed"}
Â    except Exception as e:
Â        return {"status": "failed", "error": str(e)}
Â    finally:
Â        await conn.close()
""",
Â                    "expected_result": "Spiritual session created successfully",
Â                    "timeout_seconds": 30
Â                },
Â                {
Â                    "test_name": "test_credit_transaction_integrity",
Â                    "description": "Test credit deduction and transaction logging",
Â                    "test_type": "unit",
Â                    "priority": "critical",
Â                    "test_code": """
async def test_credit_transaction_integrity():
Â    conn = await asyncpg.connect(DATABASE_URL)
Â    try:
Â        user_id = str(uuid.uuid4())
Â 
Â        # Create user with 100 credits
Â        await conn.execute(
Â            "INSERT INTO users (id, email, credits) VALUES ($1, $2, $3)",
Â            user_id, f"test_{user_id}@example.com", 100
Â        )
Â 
Â        # Deduct 25 credits for service
Â        await conn.execute(
Â            "UPDATE users SET credits = credits - 25 WHERE id = $1", user_id
Â        )
Â 
Â        # Log transaction
Â        await conn.execute('''
Â            INSERT INTO credit_transactions (user_id, amount, transaction_type, description)
Â            VALUES ($1, $2, $3, $4)
Â        ''', user_id, -25, "deduction", "Spiritual guidance session")
Â 
Â        # Verify final credit balance
Â        credits = await conn.fetchval("SELECT credits FROM users WHERE id = $1", user_id)
Â        assert credits == 75, f"Expected 75 credits, got {credits}"
Â 
Â        # Verify transaction logged
Â        transaction = await conn.fetchrow(
Â            "SELECT amount, transaction_type FROM credit_transactions WHERE user_id = $1",
Â            user_id
Â        )
Â        assert transaction is not None, "Transaction should be logged"
Â        assert transaction['amount'] == -25, "Transaction amount should be -25"
Â 
Â        # Cleanup
Â        await conn.execute("DELETE FROM credit_transactions WHERE user_id = $1", user_id)
Â        await conn.execute("DELETE FROM users WHERE id = $1", user_id)
Â        return {"status": "passed", "message": "Credit transaction test passed"}
Â    except Exception as e:
Â        return {"status": "failed", "error": str(e)}
Â    finally:
Â        await conn.close()
""",
Â                    "expected_result": "Credits deducted and transaction logged correctly",
Â                    "timeout_seconds": 30
Â                }
Â            ]
Â        }
Â 
Â 
Â    async def generate_api_endpoint_tests(self) -> Dict[str, Any]:
Â        """Generate API endpoint tests"""
Â        return {
Â            "test_suite_name": "API Endpoints",
Â            "test_category": "api",
Â            "description": "Tests for all API endpoints and spiritual service interfaces",
Â            "test_cases": [
Â                {
Â                    "test_name": "test_health_check_endpoint",
Â                    "description": "Test health check endpoint availability",
Â                    "test_type": "integration",
Â                    "priority": "critical",
Â                    "test_code": """
import httpx
import os
import time

async def test_health_check_endpoint():
Â    try:
Â        base_url = os.getenv("API_BASE_URL", "https://jyotiflow-ai.onrender.com")
Â        endpoint_url = f"{base_url}/health"
Â 
Â        headers = {
Â            "X-Test-Run": "true",
Â            "X-Test-Type": "health-check",
Â            "User-Agent": "JyotiFlow-TestRunner/1.0"
Â        }
Â 
Â        start_time = time.time()
Â        async with httpx.AsyncClient(timeout=10.0) as client:
Â            response = await client.get(endpoint_url, headers=headers)
Â            execution_time = int((time.time() - start_time) * 1000)
Â 
Â            result = {
Â                "status": "passed" if response.status_code == 200 else "failed",
Â                "http_status_code": response.status_code,
Â                "execution_time_ms": execution_time,
Â                "endpoint_url": endpoint_url,
Â                "message": f"Health endpoint returned {response.status_code}"
Â            }
Â 
Â            if response.status_code == 200:
Â                try:
Â                    data = response.json()
Â                    result["message"] = f"Health check passed - API status: {data.get('status', 'unknown')}"
Â                except:
Â                    result["message"] = "Health endpoint returned 200 but invalid JSON"
Â 
Â            return result
Â    except Exception as e:
Â        return {"status": "failed", "error": str(e), "http_status_code": None}
""",
Â                    "expected_result": "Health endpoint returns 200 with healthy status",
Â                    "timeout_seconds": 10
Â                },
Â                {
Â                    "test_name": "test_user_registration_endpoint",
Â                    "description": "Test user registration API endpoint",
Â                    "test_type": "integration",
Â                    "priority": "critical",
Â                    "test_code": """
import httpx
import uuid
import secrets
import string
import os
import time

def generate_secure_test_password():
Â    alphabet = string.ascii_letters + string.digits + "!@#$%^&*"
Â    return ''.join(secrets.choice(alphabet) for _ in range(16))

def generate_test_email():
Â    return f"test_{uuid.uuid4()}@example.com"

async def test_user_registration_endpoint():
Â    try:
Â        base_url = os.getenv("API_BASE_URL", "https://jyotiflow-ai.onrender.com")
Â        endpoint_url = f"{base_url}/api/auth/register"
Â 
Â        test_email = generate_test_email()
Â        headers = {
Â            "X-Test-Run": "true",
Â            "X-Test-Type": "user-registration",
Â            "User-Agent": "JyotiFlow-TestRunner/1.0",
Â            "Content-Type": "application/json"
Â        }
Â 
Â        payload = {
Â            "email": test_email,
Â            "password": generate_secure_test_password(),
Â            "name": "JyotiFlow Test User"
Â        }
Â 
Â        start_time = time.time()
Â        async with httpx.AsyncClient(timeout=15.0) as client:
Â            response = await client.post(endpoint_url, json=payload, headers=headers)
Â            execution_time = int((time.time() - start_time) * 1000)
Â 
Â            result = {
Â                "status": "passed" if response.status_code in [200, 201] else "failed",
Â                "http_status_code": response.status_code,
Â                "execution_time_ms": execution_time,
Â                "endpoint_url": endpoint_url,
Â                "test_email": test_email,
Â                "message": f"Registration endpoint returned {response.status_code}"
Â            }
Â 
Â            if response.status_code in [200, 201]:
Â                try:
Â                    data = response.json()
Â                    result["message"] = f"User registration successful - Status: {data.get('status', 'unknown')}"
Â                except:
Â                    result["message"] = f"Registration returned {response.status_code} but invalid JSON"
Â 
Â            return result
Â    except Exception as e:
Â        return {"status": "failed", "error": str(e), "http_status_code": None}
""",
Â                    "expected_result": "User registration succeeds with user_id returned",
Â                    "timeout_seconds": 15
Â                },
Â                {
Â                    "test_name": "test_user_login_endpoint",
Â                    "description": "Test user login API endpoint",
Â                    "test_type": "integration",
Â                    "priority": "critical",
Â                    "test_code": """
import httpx
import uuid
import secrets
import string
import os
import time

def generate_secure_test_password():
Â    alphabet = string.ascii_letters + string.digits + "!@#$%^&*"
Â    return ''.join(secrets.choice(alphabet) for _ in range(16))

def generate_test_email():
Â    return f"test_{uuid.uuid4()}@example.com"

async def test_user_login_endpoint():
Â    try:
Â        base_url = os.getenv("API_BASE_URL", "https://jyotiflow-ai.onrender.com")
Â        register_url = f"{base_url}/api/auth/register"
Â        login_url = f"{base_url}/api/auth/login"
Â 
Â        test_email = generate_test_email()
Â        test_password = generate_secure_test_password()
Â 
Â        headers = {
Â            "X-Test-Run": "true",
Â            "X-Test-Type": "user-login",
Â            "User-Agent": "JyotiFlow-TestRunner/1.0",
Â            "Content-Type": "application/json"
Â        }
Â 
Â        start_time = time.time()
Â        async with httpx.AsyncClient(timeout=15.0) as client:
Â            # First register a test user
Â            reg_payload = {
Â                "email": test_email,
Â                "password": test_password,
Â                "name": "JyotiFlow Test User"
Â            }
Â            reg_response = await client.post(register_url, json=reg_payload, headers=headers)
Â 
Â            # Now test login
Â            login_payload = {
Â                "email": test_email,
Â                "password": test_password
Â            }
Â            response = await client.post(login_url, json=login_payload, headers=headers)
Â            execution_time = int((time.time() - start_time) * 1000)
Â 
Â            result = {
Â                "status": "passed" if response.status_code in [200, 201] else "failed",
Â                "http_status_code": response.status_code,
Â                "execution_time_ms": execution_time,
Â                "endpoint_url": login_url,
Â                "test_email": test_email,
Â                "registration_status": reg_response.status_code,
Â                "message": f"Login endpoint returned {response.status_code}"
Â            }
Â 
Â            if response.status_code in [200, 201]:
Â                try:
Â                    data = response.json()
Â                    result["message"] = f"User login successful - Status: {data.get('status', 'unknown')}"
Â                except:
Â                    result["message"] = f"Login returned {response.status_code} but invalid JSON"
Â            else:
Â                if reg_response.status_code not in [200, 201]:
Â                    result["error"] = f"Expected 200/201, got {response.status_code} (Registration failed with {reg_response.status_code})"
Â 
Â            return result
Â    except Exception as e:
Â        return {"status": "failed", "error": str(e), "http_status_code": None}
""",
Â                    "expected_result": "User login succeeds with access token returned",
Â                    "timeout_seconds": 15
Â                },
Â                {
Â                    "test_name": "test_spiritual_guidance_endpoint",
Â                    "description": "Test spiritual guidance API endpoint",
Â                    "test_type": "integration",
Â                    "priority": "high",
Â                    "test_code": """
import httpx
import os
import time

async def test_spiritual_guidance_endpoint():
Â    try:
Â        base_url = os.getenv("API_BASE_URL", "https://jyotiflow-ai.onrender.com")
Â        endpoint_url = f"{base_url}/api/spiritual/guidance"
Â 
Â        headers = {
Â            "X-Test-Run": "true",
Â            "X-Test-Type": "spiritual-guidance",
Â            "User-Agent": "JyotiFlow-TestRunner/1.0",
Â            "Content-Type": "application/json"
Â        }
Â 
Â        payload = {
Â            "question": "What is my spiritual purpose in life?",
Â            "birth_details": {
Â                "date": "1990-01-01",
Â                "time": "12:00",
Â                "location": "Test City"
Â            },
Â            "language": "en"
Â        }
Â 
Â        start_time = time.time()
Â        async with httpx.AsyncClient(timeout=15.0) as client:
Â            response = await client.post(endpoint_url, json=payload, headers=headers)
Â            execution_time = int((time.time() - start_time) * 1000)
Â 
Â            # Accept only legitimate statuses by default: 200 (success), 401 (auth required), 422 (validation)
Â            # Only allow 500 responses when explicitly enabled via environment flag
Â            valid_statuses = [200, 401, 422]
Â            allow_500_responses = os.getenv("ALLOW_500_RESPONSES", "false").lower() == "true"
Â            if allow_500_responses:
Â                valid_statuses.append(500)
Â 
Â            result = {
Â                "status": "passed" if response.status_code in valid_statuses else "failed",
Â                "http_status_code": response.status_code,
Â                "execution_time_ms": execution_time,
Â                "endpoint_url": endpoint_url,
Â                "message": f"Spiritual guidance endpoint returned {response.status_code}"
Â            }
Â 
Â            if response.status_code == 200:
Â                result["message"] = "Spiritual guidance endpoint accessible and working"
Â            elif response.status_code == 401:
Â                result["message"] = "Spiritual guidance endpoint accessible (authentication required as expected)"
Â            elif response.status_code == 422:
Â                result["message"] = "Spiritual guidance endpoint accessible (validation error as expected)"
Â            elif response.status_code == 500:
Â                # Mark 500 responses as failed unless explicitly allowed via environment flag
Â                if not allow_500_responses:
Â                    result["status"] = "failed"
Â                    result["message"] = "Spiritual guidance endpoint returned 500 Server Error"
Â                    # Include response details for debugging
Â                    try:
Â                        error_body = response.json()
Â                        result["details"] = {
Â                            "status_code": response.status_code,
Â                            "response_body": error_body
Â                        }
Â                    except:
Â                        result["details"] = {
Â                            "status_code": response.status_code,
Â                            "response_text": response.text if hasattr(response, 'text') else "Unable to parse response"
Â                        }
Â                else:
Â                    result["message"] = "Spiritual guidance endpoint returned 500 (allowed when ALLOW_500_RESPONSES=true)"
Â                    result["details"] = {"status_code": response.status_code, "note": "500 responses explicitly allowed"}
Â            else:
Â                result["error"] = f"Unexpected status code: {response.status_code}"
Â 
Â            return result
Â    except Exception as e:
Â        return {"status": "failed", "error": str(e), "http_status_code": None}
""",
Â                    "expected_result": "Spiritual guidance endpoint accessible (200, 401, or 422 expected; 500 allowed when ALLOW_500_RESPONSES=true)",
Â                    "timeout_seconds": 15
Â                }
Â            ]
Â        }
Â 
Â    async def generate_spiritual_services_tests(self) -> Dict[str, Any]:
Â        """Generate spiritual services HTTP endpoint tests - SPIRITUAL CORE CRITICAL - Environment-configurable base URL with direct endpoint configuration"""
Â        return {
Â            "test_suite_name": "Spiritual Services",
Â            "test_category": "spiritual_services",
Â            "description": "Critical tests for spiritual endpoints, avatar generation, pricing, knowledge domains, and birth chart caching - Database Driven",
Â            "test_cases": [
Â                {
Â 
Â                    "test_name": "test_business_logic_validator",
Â                    "description": "Test business logic validation endpoint with environment-configurable base URL",
Â                    "test_type": "integration",
Â                    "priority": "critical",
Â                    "test_code": """
import httpx
import os
import time

async def test_business_logic_validator():
Â    '''Test business logic validation endpoint - environment-configurable base URL, direct endpoint configuration'''
Â    try:
Â        # Direct endpoint configuration (not from database)
Â        endpoint = "/api/monitoring/business-logic-validate"
Â        method = "POST"
Â        business_function = "CORE"
Â        test_data = {
Â            "session_context": {
Â                "spiritual_question": "How can meditation help me find inner peace?",
Â                "birth_details": {"date": "1990-01-01", "time": "12:00", "location": "Mumbai, India"},
Â                "integration_results": {
Â                    "rag_knowledge": {
Â                        "passed": True,
Â                        "actual": {
Â                            "knowledge": "Meditation is a sacred practice that brings stillness to the mind and connects us with our inner divine nature."
Â                        }
Â                    },
Â                    "prokerala_data": {
Â                        "passed": True,
Â                        "actual": {
Â                            "planets": [
Â                                {"name": "Sun", "position": "Capricorn"},
Â                                {"name": "Moon", "position": "Virgo"}
Â                            ],
Â                            "nakshatra": {"name": "Uttara Ashadha", "lord": "Sun"}
Â                        }
Â                    },
Â                    "openai_guidance": {
Â                        "passed": True,
Â                        "actual": {
Â                            "response": "Dear blessed soul, meditation brings peace and spiritual awakening through regular practice."
Â                        }
Â                    }
Â                }
Â            }
Â        }
Â        api_base_url = os.getenv('API_BASE_URL', 'https://jyotiflow-ai.onrender.com')
Â        expected_codes = [200, 401, 403, 422]
Â 
Â        # Execute HTTP request to actual endpoint
Â        url = api_base_url.rstrip('/') + '/' + endpoint.lstrip('/')
Â 
Â        try:
Â            async with httpx.AsyncClient(timeout=30.0) as client:
Â                start_time = time.time()
Â 
Â                headers = {
Â                    "Content-Type": "application/json",
Â                    "X-Test-Run": "true",
Â                    "X-Test-Type": "business-logic-validator",
Â                    "User-Agent": "JyotiFlow-TestRunner/1.0"
Â                }
Â 
Â                response = await client.request(method, url, json=test_data, headers=headers)
Â 
Â                response_time_ms = int((time.time() - start_time) * 1000)
Â                status_code = response.status_code
Â                test_status = 'passed' if status_code in expected_codes else 'failed'
Â 
Â        except Exception as http_error:
Â            return {"status": "failed", "error": f"HTTP request failed: {str(http_error)}", "business_function": business_function}
Â 
Â        # Return test results (database storage handled by test execution engine)
Â        return {
Â            "status": test_status,
Â            "message": f"Business logic validator endpoint returned {status_code}",
Â            "business_function": business_function,
Â            "http_status_code": status_code,
Â            "execution_time_ms": response_time_ms,
Â            "details": {
Â                "status_code": status_code,
Â                "response_time_ms": response_time_ms,
Â                "url": url,
Â                "method": method,
Â                "endpoint": endpoint
Â            }
Â        }
Â    except Exception as e:
Â        return {"status": "failed", "error": f"Test failed: {str(e)}"}
""",
Â                    "expected_result": "Business logic validation endpoint operational",
Â                    "timeout_seconds": 30
Â                },
Â                {
Â                    "test_name": "test_spiritual_avatar_engine",
Â                    "description": "Test spiritual avatar generation endpoint with environment-configurable base URL",
Â                    "test_type": "integration",
Â                    "priority": "high",
Â                    "test_code": """
import httpx
import os
import time

async def test_spiritual_avatar_engine():
Â    '''Test spiritual avatar generation endpoint - environment-configurable base URL, direct endpoint configuration'''
Â    try:
Â        # Direct endpoint configuration (not from database)
Â        endpoint = "/api/avatar/generate"
Â        method = "POST"
Â        business_function = "AI Guidance"
Â        test_data = {
Â            "user_id": "test_user_spiritual",
Â            "content": "Test spiritual guidance for avatar generation",
Â            "spiritual_context": {
Â                "birth_details": {"date": "1990-01-01", "time": "12:00", "location": "Chennai, India"},
Â                "guidance_type": "meditation"
Â            }
Â        }
Â        api_base_url = os.getenv('API_BASE_URL', 'https://jyotiflow-ai.onrender.com')
Â        expected_codes = [200, 401, 403, 422]
Â 
Â        # Execute HTTP request to actual endpoint
Â        url = api_base_url.rstrip('/') + '/' + endpoint.lstrip('/')
Â 
Â        try:
Â            async with httpx.AsyncClient(timeout=30.0) as client:
Â                start_time = time.time()
Â 
Â                headers = {
Â                    "Content-Type": "application/json",
Â                    "X-Test-Run": "true",
Â                    "X-Test-Type": "spiritual-avatar",
Â                    "User-Agent": "JyotiFlow-TestRunner/1.0"
Â                }
Â 
Â                response = await client.request(method, url, json=test_data, headers=headers)
Â 
Â                response_time_ms = int((time.time() - start_time) * 1000)
Â                status_code = response.status_code
Â                test_status = 'passed' if status_code in expected_codes else 'failed'
Â 
Â        except Exception as http_error:
Â            return {"status": "failed", "error": f"HTTP request failed: {str(http_error)}", "business_function": business_function}
Â 
Â        # Return test results (database storage handled by test execution engine)
Â        return {
Â            "status": test_status,
Â            "message": f"Spiritual avatar generation endpoint returned {status_code}",
Â            "business_function": business_function,
Â            "http_status_code": status_code,
Â            "execution_time_ms": response_time_ms,
Â            "details": {
Â                "status_code": status_code,
Â                "response_time_ms": response_time_ms,
Â                "url": url,
Â                "method": method,
Â                "endpoint": endpoint
Â            }
Â        }
Â    except Exception as e:
Â        return {"status": "failed", "error": f"Test failed: {str(e)}"}
""",
Â                    "expected_result": "Spiritual avatar generation endpoint operational",
Â                    "timeout_seconds": 30
Â                },
Â                {
Â                    "test_name": "test_monetization_optimizer",
Â                    "description": "Test monetization optimizer smart pricing recommendations endpoint",
Â                    "test_type": "integration",
Â                    "priority": "medium",
Â                    "test_code": """
import httpx
import os
import time

async def test_monetization_optimizer():
Â    '''Test monetization optimizer pricing recommendations endpoint - environment-configurable base URL, direct endpoint configuration'''
Â    try:
Â        # Direct endpoint configuration (not from database)
Â        endpoint = "/api/spiritual/enhanced/pricing/smart-recommendations"
Â        method = "GET"
Â        business_function = "Revenue Logic"
Â        test_data = {"timeframe": "monthly", "service_type": "spiritual_guidance"}
Â        api_base_url = os.getenv('API_BASE_URL', 'https://jyotiflow-ai.onrender.com')
Â        expected_codes = [200, 401, 403, 422]
Â 
Â        # Execute HTTP request to actual endpoint
Â        url = api_base_url.rstrip('/') + '/' + endpoint.lstrip('/')
Â 
Â        try:
Â            async with httpx.AsyncClient(timeout=30.0) as client:
Â                start_time = time.time()
Â 
Â                headers = {
Â                    "Content-Type": "application/json",
Â                    "X-Test-Run": "true",
Â                    "X-Test-Type": "monetization-optimizer",
Â                    "User-Agent": "JyotiFlow-TestRunner/1.0"
Â                }
Â 
Â                if method == 'GET':
Â                    response = await client.get(url, params=test_data, headers=headers)
Â                else:
Â                    response = await client.request(method, url, json=test_data, headers=headers)
Â 
Â                response_time_ms = int((time.time() - start_time) * 1000)
Â                status_code = response.status_code
Â                test_status = 'passed' if status_code in expected_codes else 'failed'
Â 
Â        except Exception as http_error:
Â            return {"status": "failed", "error": f"HTTP request failed: {str(http_error)}", "business_function": business_function}
Â 
Â        # Return test results (database storage handled by test execution engine)
Â        return {
Â            "status": test_status,
Â            "message": f"Monetization optimizer endpoint returned {status_code}",
Â            "business_function": business_function,
Â            "http_status_code": status_code,
Â            "execution_time_ms": response_time_ms,
Â            "details": {
Â                "status_code": status_code,
Â                "response_time_ms": response_time_ms,
Â                "url": url,
Â                "method": method,
Â                "endpoint": endpoint
Â            }
Â        }
Â    except Exception as e:
Â        return {"status": "failed", "error": f"Test failed: {str(e)}"}
""",
Â                    "expected_result": "Monetization optimizer pricing recommendations endpoint operational",
Â                    "timeout_seconds": 25
Â                },
Â                {
Â                    "test_name": "test_rag_knowledge_retrieval",
Â                    "description": "Test RAG knowledge domains retrieval endpoint",
Â                    "test_type": "integration",
Â                    "priority": "high",
Â                    "test_code": """
import httpx
import os
import time

async def test_rag_knowledge_retrieval():
Â    '''Test RAG knowledge domains endpoint - environment-configurable base URL, direct endpoint configuration'''
Â    try:
Â        # Direct endpoint configuration (not from database)
Â        endpoint = "/api/spiritual/enhanced/knowledge-domains"
Â        method = "GET"
Â        business_function = "Knowledge Base"
Â        test_data = {"domain": "meditation", "language": "en"}
Â        api_base_url = os.getenv('API_BASE_URL', 'https://jyotiflow-ai.onrender.com')
Â        expected_codes = [200, 401, 403, 422]
Â 
Â        # Execute HTTP request to actual endpoint
Â        url = api_base_url.rstrip('/') + '/' + endpoint.lstrip('/')
Â 
Â        try:
Â            async with httpx.AsyncClient(timeout=30.0) as client:
Â                start_time = time.time()
Â 
Â                headers = {
Â                    "Content-Type": "application/json",
Â                    "X-Test-Run": "true",
Â                    "X-Test-Type": "rag-knowledge",
Â                    "User-Agent": "JyotiFlow-TestRunner/1.0"
Â                }
Â 
Â                response = await client.get(url, params=test_data, headers=headers)
Â 
Â                response_time_ms = int((time.time() - start_time) * 1000)
Â                status_code = response.status_code
Â                test_status = 'passed' if status_code in expected_codes else 'failed'
Â 
Â        except Exception as http_error:
Â            return {"status": "failed", "error": f"HTTP request failed: {str(http_error)}", "business_function": business_function}
Â 
Â        # Return test results (database storage handled by test execution engine)
Â        return {
Â            "status": test_status,
Â            "message": f"RAG knowledge domains endpoint returned {status_code}",
Â            "business_function": business_function,
Â            "http_status_code": status_code,
Â            "execution_time_ms": response_time_ms,
Â            "details": {
Â                "status_code": status_code,
Â                "response_time_ms": response_time_ms,
Â                "url": url,
Â                "method": method,
Â                "endpoint": endpoint
Â            }
Â        }
Â    except Exception as e:
Â        return {"status": "failed", "error": f"Test failed: {str(e)}"}
""",
Â                    "expected_result": "RAG knowledge domains endpoint operational",
Â                    "timeout_seconds": 20
Â                },
Â                {
Â                    "test_name": "test_birth_chart_cache",
Â                    "description": "Test birth chart cache status endpoint",
Â                    "test_type": "integration",
Â                    "priority": "medium",
Â                    "test_code": """
import httpx
import os
import time

async def test_birth_chart_cache():
Â    '''Test birth chart cache status endpoint - environment-configurable base URL, direct endpoint configuration'''
Â    try:
Â        # Direct endpoint configuration (not from database)
Â        endpoint = "/api/spiritual/birth-chart/cache-status"
Â        method = "GET"
Â        business_function = "Astrological Cache"
Â        test_data = {"cache_key": "test_cache", "user_id": "test_user"}
Â        api_base_url = os.getenv('API_BASE_URL', 'https://jyotiflow-ai.onrender.com')
Â        expected_codes = [200, 401, 403, 422]
Â 
Â        # Execute HTTP request to actual endpoint
Â        url = api_base_url.rstrip('/') + '/' + endpoint.lstrip('/')
Â 
Â        try:
Â            async with httpx.AsyncClient(timeout=30.0) as client:
Â                start_time = time.time()
Â 
Â                headers = {
Â                    "Content-Type": "application/json",
Â                    "X-Test-Run": "true",
Â                    "X-Test-Type": "birth-chart-cache",
Â                    "User-Agent": "JyotiFlow-TestRunner/1.0"
Â                }
Â 
Â                response = await client.get(url, params=test_data, headers=headers)
Â 
Â                response_time_ms = int((time.time() - start_time) * 1000)
Â                status_code = response.status_code
Â                test_status = 'passed' if status_code in expected_codes else 'failed'
Â 
Â        except Exception as http_error:
Â            return {"status": "failed", "error": f"HTTP request failed: {str(http_error)}", "business_function": business_function}
Â 
Â        # Return test results (database storage handled by test execution engine)
Â        return {
Â            "status": test_status,
Â            "message": f"Birth chart cache status endpoint returned {status_code}",
Â            "business_function": business_function,
Â            "http_status_code": status_code,
Â            "execution_time_ms": response_time_ms,
Â            "details": {
Â                "status_code": status_code,
Â                "response_time_ms": response_time_ms,
Â                "url": url,
Â                "method": method,
Â                "endpoint": endpoint
Â            }
Â        }
Â    except Exception as e:
Â        return {"status": "failed", "error": f"Test failed: {str(e)}"}
""",
Â                    "expected_result": "Birth chart cache status endpoint operational",
Â                    "timeout_seconds": 20
Â                }
Â            ]
Â        }
Â 
Â    async def generate_integration_tests(self) -> Dict[str, Any]:
Â        """Generate integration tests for system components"""
Â        return {
Â            "test_suite_name": "Integration Tests",
Â            "test_category": "integration",
Â            "description": "Tests for component integration and end-to-end workflows",
Â            "test_cases": [
Â                {
Â                    "test_name": "test_user_journey_spiritual_session",
Â                    "description": "Complete user journey from registration to spiritual session",
Â                    "test_type": "integration",
Â                    "priority": "critical",
Â                    "test_code": """
import httpx
import uuid

async def test_user_journey_spiritual_session():
Â    try:
Â        test_email = f"journey_{uuid.uuid4()}@example.com"
Â 
Â        async with httpx.AsyncClient() as client:
Â            # Step 1: User registration
Â            reg_response = await client.post(
Â                "https://jyotiflow-ai.onrender.com/api/register",
Â                json={"email": test_email, "password": generate_secure_test_password(), "name": "Journey Test"}
Â            )
Â 
Â            if reg_response.status_code not in [200, 201]:
Â                return {"status": "failed", "error": f"Registration failed: {reg_response.status_code}"}
Â 
Â            # Step 2: User login (if login endpoint exists)
Â            # This would get auth token for subsequent requests
Â 
Â            # Step 3: Check credit balance
Â            # Step 4: Start spiritual session
Â            # Step 5: Verify session creation
Â 
Â            return {"status": "passed", "message": "User journey integration working"}
Â    except Exception as e:
Â        return {"status": "failed", "error": str(e)}
""",
Â                    "expected_result": "Complete user journey from registration to spiritual session",
Â                    "timeout_seconds": 60
Â                },
Â                {
Â                    "test_name": "test_monitoring_system_integration",
Â                    "description": "Test monitoring system integration with self-healing",
Â                    "test_type": "integration",
Â                    "priority": "high",
Â                    "test_code": """
async def test_monitoring_system_integration():
Â    conn = await asyncpg.connect(DATABASE_URL)
Â    try:
Â        # Create test validation session
Â        session_id = str(uuid.uuid4())
Â        await conn.execute('''
Â            INSERT INTO validation_sessions (session_id, validation_type, status)
Â            VALUES ($1, $2, $3)
Â        ''', session_id, "integration_test", "running")
Â 
Â        # Verify session tracked
Â        result = await conn.fetchrow(
Â            "SELECT status FROM validation_sessions WHERE session_id = $1", session_id
Â        )
Â        assert result is not None, "Validation session should be tracked"
Â        assert result['status'] == 'running', "Session should be running"
Â 
Â        # Cleanup
Â        await conn.execute("DELETE FROM validation_sessions WHERE session_id = $1", session_id)
Â        return {"status": "passed", "message": "Monitoring system integration working"}
Â    except Exception as e:
Â        return {"status": "failed", "error": str(e)}
Â    finally:
Â        await conn.close()
""",
Â                    "expected_result": "Monitoring system tracks validation sessions correctly",
Â                    "timeout_seconds": 30
Â                }
Â            ]
Â        }
Â 
Â    async def generate_performance_tests(self) -> Dict[str, Any]:
Â        """Generate performance tests"""
Â        return {
Â            "test_suite_name": "Performance Tests",
Â            "test_category": "performance",
Â            "description": "Load and performance tests for spiritual services",
Â            "test_cases": [
Â                {
Â                    "test_name": "test_database_connection_pool",
Â                    "description": "Test database connection pool under load",
Â                    "test_type": "performance",
Â                    "priority": "medium",
Â                    "test_code": """
import asyncio
import time

async def test_database_connection_pool():
Â    start_time = time.time()
Â    tasks = []
Â 
Â    async def db_operation():
Â        conn = await asyncpg.connect(DATABASE_URL)
Â        try:
Â            await conn.fetchval("SELECT 1")
Â        finally:
Â            await conn.close()
Â 
Â    try:
Â        # Create 20 concurrent database operations
Â        for i in range(20):
Â            tasks.append(db_operation())
Â 
Â        await asyncio.gather(*tasks)
Â 
Â        execution_time = time.time() - start_time
Â        assert execution_time < 10, f"Operations took too long: {execution_time}s"
Â 
Â        return {
Â            "status": "passed",
Â            "message": f"20 concurrent DB operations completed in {execution_time:.2f}s"
Â        }
Â    except Exception as e:
Â        return {"status": "failed", "error": str(e)}
""",
Â                    "expected_result": "20 concurrent database operations complete within 10 seconds",
Â                    "timeout_seconds": 15
Â                }
Â            ]
Â        }
Â 
Â    async def generate_security_tests(self) -> Dict[str, Any]:
Â        """Generate security tests"""
Â        return {
Â            "test_suite_name": "Security Tests",
Â            "test_category": "security",
Â            "description": "Security validation for spiritual services platform",
Â            "test_cases": [
Â                {
Â                    "test_name": "test_sql_injection_protection",
Â                    "description": "Test SQL injection protection in user inputs",
Â                    "test_type": "security",
Â                    "priority": "critical",
Â                    "test_code": """
async def test_sql_injection_protection():
Â    conn = await asyncpg.connect(DATABASE_URL)
Â    try:
Â        # Test malicious input that could cause SQL injection
Â        malicious_input = "'; DROP TABLE users; --"
Â 
Â        # This should not cause any damage due to parameterized queries
Â        result = await conn.fetchval(
Â            "SELECT COUNT(*) FROM users WHERE email = $1", malicious_input
Â        )
Â 
Â        # Should return 0 (no user with that email) rather than causing error
Â        assert result == 0, "Malicious input should return 0 results"
Â 
Â        # Verify users table still exists
Â        table_check = await conn.fetchval('''
Â            SELECT EXISTS(
Â                SELECT 1 FROM information_schema.tables
Â                WHERE table_name = 'users'
Â            )
Â        ''')
Â        assert table_check is True, "Users table should still exist"
Â 
Â        return {"status": "passed", "message": "SQL injection protection working"}
Â    except Exception as e:
Â        return {"status": "failed", "error": str(e)}
Â    finally:
Â        await conn.close()
""",
Â                    "expected_result": "SQL injection attempts blocked by parameterized queries",
Â                    "timeout_seconds": 30
Â                }
Â            ]
Â        }
Â 
Â    async def generate_auto_healing_tests(self) -> Dict[str, Any]:
Â        """Generate auto-healing system tests"""
Â        return {
Â            "test_suite_name": "Auto-Healing System Tests",
Â            "test_category": "auto_healing",
Â            "description": "Tests for database self-healing and auto-fix capabilities",
Â            "test_cases": [
Â                {
Â                    "test_name": "test_missing_table_detection",
Â                    "description": "Test detection of missing critical tables",
Â                    "test_type": "unit",
Â                    "priority": "critical",
Â                    "test_code": """
from database_self_healing_system import DatabaseSelfHealingSystem

async def test_missing_table_detection():
Â    try:
Â        healing_system = DatabaseSelfHealingSystem()
Â 
Â        # Test with a SQL query referencing a missing table
Â        test_query = "SELECT * FROM non_existent_spiritual_table WHERE user_id = 123"
Â 
Â        # This should detect the missing table reference
Â        missing_tables = await healing_system.analyze_missing_tables([test_query])
Â 
Â        # The system should identify this as a potential issue
Â        # (but correctly filter out false positives)
Â 
Â        return {"status": "passed", "message": "Missing table detection working"}
Â    except Exception as e:
Â        return {"status": "failed", "error": str(e)}
""",
Â                    "expected_result": "Missing table detection identifies real issues without false positives",
Â                    "timeout_seconds": 30
Â                },
Â                {
Â                    "test_name": "test_false_positive_filtering",
Â                    "description": "Test that system tables and keywords are correctly filtered",
Â                    "test_type": "unit",
Â                    "priority": "critical",
Â                    "test_code": """
from database_self_healing_system import extract_table_from_query

async def test_false_positive_filtering():
Â    try:
Â        # Test queries that should NOT trigger false positives
Â        test_queries = [
Â            "SELECT * FROM information_schema.tables",
Â            "SELECT COUNT(*) as count FROM users",
Â            "SELECT id, name FROM users WHERE created_at > current_timestamp",
Â            "UPDATE users SET status = 'active' WHERE id = 123"
Â        ]
Â 
Â        false_positives = 0
Â        for query in test_queries:
Â            table_name = extract_table_from_query(query)
Â            # These should either return valid table names or None (filtered out)
Â            if table_name in ['information_schema', 'count', 'current_timestamp']:
Â                false_positives += 1
Â 
Â        assert false_positives == 0, f"Found {false_positives} false positives"
Â 
Â        return {"status": "passed", "message": "False positive filtering working correctly"}
Â    except Exception as e:
Â        return {"status": "failed", "error": str(e)}
""",
Â                    "expected_result": "System correctly filters out PostgreSQL system tables and keywords",
Â                    "timeout_seconds": 20
Â                }
Â            ]
Â        }
Â 
Â    async def generate_social_media_tests(self) -> Dict[str, Any]:
Â        """Generate comprehensive social media automation tests - BUSINESS CRITICAL"""
Â        return {
Â            "test_suite_name": "Social Media Marketing Automation",
Â            "test_category": "social_media_business_critical",
Â            "description": "Business-critical tests for social media automation, content generation, platform integrations, and revenue tracking",
Â            "test_cases": [
Â                {
Â                    "test_name": "test_social_media_marketing_engine_core",
Â                    "description": "Test SocialMediaMarketingEngine core business functions",
Â                    "test_type": "integration",
Â                    "priority": "critical",
Â                    "test_code": """
async def test_social_media_marketing_engine_core():
Â    try:
Â        # Import SocialMediaMarketingEngine
Â        if not SOCIAL_MEDIA_ENGINE_AVAILABLE:
Â            return {"status": "failed", "error": "SocialMediaMarketingEngine not available"}
Â 
Â        # Initialize engine
Â        engine = SocialMediaMarketingEngine()
Â 
Â        # Test basic functionality
Â        assert engine is not None, "Engine should initialize"
Â        assert hasattr(engine, 'platform_configs'), "Should have platform configurations"
Â        assert hasattr(engine, 'daily_post_schedule'), "Should have posting schedule"
Â 
Â        # Test platform coverage (business critical)
Â        expected_platforms = ['youtube', 'instagram', 'facebook', 'tiktok', 'twitter', 'linkedin']
Â        actual_platforms = list(engine.platform_configs.keys())
Â        platform_coverage = len([p for p in expected_platforms if any(str(p) in str(ap) for ap in actual_platforms)])
Â 
Â        # Test daily content plan generation (revenue critical)
Â        daily_plan = await engine.generate_daily_content_plan()
Â        assert daily_plan is not None, "Should generate daily content plan"
Â        assert len(daily_plan) > 0, "Daily plan should contain platforms"
Â 
Â        # Test automated posting capability
Â        posting_result = await engine.execute_automated_posting()
Â        assert posting_result is not None, "Should return posting result"
Â 
Â        return {
Â            "status": "passed",
Â            "message": "Social media marketing engine core functions working",
Â            "platform_coverage": platform_coverage,
Â            "expected_platforms": len(expected_platforms),
Â            "daily_plan_generated": bool(daily_plan),
Â            "automated_posting_available": bool(posting_result)
Â        }
Â 
Â    except Exception as e:
Â        return {"status": "failed", "error": f"Social media engine core test failed: {str(e)}"}
""",
Â                    "expected_result": "SocialMediaMarketingEngine core business functions operational",
Â                    "timeout_seconds": 45
Â                },
Â                {
Â                    "test_name": "test_platform_services_integration",
Â                    "description": "Test all social media platform service integrations",
Â                    "test_type": "integration",
Â                    "priority": "critical",
Â                    "test_code": """
async def test_platform_services_integration():
Â    try:
Â        # Test all platform services
Â        platforms = ['facebook', 'instagram', 'youtube', 'twitter', 'tiktok']
Â        service_results = {}
Â 
Â        for platform in platforms:
Â            try:
Â                # Import platform service
Â                module_name = f"{platform}_service"
Â                service = __import__(f"services.{module_name}", fromlist=[f"{platform}_service"])
Â 
Â                # Test service initialization
Â                service_class = getattr(service, f"{platform.capitalize()}Service")
Â                platform_service = service_class()
Â 
Â                service_results[platform] = {
Â                    "service_available": True,
Â                    "initialized": platform_service is not None,
Â                    "has_validate_method": hasattr(platform_service, 'validate_credentials'),
Â                    "has_post_method": hasattr(platform_service, 'post_content') or hasattr(platform_service, 'upload_video')
Â                }
Â 
Â            except Exception as platform_error:
Â                service_results[platform] = {
Â                    "service_available": False,
Â                    "error": str(platform_error)
Â                }
Â 
Â        # Calculate business impact
Â        available_services = sum(1 for result in service_results.values() if result.get("service_available", False))
Â        total_services = len(platforms)
Â        service_coverage = (available_services / total_services) * 100
Â 
Â        return {
Â            "status": "passed" if available_services > 0 else "failed",
Â            "message": f"Platform services integration tested",
Â            "service_coverage_percent": service_coverage,
Â            "available_services": available_services,
Â            "total_services": total_services,
Â            "platform_results": service_results
Â        }
Â 
Â    except Exception as e:
Â        return {"status": "failed", "error": f"Platform services integration test failed: {str(e)}"}
""",
Â                    "expected_result": "All social media platform services integrate correctly",
Â                    "timeout_seconds": 30
Â                },
Â                {
Â                    "test_name": "test_content_generation_pipeline",
Â                    "description": "Test AI content generation to avatar video pipeline (revenue critical)",
Â                    "test_type": "integration",
Â                    "priority": "critical",
Â                    "test_code": """
async def test_content_generation_pipeline():
Â    try:
Â        # Import required modules
Â        if not SOCIAL_MEDIA_ENGINE_AVAILABLE:
Â            return {"status": "failed", "error": "SocialMediaMarketingEngine not available"}
Â 
Â        engine = SocialMediaMarketingEngine()
Â 
Â        # Test content generation for business-critical content types
Â        content_types = ["daily_wisdom", "spiritual_quote", "satsang_promo"]
Â        platforms = ["youtube", "instagram", "facebook"]
Â 
Â        generation_results = {}
Â 
Â        for platform in platforms:
Â            platform_results = {}
Â 
Â            for content_type in content_types:
Â                try:
Â                    # Test AI content generation
Â                    content = await engine._generate_ai_content(
Â                        platform=f"{platform}",  # Convert to enum if needed
Â                        content_type=content_type
Â                    )
Â 
Â                    platform_results[content_type] = {
Â                        "content_generated": content is not None,
Â                        "has_title": bool(content.get("title", "") if content else False),
Â                        "has_description": bool(content.get("description", "") if content else False),
Â                        "has_hashtags": bool(content.get("hashtags", []) if content else False),
Â                        "revenue_potential": content_type in ["satsang_promo", "daily_wisdom"]
Â                    }
Â 
Â                except Exception as content_error:
Â                    platform_results[content_type] = {
Â                        "content_generated": False,
Â                        "error": str(content_error)
Â                    }
Â 
Â            generation_results[platform] = platform_results
Â 
Â        # Test avatar video generation (business critical for engagement)
Â        try:
Â            # Test media content generation capability
Â            test_post_data = {
Â                "content_type": "daily_wisdom",
Â                "base_content": "Test spiritual guidance for social media automation"
Â            }
Â 
Â            media_result = await engine._generate_media_content(test_post_data)
Â            avatar_generation_available = media_result is not None
Â 
Â        except Exception as avatar_error:
Â            avatar_generation_available = False
Â            avatar_error_msg = str(avatar_error)
Â 
Â        # Calculate business impact metrics
Â        successful_generations = sum(
Â            1 for platform_results in generation_results.values()
Â            for content_result in platform_results.values()
Â            if content_result.get("content_generated", False)
Â        )
Â 
Â        total_attempts = len(platforms) * len(content_types)
Â        generation_success_rate = (successful_generations / total_attempts) * 100 if total_attempts > 0 else 0
Â 
Â        return {
Â            "status": "passed" if generation_success_rate > 50 else "failed",
Â            "message": "Content generation pipeline tested",
Â            "generation_success_rate": generation_success_rate,
Â            "successful_generations": successful_generations,
Â            "total_attempts": total_attempts,
Â            "avatar_generation_available": avatar_generation_available,
Â            "platform_results": generation_results
Â        }
Â 
Â    except Exception as e:
Â        return {"status": "failed", "error": f"Content generation pipeline test failed: {str(e)}"}
""",
Â                    "expected_result": "AI content generation and avatar video pipeline functional",
Â                    "timeout_seconds": 60
Â                },
Â                {
Â                    "test_name": "test_social_media_api_endpoints",
Â                    "description": "Test social media marketing API endpoints (business operations)",
Â                    "test_type": "integration",
Â                    "priority": "high",
Â                    "test_code": """
import httpx

async def test_social_media_api_endpoints():
Â    try:
Â        # Test critical business endpoints
Â        endpoints_to_test = [
Â            {"url": "/api/admin/social-marketing/overview", "method": "GET", "business_function": "Performance Analytics"},
Â            {"url": "/api/monitoring/social-media-status", "method": "GET", "business_function": "System Health"},
Â            {"url": "/api/monitoring/social-media-campaigns", "method": "GET", "business_function": "Campaign Management"},
Â            {"url": "/api/monitoring/social-media-test", "method": "POST", "business_function": "Automation Testing"}
Â        ]
Â 
Â        endpoint_results = {}
Â 
Â        async with httpx.AsyncClient() as client:
Â            for endpoint in endpoints_to_test:
Â                try:
Â                    url = f"https://jyotiflow-ai.onrender.com{endpoint['url']}"
Â 
Â                    if endpoint['method'] == 'GET':
Â                        response = await client.get(url)
Â                    else:
Â                        response = await client.post(url, json={})
Â 
Â                    # Business-critical endpoints should be accessible (even if auth required)
Â                    endpoint_results[endpoint['business_function']] = {
Â                        "endpoint_accessible": response.status_code in [200, 401, 403, 422],
Â                        "status_code": response.status_code,
Â                        "business_impact": "HIGH" if endpoint['business_function'] in ["Performance Analytics", "Campaign Management"] else "MEDIUM"
Â                    }
Â 
Â                except Exception as endpoint_error:
Â                    endpoint_results[endpoint['business_function']] = {
Â                        "endpoint_accessible": False,
Â                        "error": str(endpoint_error),
Â                        "business_impact": "HIGH"
Â                    }
Â 
Â        # Calculate business continuity score
Â        accessible_endpoints = sum(1 for result in endpoint_results.values() if result.get("endpoint_accessible", False))
Â        total_endpoints = len(endpoints_to_test)
Â        business_continuity_score = (accessible_endpoints / total_endpoints) * 100
Â 
Â        return {
Â            "status": "passed" if business_continuity_score > 75 else "failed",
Â            "message": "Social media API endpoints tested",
Â            "business_continuity_score": business_continuity_score,
Â            "accessible_endpoints": accessible_endpoints,
Â            "total_endpoints": total_endpoints,
Â            "endpoint_results": endpoint_results
Â        }
Â 
Â    except Exception as e:
Â        return {"status": "failed", "error": f"Social media API endpoints test failed: {str(e)}"}
""",
Â                    "expected_result": "Social media marketing API endpoints operational for business functions",
Â                    "timeout_seconds": 25
Â                },
Â                {
Â                    "test_name": "test_social_media_database_schema",
Â                    "description": "Test social media database tables and business data integrity",
Â                    "test_type": "unit",
Â                    "priority": "high",
Â                    "test_code": """
async def test_social_media_database_schema():
Â    conn = await asyncpg.connect(DATABASE_URL)
Â    try:
Â        # Test business-critical social media tables
Â        business_critical_tables = [
Â            'social_campaigns',
Â            'social_posts',
Â            'social_media_validation_log'
Â        ]
Â 
Â        table_validation_results = {}
Â 
Â        for table in business_critical_tables:
Â            try:
Â                # Check table exists
Â                table_exists = await conn.fetchrow('''
Â                    SELECT table_name FROM information_schema.tables
Â                    WHERE table_name = $1 AND table_schema = 'public'
Â                ''', table)
Â 
Â                if table_exists:
Â                    # Test table structure for business requirements
Â                    columns = await conn.fetch('''
Â                        SELECT column_name, data_type, is_nullable
Â                        FROM information_schema.columns
Â                        WHERE table_name = $1 AND table_schema = 'public'
Â                        ORDER BY ordinal_position
Â                    ''', table)
Â 
Â                    column_names = [col['column_name'] for col in columns]
Â 
Â                    # Validate business-critical columns exist
Â                    required_columns = {
Â                        'social_campaigns': ['id', 'name', 'platform', 'status', 'budget', 'created_at'],
Â                        'social_posts': ['id', 'campaign_id', 'platform', 'content', 'engagement_metrics', 'created_at'],
Â                        'social_media_validation_log': ['id', 'platform', 'validation_result', 'created_at']
Â                    }
Â 
Â                    missing_columns = [col for col in required_columns.get(table, []) if col not in column_names]
Â 
Â                    table_validation_results[table] = {
Â                        "exists": True,
Â                        "column_count": len(column_names),
Â                        "has_required_columns": len(missing_columns) == 0,
Â                        "missing_columns": missing_columns,
Â                        "business_ready": len(missing_columns) == 0
Â                    }
Â 
Â                    # Test business operations on table
Â                    if table == 'social_campaigns':
Â                        # Test campaign creation (business critical)
Â                        campaign_id = str(uuid.uuid4())
Â                        await conn.execute('''
Â                            INSERT INTO social_campaigns (id, name, platform, status, budget, created_at)
Â                            VALUES ($1, $2, $3, $4, $5, $6)
Â                        ''', campaign_id, "Test Business Campaign", "instagram", "active", 100.00, datetime.now(timezone.utc))
Â 
Â                        # Verify business data integrity
Â                        campaign = await conn.fetchrow(
Â                            "SELECT name, platform, budget FROM social_campaigns WHERE id = $1", campaign_id
Â                        )
Â 
Â                        assert campaign is not None, "Campaign should be created"
Â                        assert campaign['budget'] == 100.00, "Budget should be stored correctly"
Â 
Â                        # Cleanup
Â                        await conn.execute("DELETE FROM social_campaigns WHERE id = $1", campaign_id)
Â 
Â                        table_validation_results[table]["business_operations_tested"] = True
Â 
Â                else:
Â                    table_validation_results[table] = {
Â                        "exists": False,
Â                        "business_ready": False,
Â                        "business_impact": "HIGH - Revenue tracking disabled"
Â                    }
Â 
Â            except Exception as table_error:
Â                table_validation_results[table] = {
Â                    "exists": False,
Â                    "error": str(table_error),
Â                    "business_impact": "HIGH - Data operations failed"
Â                }
Â 
Â        # Calculate business readiness score
Â        business_ready_tables = sum(1 for result in table_validation_results.values() if result.get("business_ready", False))
Â        total_tables = len(business_critical_tables)
Â        business_readiness_score = (business_ready_tables / total_tables) * 100
Â 
Â        return {
Â            "status": "passed" if business_readiness_score > 80 else "failed",
Â            "message": "Social media database schema validated",
Â            "business_readiness_score": business_readiness_score,
Â            "business_ready_tables": business_ready_tables,
Â            "total_tables": total_tables,
Â            "table_results": table_validation_results
Â        }
Â 
Â    except Exception as e:
Â        return {"status": "failed", "error": f"Social media database schema test failed: {str(e)}"}
Â    finally:
Â        await conn.close()
""",
Â                    "expected_result": "Social media database schema supports all business operations",
Â                    "timeout_seconds": 30
Â                },
Â                {
Â                    "test_name": "test_social_media_validator_business_logic",
Â                    "description": "Test SocialMediaValidator for business compliance and content quality",
Â                    "test_type": "integration",
Â                    "priority": "high",
Â                    "test_code": """
async def test_social_media_validator_business_logic():
Â    try:
Â        # Import SocialMediaValidator
Â        if not SOCIAL_MEDIA_VALIDATOR_AVAILABLE:
Â            return {"status": "failed", "error": "SocialMediaValidator not available"}
Â 
Â        validator = SocialMediaValidator()
Â 
Â        # Test business-critical validation scenarios
Â        validation_scenarios = [
Â            {
Â                "scenario": "Successful Campaign Post",
Â                "input_data": {
Â                    "platform": "facebook",
Â                    "content": "Join our sacred Satsang session this Sunday at 7 PM. Experience divine wisdom and inner peace with Swami Jyotirananthan.",
Â                    "hashtags": ["#Satsang", "#SpiritualWisdom", "#InnerPeace"],
Â                    "campaign_type": "satsang_promo"
Â                },
Â                "output_data": {
Â                    "post_id": "fb_12345",
Â                    "status": "posted",
Â                    "engagement": {"likes": 25, "comments": 8, "shares": 12}
Â                },
Â                "business_impact": "HIGH"
Â            },
Â            {
Â                "scenario": "Daily Wisdom Content",
Â                "input_data": {
Â                    "platform": "instagram",
Â                    "content": "Today's wisdom: The path to enlightenment begins with inner silence. Listen to your soul's whispers.",
Â                    "hashtags": ["#DailyWisdom", "#Spirituality", "#Meditation"],
Â                    "campaign_type": "daily_wisdom"
Â                },
Â                "output_data": {
Â                    "post_id": "ig_67890",
Â                    "status": "posted",
Â                    "engagement": {"likes": 156, "comments": 23, "shares": 45}
Â                },
Â                "business_impact": "MEDIUM"
Â            },
Â            {
Â                "scenario": "Failed Post",
Â                "input_data": {
Â                    "platform": "twitter",
Â                    "content": "Spiritual guidance post",
Â                    "hashtags": ["#Wisdom"]
Â                },
Â                "output_data": {
Â                    "post_id": None,
Â                    "status": "failed",
Â                    "error": "API rate limit exceeded"
Â                },
Â                "business_impact": "HIGH"
Â            }
Â        ]
Â 
Â        validation_results = {}
Â 
Â        for scenario in validation_scenarios:
Â            try:
Â                result = await validator.validate(
Â                    scenario["input_data"],
Â                    scenario["output_data"],
Â                    {"business_context": scenario["business_impact"]}
Â                )
Â 
Â                validation_results[scenario["scenario"]] = {
Â                    "validation_completed": result is not None,
Â                    "validation_passed": result.get("passed", False) if result else False,
Â                    "business_impact": scenario["business_impact"],
Â                    "has_error_detection": "errors" in result if result else False,
Â                    "auto_fixable": result.get("auto_fixable", False) if result else False
Â                }
Â 
Â            except Exception as scenario_error:
Â                validation_results[scenario["scenario"]] = {
Â                    "validation_completed": False,
Â                    "error": str(scenario_error),
Â                    "business_impact": scenario["business_impact"]
Â                }
Â 
Â        # Calculate business protection score
Â        successful_validations = sum(1 for result in validation_results.values() if result.get("validation_completed", False))
Â        total_scenarios = len(validation_scenarios)
Â        business_protection_score = (successful_validations / total_scenarios) * 100
Â 
Â        return {
Â            "status": "passed" if business_protection_score > 70 else "failed",
Â            "message": "Social media validator business logic tested",
Â            "business_protection_score": business_protection_score,
Â            "successful_validations": successful_validations,
Â            "total_scenarios": total_scenarios,
Â            "scenario_results": validation_results
Â        }
Â 
Â    except Exception as e:
Â        return {"status": "failed", "error": f"Social media validator business logic test failed: {str(e)}"}
""",
Â                    "expected_result": "SocialMediaValidator protects business operations and ensures content quality",
Â                    "timeout_seconds": 35
Â                },
Â                {
Â                    "test_name": "test_social_media_automation_health",
Â                    "description": "Test overall social media automation system health (business continuity)",
Â                    "test_type": "integration",
Â                    "priority": "critical",
Â                    "test_code": """
async def test_social_media_automation_health():
Â    try:
Â        # Test comprehensive automation health
Â        health_components = {}
Â 
Â        # Component 1: SocialMediaMarketingEngine availability
Â        try:
Â            # Import social media engine using importlib (safer for exec context)
Â            import importlib.util
Â            import os
Â 
Â            # Try to import from current backend directory
Â            spec = importlib.util.find_spec('social_media_marketing_automation')
Â            if spec:
Â                social_module = importlib.util.module_from_spec(spec)
Â                spec.loader.exec_module(social_module)
Â                SocialMediaMarketingEngine = social_module.SocialMediaMarketingEngine
Â            else:
Â                # Fallback: direct import
Â                from social_media_marketing_automation import SocialMediaMarketingEngine
Â 
Â            engine = SocialMediaMarketingEngine()
Â            health_components["marketing_engine"] = {
Â                "available": True,
Â                "business_function": "Content Generation & Automation",
Â                "criticality": "CRITICAL"
Â            }
Â        except Exception as engine_error:
Â            health_components["marketing_engine"] = {
Â                "available": False,
Â                "error": str(engine_error),
Â                "business_function": "Content Generation & Automation",
Â                "criticality": "CRITICAL"
Â            }
Â 
Â        # Component 2: SocialMediaValidator availability
Â        try:
Â            # Import validator using importlib (safer for exec context)
Â            import importlib.util
Â 
Â            # Try to import the validator
Â            spec = importlib.util.find_spec('validators.social_media_validator')
Â            if spec:
Â                validator_module = importlib.util.module_from_spec(spec)
Â                spec.loader.exec_module(validator_module)
Â                SocialMediaValidator = validator_module.SocialMediaValidator
Â            else:
Â                # Fallback: direct import
Â                from validators.social_media_validator import SocialMediaValidator
Â 
Â            validator = SocialMediaValidator()
Â            health_components["validator"] = {
Â                "available": True,
Â                "business_function": "Content Quality Assurance",
Â                "criticality": "HIGH"
Â            }
Â        except Exception as validator_error:
Â            health_components["validator"] = {
Â                "available": False,
Â                "error": str(validator_error),
Â                "business_function": "Content Quality Assurance",
Â                "criticality": "HIGH"
Â            }
Â 
Â        # Component 3: Platform Services availability
Â        platforms = ['facebook', 'instagram', 'youtube', 'twitter', 'tiktok']
Â        available_platforms = 0
Â 
Â        for platform in platforms:
Â            try:
Â                # Use importlib for safer and more consistent module importing
Â                import importlib.util
Â                import importlib
Â 
Â                module_name = f"services.{platform}_service"
Â 
Â                # Check if module exists first
Â                spec = importlib.util.find_spec(module_name)
Â                if spec is not None:
Â                    # Import the module safely
Â                    service = importlib.import_module(module_name)
Â                    available_platforms += 1
Â                else:
Â                    logger.debug(f"Platform service {platform} module not found")
Â 
Â            except Exception as import_error:
Â                # Catch all exceptions to prevent single faulty module from crashing entire test
Â                logger.debug(f"Platform service {platform} not available: {import_error}")
Â                logger.debug(f"Error type: {type(import_error).__name__}")
Â 
Â        health_components["platform_services"] = {
Â            "available": available_platforms > 0,
Â            "available_count": available_platforms,
Â            "total_count": len(platforms),
Â            "coverage_percent": (available_platforms / len(platforms)) * 100,
Â            "business_function": "Social Media Posting",
Â            "criticality": "CRITICAL"
Â        }
Â 
Â        # Component 4: API Endpoints health
Â        try:
Â            import httpx
Â            async with httpx.AsyncClient() as client:
Â                response = await client.get("https://jyotiflow-ai.onrender.com/api/monitoring/social-media-status")
Â                api_healthy = response.status_code in [200, 401, 403]
Â        except:
Â            api_healthy = False
Â 
Â        health_components["api_endpoints"] = {
Â            "available": api_healthy,
Â            "business_function": "Dashboard & Campaign Management",
Â            "criticality": "HIGH"
Â        }
Â 
Â        # Calculate overall business continuity score
Â        critical_components = [comp for comp in health_components.values() if comp.get("criticality") == "CRITICAL"]
Â        critical_available = sum(1 for comp in critical_components if comp.get("available", False))
Â 
Â        high_components = [comp for comp in health_components.values() if comp.get("criticality") == "HIGH"]
Â        high_available = sum(1 for comp in high_components if comp.get("available", False))
Â 
Â        # Business continuity formula: Critical components weight 70%, High components 30%
Â        total_critical = len(critical_components)
Â        total_high = len(high_components)
Â 
Â        if total_critical > 0 and total_high > 0:
Â            business_continuity_score = (
Â                (critical_available / total_critical) * 0.7 +
Â                (high_available / total_high) * 0.3
Â            ) * 100
Â        else:
Â            business_continuity_score = 0
Â 
Â        # Determine business status
Â        if business_continuity_score >= 90:
Â            business_status = "OPTIMAL"
Â        elif business_continuity_score >= 70:
Â            business_status = "OPERATIONAL"
Â        elif business_continuity_score >= 50:
Â            business_status = "DEGRADED"
Â        else:
Â            business_status = "CRITICAL"
Â 
Â        return {
Â            "status": "passed" if business_continuity_score > 60 else "failed",
Â            "message": "Social media automation system health checked",
Â            "business_continuity_score": business_continuity_score,
Â            "business_status": business_status,
Â            "critical_components_available": critical_available,
Â            "total_critical_components": total_critical,
Â            "high_components_available": high_available,
Â            "total_high_components": total_high,
Â            "component_health": health_components
Â        }
Â 
Â    except Exception as e:
Â        return {"status": "failed", "error": f"Social media automation health test failed: {str(e)}"}
""",
Â                    "expected_result": "Social media automation system maintains business continuity",
Â                    "timeout_seconds": 40
Â                }
Â            ]
Â        }
Â 
Â    async def generate_live_audio_video_tests(self) -> Dict[str, Any]:
Â        """Generate comprehensive live chat, audio, and video functionality tests - BUSINESS CRITICAL"""
Â        return {
Â            "test_suite_name": "Live Audio Video Chat System",
Â            "test_category": "live_audio_video_business_critical",
Â            "description": "Business-critical tests for live chat, audio consultation, video calls, WebRTC integration, and Agora service",
Â            "test_cases": [
Â                {
Â                    "test_name": "test_agora_service_integration",
Â                    "description": "Test Agora service for video/audio session management (revenue critical)",
Â                    "test_type": "integration",
Â                    "priority": "critical",
Â                    "test_code": """
async def test_agora_service_integration():
Â    try:
Â        # Import Agora service
Â        if not AGORA_SERVICE_AVAILABLE:
Â            return {"status": "failed", "error": "AgoraService not available"}
Â 
Â        agora_service = AgoraService()
Â 
Â        # Test Agora service initialization
Â        assert agora_service is not None, "Agora service should initialize"
Â        assert hasattr(agora_service, 'app_id'), "Should have Agora app ID configured"
Â        assert hasattr(agora_service, 'app_certificate'), "Should have Agora certificate configured"
Â 
Â        # Test token generation (business critical for video calls)
Â        test_channel = f"test_channel_{uuid.uuid4()}"
Â        test_user_id = str(uuid.uuid4())
Â 
Â        token_result = await agora_service.generate_token(
Â            channel_name=test_channel,
Â            uid=test_user_id,
Â            role="publisher"
Â        )
Â 
Â        assert token_result is not None, "Should generate Agora token"
Â        assert token_result.get("token"), "Token should be present"
Â        assert token_result.get("channel") == test_channel, "Channel should match"
Â 
Â        # Test session creation (revenue critical)
Â        session_data = {
Â            "user_id": test_user_id,
Â            "service_type": "video_consultation",
Â            "duration_minutes": 30,
Â            "cost": 50.00
Â        }
Â 
Â        session_result = await agora_service.create_session(session_data)
Â        assert session_result is not None, "Should create video session"
Â        session_id = session_result.get("session_id")
Â        assert session_id, "Session ID should be generated"
Â 
Â        # Test session status check
Â        status_result = await agora_service.get_session_status(session_id)
Â        assert status_result is not None, "Should get session status"
Â        assert status_result.get("status") in ["active", "waiting", "ended"], "Status should be valid"
Â 
Â        # Test session cleanup
Â        cleanup_result = await agora_service.end_session(session_id)
Â        assert cleanup_result is not None, "Should end session successfully"
Â 
Â        return {
Â            "status": "passed",
Â            "message": "Agora service integration working correctly",
Â            "token_generated": bool(token_result.get("token")),
Â            "session_created": bool(session_id),
Â            "session_managed": bool(status_result and cleanup_result),
Â            "business_ready": True
Â        }
Â 
Â    except Exception as e:
Â        return {"status": "failed", "error": f"Agora service integration test failed: {str(e)}"}
""",
Â                    "expected_result": "Agora service fully functional for video/audio sessions",
Â                    "timeout_seconds": 45
Â                },
Â                {
Â                    "test_name": "test_live_chat_api_endpoints",
Â                    "description": "Test live chat API endpoints for session management (business operations)",
Â                    "test_type": "integration",
Â                    "priority": "critical",
Â                    "test_code": """
import httpx

async def test_live_chat_api_endpoints():
Â    try:
Â        # Test business-critical live chat endpoints
Â        endpoints_to_test = [
Â            {"url": "/api/livechat/initiate", "method": "POST", "business_function": "Session Initiation", "test_data": {"service_type": "video_consultation", "duration": 30}},
Â            {"url": "/api/livechat/status/test_session", "method": "GET", "business_function": "Session Status", "test_data": None},
Â            {"url": "/api/livechat/user-sessions", "method": "GET", "business_function": "User History", "test_data": None},
Â            {"url": "/api/admin/agora/overview", "method": "GET", "business_function": "Admin Analytics", "test_data": None}
Â        ]
Â 
Â        endpoint_results = {}
Â 
Â        async with httpx.AsyncClient() as client:
Â            for endpoint in endpoints_to_test:
Â                try:
Â                    url = f"https://jyotiflow-ai.onrender.com{endpoint['url']}"
Â 
Â                    if endpoint['method'] == 'GET':
Â                        response = await client.get(url)
Â                    else:
Â                        response = await client.post(url, json=endpoint['test_data'] or {})
Â 
Â                    # Business-critical endpoints should be accessible (even if auth required)
Â                    endpoint_results[endpoint['business_function']] = {
Â                        "endpoint_accessible": response.status_code in [200, 401, 403, 422],
Â                        "status_code": response.status_code,
Â                        "business_impact": "HIGH" if endpoint['business_function'] in ["Session Initiation", "Session Status"] else "MEDIUM",
Â                        "revenue_critical": endpoint['business_function'] == "Session Initiation"
Â                    }
Â 
Â                except Exception as endpoint_error:
Â                    endpoint_results[endpoint['business_function']] = {
Â                        "endpoint_accessible": False,
Â                        "error": str(endpoint_error),
Â                        "business_impact": "HIGH"
Â                    }
Â 
Â        # Calculate business continuity score
Â        accessible_endpoints = sum(1 for result in endpoint_results.values() if result.get("endpoint_accessible", False))
Â        total_endpoints = len(endpoints_to_test)
Â        business_continuity_score = (accessible_endpoints / total_endpoints) * 100
Â 
Â        # Check revenue-critical endpoints specifically
Â        revenue_critical_working = sum(1 for result in endpoint_results.values()
Â                                     if result.get("revenue_critical", False) and result.get("endpoint_accessible", False))
Â 
Â        return {
Â            "status": "passed" if business_continuity_score > 75 else "failed",
Â            "message": "Live chat API endpoints tested",
Â            "business_continuity_score": business_continuity_score,
Â            "accessible_endpoints": accessible_endpoints,
Â            "total_endpoints": total_endpoints,
Â            "revenue_critical_working": revenue_critical_working,
Â            "endpoint_results": endpoint_results
Â        }
Â 
Â    except Exception as e:
Â        return {"status": "failed", "error": f"Live chat API endpoints test failed: {str(e)}"}
""",
Â                    "expected_result": "Live chat API endpoints operational for business functions",
Â                    "timeout_seconds": 30
Â                },
Â                {
Â                    "test_name": "test_webrtc_audio_functionality",
Â                    "description": "Test WebRTC audio functionality for InteractiveAudioChat (user experience critical)",
Â                    "test_type": "integration",
Â                    "priority": "high",
Â                    "test_code": """
async def test_webrtc_audio_functionality():
Â    try:
Â        # Test InteractiveAudioChat component functionality
Â        # Note: This tests the logic and API integration, not actual WebRTC in backend
Â 
Â        audio_features_tested = {}
Â 
Â        # Test 1: Speech Recognition API availability
Â        try:
Â            # Mock speech recognition capability test
Â            speech_recognition_available = True  # Would test browser API availability
Â            audio_features_tested["speech_recognition"] = {
Â                "available": speech_recognition_available,
Â                "business_function": "Voice Input Processing",
Â                "user_experience_impact": "HIGH"
Â            }
Â        except Exception as sr_error:
Â            audio_features_tested["speech_recognition"] = {
Â                "available": False,
Â                "error": str(sr_error),
Â                "business_function": "Voice Input Processing"
Â            }
Â 
Â        # Test 2: Speech Synthesis API availability
Â        try:
Â            # Mock speech synthesis capability test
Â            speech_synthesis_available = True  # Would test browser API availability
Â            audio_features_tested["speech_synthesis"] = {
Â                "available": speech_synthesis_available,
Â                "business_function": "Voice Output Generation",
Â                "user_experience_impact": "HIGH"
Â            }
Â        except Exception as ss_error:
Â            audio_features_tested["speech_synthesis"] = {
Â                "available": False,
Â                "error": str(ss_error),
Â                "business_function": "Voice Output Generation"
Â            }
Â 
Â        # Test 3: Audio processing pipeline
Â        try:
Â            # Test audio processing logic
Â            audio_processing_pipeline = {
Â                "input_capture": True,
Â                "noise_reduction": True,
Â                "voice_activity_detection": True,
Â                "spiritual_response_generation": True,
Â                "audio_output": True
Â            }
Â 
Â            pipeline_working = all(audio_processing_pipeline.values())
Â            audio_features_tested["audio_pipeline"] = {
Â                "available": pipeline_working,
Â                "pipeline_stages": audio_processing_pipeline,
Â                "business_function": "Complete Audio Experience",
Â                "user_experience_impact": "CRITICAL"
Â            }
Â        except Exception as ap_error:
Â            audio_features_tested["audio_pipeline"] = {
Â                "available": False,
Â                "error": str(ap_error),
Â                "business_function": "Complete Audio Experience"
Â            }
Â 
Â        # Test 4: Spiritual AI integration with audio
Â        try:
Â            # Test spiritual guidance integration
Â            spiritual_audio_integration = {
Â                "voice_to_text_spiritual_query": True,
Â                "ai_spiritual_response": True,
Â                "text_to_speech_guidance": True,
Â                "multilingual_support": True
Â            }
Â 
Â            spiritual_integration_working = all(spiritual_audio_integration.values())
Â            audio_features_tested["spiritual_integration"] = {
Â                "available": spiritual_integration_working,
Â                "integration_features": spiritual_audio_integration,
Â                "business_function": "Spiritual Audio Guidance",
Â                "revenue_impact": "HIGH"
Â            }
Â        except Exception as si_error:
Â            audio_features_tested["spiritual_integration"] = {
Â                "available": False,
Â                "error": str(si_error),
Â                "business_function": "Spiritual Audio Guidance"
Â            }
Â 
Â        # Calculate user experience score
Â        working_features = sum(1 for feature in audio_features_tested.values() if feature.get("available", False))
Â        total_features = len(audio_features_tested)
Â        user_experience_score = (working_features / total_features) * 100
Â 
Â        return {
Â            "status": "passed" if user_experience_score > 75 else "failed",
Â            "message": "WebRTC audio functionality tested",
Â            "user_experience_score": user_experience_score,
Â            "working_features": working_features,
Â            "total_features": total_features,
Â            "feature_results": audio_features_tested
Â        }
Â 
Â    except Exception as e:
Â        return {"status": "failed", "error": f"WebRTC audio functionality test failed: {str(e)}"}
""",
Â                    "expected_result": "WebRTC audio functionality supports spiritual voice conversations",
Â                    "timeout_seconds": 25
Â                },
Â                {
Â                    "test_name": "test_video_call_database_schema",
Â                    "description": "Test video chat database tables and session data integrity (business data)",
Â                    "test_type": "unit",
Â                    "priority": "high",
Â                    "test_code": """
async def test_video_call_database_schema():
Â    conn = await asyncpg.connect(DATABASE_URL)
Â    try:
Â        # Test business-critical video chat tables
Â        video_chat_tables = [
Â            'video_chat_sessions',
Â            'video_chat_recordings',
Â            'video_chat_analytics',
Â            'live_chat_sessions'
Â        ]
Â 
Â        table_validation_results = {}
Â 
Â        for table in video_chat_tables:
Â            try:
Â                # Check table exists
Â                table_exists = await conn.fetchrow('''
Â                    SELECT table_name FROM information_schema.tables
Â                    WHERE table_name = $1 AND table_schema = 'public'
Â                ''', table)
Â 
Â                if table_exists:
Â                    # Test table structure for business requirements
Â                    columns = await conn.fetch('''
Â                        SELECT column_name, data_type, is_nullable
Â                        FROM information_schema.columns
Â                        WHERE table_name = $1 AND table_schema = 'public'
Â                        ORDER BY ordinal_position
Â                    ''', table)
Â 
Â                    column_names = [col['column_name'] for col in columns]
Â 
Â                    # Validate business-critical columns exist
Â                    required_columns = {
Â                        'video_chat_sessions': ['session_id', 'user_id', 'agora_channel', 'status', 'start_time', 'cost'],
Â                        'video_chat_recordings': ['recording_id', 'session_id', 'file_path', 'duration'],
Â                        'video_chat_analytics': ['session_id', 'duration', 'quality_score', 'user_satisfaction'],
Â                        'live_chat_sessions': ['session_id', 'user_id', 'agora_token', 'status', 'created_at']
Â                    }
Â 
Â                    missing_columns = [col for col in required_columns.get(table, []) if col not in column_names]
Â 
Â                    table_validation_results[table] = {
Â                        "exists": True,
Â                        "column_count": len(column_names),
Â                        "has_required_columns": len(missing_columns) == 0,
Â                        "missing_columns": missing_columns,
Â                        "business_ready": len(missing_columns) == 0
Â                    }
Â 
Â                    # Test business operations on table
Â                    if table == 'live_chat_sessions':
Â                        # Test session creation (business critical)
Â                        session_id = str(uuid.uuid4())
Â                        await conn.execute('''
Â                            INSERT INTO live_chat_sessions (session_id, user_id, agora_token, status, created_at)
Â                            VALUES ($1, $2, $3, $4, $5)
Â                        ''', session_id, "test_user_123", "test_token_456", "active", datetime.now(timezone.utc))
Â 
Â                        # Verify business data integrity
Â                        session = await conn.fetchrow(
Â                            "SELECT user_id, status FROM live_chat_sessions WHERE session_id = $1", session_id
Â                        )
Â 
Â                        assert session is not None, "Session should be created"
Â                        assert session['status'] == "active", "Status should be stored correctly"
Â 
Â                        # Cleanup
Â                        await conn.execute("DELETE FROM live_chat_sessions WHERE session_id = $1", session_id)
Â 
Â                        table_validation_results[table]["business_operations_tested"] = True
Â 
Â                else:
Â                    table_validation_results[table] = {
Â                        "exists": False,
Â                        "business_ready": False,
Â                        "business_impact": "HIGH - Video chat sessions disabled"
Â                    }
Â 
Â            except Exception as table_error:
Â                table_validation_results[table] = {
Â                    "exists": False,
Â                    "error": str(table_error),
Â                    "business_impact": "HIGH - Video operations failed"
Â                }
Â 
Â        # Calculate business readiness score
Â        business_ready_tables = sum(1 for result in table_validation_results.values() if result.get("business_ready", False))
Â        total_tables = len(video_chat_tables)
Â        business_readiness_score = (business_ready_tables / total_tables) * 100
Â 
Â        return {
Â            "status": "passed" if business_readiness_score > 75 else "failed",
Â            "message": "Video chat database schema validated",
Â            "business_readiness_score": business_readiness_score,
Â            "business_ready_tables": business_ready_tables,
Â            "total_tables": total_tables,
Â            "table_results": table_validation_results
Â        }
Â 
Â    except Exception as e:
Â        return {"status": "failed", "error": f"Video chat database schema test failed: {str(e)}"}
Â    finally:
Â        await conn.close()
""",
Â                    "expected_result": "Video chat database schema supports all business operations",
Â                    "timeout_seconds": 30
Â                },
Â                {
Â                    "test_name": "test_live_chat_frontend_integration",
Â                    "description": "Test LiveChat and InteractiveAudioChat frontend component integration",
Â                    "test_type": "integration",
Â                    "priority": "high",
Â                    "test_code": """
async def test_live_chat_frontend_integration():
Â    try:
Â        # Test frontend component availability and integration
Â        frontend_components = {}
Â 
Â        # Test 1: LiveChat component structure
Â        try:
Â            # Mock LiveChat component testing (would be actual component testing in real scenario)
Â            livechat_features = {
Â                "agora_video_call_integration": True,
Â                "donation_system_integration": True,
Â                "subscription_check": True,
Â                "session_management": True,
Â                "authentication_flow": True
Â            }
Â 
Â            livechat_working = all(livechat_features.values())
Â            frontend_components["LiveChat"] = {
Â                "available": livechat_working,
Â                "features": livechat_features,
Â                "business_function": "Video Consultation Interface",
Â                "revenue_impact": "CRITICAL"
Â            }
Â        except Exception as lc_error:
Â            frontend_components["LiveChat"] = {
Â                "available": False,
Â                "error": str(lc_error),
Â                "business_function": "Video Consultation Interface"
Â            }
Â 
Â        # Test 2: InteractiveAudioChat component structure
Â        try:
Â            # Mock InteractiveAudioChat component testing
Â            audio_chat_features = {
Â                "webrtc_integration": True,
Â                "speech_recognition": True,
Â                "speech_synthesis": True,
Â                "spiritual_ai_integration": True,
Â                "multilingual_support": True,
Â                "call_controls": True
Â            }
Â 
Â            audio_chat_working = all(audio_chat_features.values())
Â            frontend_components["InteractiveAudioChat"] = {
Â                "available": audio_chat_working,
Â                "features": audio_chat_features,
Â                "business_function": "Voice Consultation Interface",
Â                "user_experience_impact": "HIGH"
Â            }
Â        except Exception as ac_error:
Â            frontend_components["InteractiveAudioChat"] = {
Â                "available": False,
Â                "error": str(ac_error),
Â                "business_function": "Voice Consultation Interface"
Â            }
Â 
Â        # Test 3: AgoraVideoCall component integration
Â        try:
Â            # Mock AgoraVideoCall component testing
Â            agora_features = {
Â                "video_stream_management": True,
Â                "audio_stream_management": True,
Â                "screen_sharing": True,
Â                "call_quality_monitoring": True,
Â                "recording_capability": True
Â            }
Â 
Â            agora_working = all(agora_features.values())
Â            frontend_components["AgoraVideoCall"] = {
Â                "available": agora_working,
Â                "features": agora_features,
Â                "business_function": "Video Call Engine",
Â                "technical_criticality": "CRITICAL"
Â            }
Â        except Exception as av_error:
Â            frontend_components["AgoraVideoCall"] = {
Â                "available": False,
Â                "error": str(av_error),
Â                "business_function": "Video Call Engine"
Â            }
Â 
Â        # Test 4: Frontend API integration
Â        try:
Â            # Mock API integration testing
Â            api_integrations = {
Â                "livechat_initiate_api": True,
Â                "livechat_status_api": True,
Â                "agora_token_generation": True,
Â                "session_management_api": True,
Â                "spiritual_guidance_api": True
Â            }
Â 
Â            api_integration_working = all(api_integrations.values())
Â            frontend_components["API_Integration"] = {
Â                "available": api_integration_working,
Â                "integrations": api_integrations,
Â                "business_function": "Frontend-Backend Communication",
Â                "system_criticality": "CRITICAL"
Â            }
Â        except Exception as api_error:
Â            frontend_components["API_Integration"] = {
Â                "available": False,
Â                "error": str(api_error),
Â                "business_function": "Frontend-Backend Communication"
Â            }
Â 
Â        # Calculate frontend integration score
Â        working_components = sum(1 for component in frontend_components.values() if component.get("available", False))
Â        total_components = len(frontend_components)
Â        frontend_integration_score = (working_components / total_components) * 100
Â 
Â        # Check critical components specifically
Â        critical_components_working = sum(1 for component in frontend_components.values()
Â                                        if component.get("available", False) and
Â                                        (component.get("revenue_impact") == "CRITICAL" or
Â                                         component.get("technical_criticality") == "CRITICAL" or
Â                                         component.get("system_criticality") == "CRITICAL"))
Â 
Â        return {
Â            "status": "passed" if frontend_integration_score > 75 else "failed",
Â            "message": "Live chat frontend integration tested",
Â            "frontend_integration_score": frontend_integration_score,
Â            "working_components": working_components,
Â            "total_components": total_components,
Â            "critical_components_working": critical_components_working,
Â            "component_results": frontend_components
Â        }
Â 
Â    except Exception as e:
Â        return {"status": "failed", "error": f"Live chat frontend integration test failed: {str(e)}"}
""",
Â                    "expected_result": "Frontend components integrate seamlessly for live audio/video experience",
Â                    "timeout_seconds": 35
Â                },
Â                {
Â                    "test_name": "test_live_audio_video_system_health",
Â                    "description": "Test overall live audio/video system health and business continuity",
Â                    "test_type": "integration",
Â                    "priority": "critical",
Â                    "test_code": """
async def test_live_audio_video_system_health():
Â    try:
Â        # Test comprehensive audio/video system health
Â        system_components = {}
Â 
Â        # Component 1: Agora Service availability
Â        try:
Â            # Import Agora service using importlib (safer for exec context)
Â            import importlib.util
Â 
Â            # Try to import the Agora service
Â            spec = importlib.util.find_spec('agora_service')
Â            if spec:
Â                agora_module = importlib.util.module_from_spec(spec)
Â                spec.loader.exec_module(agora_module)
Â                AgoraService = agora_module.AgoraService
Â            else:
Â                # Fallback: direct import
Â                from agora_service import AgoraService
Â 
Â            agora_service = AgoraService()
Â            system_components["agora_service"] = {
Â                "available": True,
Â                "business_function": "Video/Audio Infrastructure",
Â                "criticality": "CRITICAL",
Â                "revenue_impact": "HIGH"
Â            }
Â        except Exception as agora_error:
Â            system_components["agora_service"] = {
Â                "available": False,
Â                "error": str(agora_error),
Â                "business_function": "Video/Audio Infrastructure",
Â                "criticality": "CRITICAL"
Â            }
Â 
Â        # Component 2: LiveChat Router availability
Â        try:
Â            # Test livechat router import
Â            from routers.livechat import livechat_router
Â            system_components["livechat_router"] = {
Â                "available": True,
Â                "business_function": "Session Management API",
Â                "criticality": "CRITICAL",
Â                "revenue_impact": "HIGH"
Â            }
Â        except Exception as router_error:
Â            system_components["livechat_router"] = {
Â                "available": False,
Â                "error": str(router_error),
Â                "business_function": "Session Management API",
Â                "criticality": "CRITICAL"
Â            }
Â 
Â        # Component 3: Database Tables availability
Â        try:
Â            conn = await asyncpg.connect(DATABASE_URL)
Â 
Â            # Check critical tables
Â            tables_to_check = ['live_chat_sessions', 'video_chat_sessions']
Â            tables_available = 0
Â 
Â            for table in tables_to_check:
Â                result = await conn.fetchrow('''
Â                    SELECT table_name FROM information_schema.tables
Â                    WHERE table_name = $1 AND table_schema = 'public'
Â                ''', table)
Â                if result:
Â                    tables_available += 1
Â 
Â            await conn.close()
Â 
Â            system_components["database_tables"] = {
Â                "available": tables_available > 0,
Â                "available_count": tables_available,
Â                "total_count": len(tables_to_check),
Â                "coverage_percent": (tables_available / len(tables_to_check)) * 100,
Â                "business_function": "Session Data Storage",
Â                "criticality": "HIGH"
Â            }
Â        except Exception as db_error:
Â            system_components["database_tables"] = {
Â                "available": False,
Â                "error": str(db_error),
Â                "business_function": "Session Data Storage",
Â                "criticality": "HIGH"
Â            }
Â 
Â        # Component 4: Frontend Components availability
Â        try:
Â            # Mock frontend component availability check
Â            frontend_components_available = True  # Would check actual component files
Â            system_components["frontend_components"] = {
Â                "available": frontend_components_available,
Â                "business_function": "User Interface",
Â                "criticality": "HIGH",
Â                "user_experience_impact": "CRITICAL"
Â            }
Â        except Exception as frontend_error:
Â            system_components["frontend_components"] = {
Â                "available": False,
Â                "error": str(frontend_error),
Â                "business_function": "User Interface",
Â                "criticality": "HIGH"
Â            }
Â 
Â        # Component 5: API Endpoints health
Â        try:
Â            import httpx
Â            async with httpx.AsyncClient() as client:
Â                response = await client.get("https://jyotiflow-ai.onrender.com/api/admin/agora/overview")
Â                api_healthy = response.status_code in [200, 401, 403]
Â        except (ImportError, Exception) as api_error:
Â             logger.debug(f"API health check failed: {api_error}")
Â             api_healthy = False
Â 
Â        system_components["api_endpoints"] = {
Â            "available": api_healthy,
Â            "business_function": "Live Session Management",
Â            "criticality": "HIGH"
Â        }
Â 
Â        # Calculate overall business continuity score
Â        critical_components = [comp for comp in system_components.values() if comp.get("criticality") == "CRITICAL"]
Â        critical_available = sum(1 for comp in critical_components if comp.get("available", False))
Â 
Â        high_components = [comp for comp in system_components.values() if comp.get("criticality") == "HIGH"]
Â        high_available = sum(1 for comp in high_components if comp.get("available", False))
Â 
Â        # Business continuity formula: Critical components weight 70%, High components 30%
Â        total_critical = len(critical_components)
Â        total_high = len(high_components)
Â 
Â        if total_critical > 0 and total_high > 0:
Â            business_continuity_score = (
Â                (critical_available / total_critical) * 0.7 +
Â                (high_available / total_high) * 0.3
Â            ) * 100
Â        else:
Â            business_continuity_score = 0
Â 
Â        # Determine business status
Â        if business_continuity_score >= 90:
Â            business_status = "OPTIMAL"
Â        elif business_continuity_score >= 70:
Â            business_status = "OPERATIONAL"
Â        elif business_continuity_score >= 50:
Â            business_status = "DEGRADED"
Â        else:
Â            business_status = "CRITICAL"
Â 
Â        # Check revenue impact
Â        revenue_critical_working = sum(1 for comp in system_components.values()
Â                                     if comp.get("revenue_impact") == "HIGH" and comp.get("available", False))
Â        total_revenue_critical = sum(1 for comp in system_components.values() if comp.get("revenue_impact") == "HIGH")
Â 
Â        revenue_continuity_score = (revenue_critical_working / total_revenue_critical * 100) if total_revenue_critical > 0 else 0
Â 
Â        return {
Â            "status": "passed" if business_continuity_score > 70 else "failed",
Â            "message": "Live audio/video system health checked",
Â            "business_continuity_score": business_continuity_score,
Â            "business_status": business_status,
Â            "revenue_continuity_score": revenue_continuity_score,
Â            "critical_components_available": critical_available,
Â            "total_critical_components": total_critical,
Â            "high_components_available": high_available,
Â            "total_high_components": total_high,
Â            "revenue_critical_working": revenue_critical_working,
Â            "total_revenue_critical": total_revenue_critical,
Â            "component_health": system_components
Â        }
Â 
Â    except Exception as e:
Â        return {"status": "failed", "error": f"Live audio/video system health test failed: {str(e)}"}
""",
Â                    "expected_result": "Live audio/video system maintains business continuity and revenue generation",
Â                    "timeout_seconds": 50
Â                }
Â            ]
Â        }
Â 
Â    async def generate_avatar_generation_tests(self) -> Dict[str, Any]:
Â        """Generate comprehensive avatar generation service tests - BUSINESS CRITICAL"""
Â        return {
Â            "test_suite_name": "Avatar Generation Services",
Â            "test_category": "avatar_generation_business_critical",
Â            "description": "Business-critical tests for avatar generation, video creation, and spiritual avatar services",
Â            "test_cases": [
Â                {
Â                    "test_name": "test_avatar_generation_api_endpoints",
Â                    "description": "Test avatar generation API endpoints (revenue critical)",
Â                    "test_type": "integration",
Â                    "priority": "critical",
Â                    "test_code": """
import httpx

async def test_avatar_generation_api_endpoints():
Â    try:
Â        # Test business-critical avatar generation endpoints
Â        endpoints_to_test = [
Â            {"url": "/api/avatar/generate", "method": "POST", "business_function": "Avatar Generation", "test_data": {"user_id": "test_user", "content": "Test spiritual guidance"}},
Â            {"url": "/api/avatar/status/test_session", "method": "GET", "business_function": "Generation Status", "test_data": None},
Â            {"url": "/api/admin/avatar/overview", "method": "GET", "business_function": "Admin Analytics", "test_data": None}
Â        ]
Â 
Â        endpoint_results = {}
Â 
Â        async with httpx.AsyncClient() as client:
Â            for endpoint in endpoints_to_test:
Â                try:
Â                    url = f"https://jyotiflow-ai.onrender.com{endpoint['url']}"
Â 
Â                    if endpoint['method'] == 'GET':
Â                        response = await client.get(url)
Â                    else:
Â                        response = await client.post(url, json=endpoint['test_data'] or {})
Â 
Â                    # Business-critical endpoints should be accessible (even if auth required)
Â                    endpoint_results[endpoint['business_function']] = {
Â                        "endpoint_accessible": response.status_code in [200, 401, 403, 422],
Â                        "status_code": response.status_code,
Â                        "business_impact": "HIGH" if endpoint['business_function'] == "Avatar Generation" else "MEDIUM",
Â                        "revenue_critical": endpoint['business_function'] == "Avatar Generation"
Â                    }
Â 
Â                except Exception as endpoint_error:
Â                    endpoint_results[endpoint['business_function']] = {
Â                        "endpoint_accessible": False,
Â                        "error": str(endpoint_error),
Â                        "business_impact": "HIGH"
Â                    }
Â 
Â        # Calculate business continuity score
Â        accessible_endpoints = sum(1 for result in endpoint_results.values() if result.get("endpoint_accessible", False))
Â        total_endpoints = len(endpoints_to_test)
Â        business_continuity_score = (accessible_endpoints / total_endpoints) * 100
Â 
Â        return {
Â            "status": "passed" if business_continuity_score > 70 else "failed",
Â            "message": "Avatar generation API endpoints tested",
Â            "business_continuity_score": business_continuity_score,
Â            "accessible_endpoints": accessible_endpoints,
Â            "total_endpoints": total_endpoints,
Â            "endpoint_results": endpoint_results
Â        }
Â 
Â    except Exception as e:
Â        return {"status": "failed", "error": f"Avatar generation API endpoints test failed: {str(e)}"}
""",
Â                    "expected_result": "Avatar generation API endpoints operational for business functions",
Â                    "timeout_seconds": 30
Â                },
Â                {
Â                    "test_name": "test_avatar_database_schema",
Â                    "description": "Test avatar generation database tables and data integrity",
Â                    "test_type": "unit",
Â                    "priority": "high",
Â                    "test_code": """
async def test_avatar_database_schema():
Â    conn = await asyncpg.connect(DATABASE_URL)
Â    try:
Â        # Test business-critical avatar tables
Â        avatar_tables = [
Â            'avatar_generations',
Â            'avatar_sessions',
Â            'avatar_cache'
Â        ]
Â 
Â        table_validation_results = {}
Â 
Â        for table in avatar_tables:
Â            try:
Â                # Check table exists
Â                table_exists = await conn.fetchrow('''
Â                    SELECT table_name FROM information_schema.tables
Â                    WHERE table_name = $1 AND table_schema = 'public'
Â                ''', table)
Â 
Â                if table_exists:
Â                    # Test table structure for business requirements
Â                    columns = await conn.fetch('''
Â                        SELECT column_name, data_type, is_nullable
Â                        FROM information_schema.columns
Â                        WHERE table_name = $1 AND table_schema = 'public'
Â                        ORDER BY ordinal_position
Â                    ''', table)
Â 
Â                    column_names = [col['column_name'] for col in columns]
Â 
Â                    table_validation_results[table] = {
Â                        "exists": True,
Â                        "column_count": len(column_names),
Â                        "business_ready": True
Â                    }
Â 
Â                else:
Â                    table_validation_results[table] = {
Â                        "exists": False,
Â                        "business_ready": False,
Â                        "business_impact": "MEDIUM - Avatar generation might use alternative storage"
Â                    }
Â 
Â            except Exception as table_error:
Â                table_validation_results[table] = {
Â                    "exists": False,
Â                    "error": str(table_error),
Â                    "business_impact": "MEDIUM"
Â                }
Â 
Â        # Calculate business readiness score
Â        business_ready_tables = sum(1 for result in table_validation_results.values() if result.get("business_ready", False))
Â        total_tables = len(avatar_tables)
Â        business_readiness_score = (business_ready_tables / total_tables) * 100 if total_tables > 0 else 100
Â 
Â        return {
Â            "status": "passed" if business_readiness_score > 50 else "failed",
Â            "message": "Avatar generation database schema validated",
Â            "business_readiness_score": business_readiness_score,
Â            "business_ready_tables": business_ready_tables,
Â            "total_tables": total_tables,
Â            "table_results": table_validation_results
Â        }
Â 
Â    except Exception as e:
Â        return {"status": "failed", "error": f"Avatar database schema test failed: {str(e)}"}
Â    finally:
Â        await conn.close()
""",
Â                    "expected_result": "Avatar generation database schema supports business operations",
Â                    "timeout_seconds": 25
Â                }
Â            ]
Â        }
Â 
Â    async def generate_credit_payment_tests(self) -> Dict[str, Any]:
Â        """Generate comprehensive credit and payment system tests - REVENUE CRITICAL"""
Â        return {
Â            "test_suite_name": "Credit & Payment Systems",
Â            "test_category": "credit_payment_revenue_critical",
Â            "description": "Revenue-critical tests for credit packages, payment processing, subscription management, and monetization",
Â            "test_cases": [
Â                {
Â                    "test_name": "test_credit_package_service",
Â                    "description": "Test CreditPackageService functionality (revenue critical)",
Â                    "test_type": "integration",
Â                    "priority": "critical",
Â                    "test_code": """
import httpx
import asyncpg
import uuid
import os
import time
import json

async def test_credit_package_service():
Â    try:
Â        # Test credit package service endpoints (revenue critical)
Â        api_base_url = os.environ.get("API_BASE_URL", "https://jyotiflow-ai.onrender.com")
Â        if not api_base_url:
Â            api_base_url = "https://jyotiflow-ai.onrender.com"
Â        test_session_id = f"credit_service_{uuid.uuid4()}"
Â 
Â        # Test revenue-critical credit package endpoints
Â        endpoints_to_test = [
Â            {"url": "/api/admin/credit-packages", "method": "GET", "business_function": "Package Listing", "revenue_impact": "HIGH"},
Â            {"url": "/api/services", "method": "GET", "business_function": "Service Types", "revenue_impact": "CRITICAL"},
Â            {"url": "/api/user/credits", "method": "GET", "business_function": "Credit Balance", "revenue_impact": "HIGH"}
Â        ]
Â 
Â        test_results = {}
Â 
Â        # Database connection for storing results
Â        conn = None
Â        try:
Â            conn = await asyncpg.connect(DATABASE_URL)
Â        except (asyncpg.PostgresError, asyncpg.PostgresConnectionError) as db_error:
Â            conn = None
Â            print(f"Database connection failed for credit package service test: {db_error}")
Â        except Exception as connection_error:
Â            conn = None
Â            print(f"Unexpected database connection error: {connection_error}")
Â 
Â 
Â        async with httpx.AsyncClient(timeout=30.0, headers={
Â            "Content-Type": "application/json",
Â            "User-Agent": "JyotiFlow-TestRunner/1.0",
Â            "X-Test-Run": "true",
Â            "X-Test-Type": "credit-package-service"
Â        }) as client:
Â            for endpoint in endpoints_to_test:
Â                # Dynamic URL construction
Â                url = f"{api_base_url}{endpoint['url']}"
Â 
Â                # Start timing before request attempt
Â                start_time = time.perf_counter()
Â                request_payload = {} if endpoint['method'] != 'GET' else None
Â 
Â                try:
Â                    if endpoint['method'] == 'GET':
Â                        response = await client.get(url)
Â                    else:
Â                        response = await client.post(url, json=request_payload)
Â                    end_time = time.perf_counter()
Â                    response_time_ms = int((end_time - start_time) * 1000)  # Convert to milliseconds
Â 
Â                    # Credit package endpoints should be accessible (even if auth required)
Â                    result = {
Â                        "available": response.status_code in [200, 401, 403, 422],
Â                        "status_code": response.status_code,
Â                        "business_function": endpoint['business_function'],
Â                        "revenue_impact": endpoint['revenue_impact'],
Â                        "endpoint_url": url,
Â                        "method": endpoint['method'],
Â                        "response_time_ms": response_time_ms
Â                    }
Â                    test_results[endpoint['business_function']] = result
Â 
Â                    # Store in database (database-driven approach)
Â                    if conn:
Â                        try:
Â                            # Store session info and request payload properly in request_body as JSON
Â                            request_body_json = json.dumps({
Â                                "test_session_id": test_session_id,
Â                                "request_payload": request_payload
Â                            }) if request_payload is not None else json.dumps({"test_session_id": test_session_id})
Â 
Â                            await conn.execute('''
Â                                INSERT INTO monitoring_api_calls
Â                                (endpoint, method, status_code, response_time, user_id, request_body, error)
Â                                VALUES ($1, $2, $3, $4, $5, $6, $7)
Â                                ON CONFLICT DO NOTHING
Â                            ''', url, endpoint['method'], response.status_code,
Â                                response_time_ms, None, request_body_json, None)
Â                        except Exception as db_error:
Â                            result["db_storage_error"] = str(db_error)
Â                            print(f"Database storage error for credit package service test: {db_error}")
Â                            # Continue without raising - preserve successful HTTP result
Â 
Â                except Exception as endpoint_error:
Â                    # Calculate response time even in exception path
Â                    end_time = time.perf_counter()
Â                    response_time_ms = int((end_time - start_time) * 1000)
Â 
Â                    error_result = {
Â                        "available": False,
Â                        "error": str(endpoint_error),
Â                        "business_function": endpoint['business_function'],
Â                        "revenue_impact": endpoint['revenue_impact'],
Â                        "endpoint_url": url,
Â                        "method": endpoint['method'],
Â                        "response_time_ms": response_time_ms
Â                    }
Â                    test_results[endpoint['business_function']] = error_result
Â 
Â                    # Store error in database
Â                    if conn:
Â                        try:
Â                            # Store session info and request payload properly in request_body as JSON
Â                            request_body_json = json.dumps({
Â                                "test_session_id": test_session_id,
Â                                "request_payload": request_payload
Â                            }) if request_payload is not None else json.dumps({"test_session_id": test_session_id})
Â 
Â                            await conn.execute('''
Â                                INSERT INTO monitoring_api_calls
Â                                (endpoint, method, status_code, response_time, user_id, request_body, error)
Â                                VALUES ($1, $2, $3, $4, $5, $6, $7)
Â                                ON CONFLICT DO NOTHING
Â                            ''', url, endpoint['method'], 500,
Â                                response_time_ms, None, request_body_json, str(endpoint_error))
Â                        except Exception as e:
Â                            db_storage_error = str(e)
Â                            error_result["db_storage_error"] = db_storage_error
Â                            print(f"Database error storing credit package service test error: {db_storage_error}")
Â 
Â        # Close database connection
Â        if conn:
Â            await conn.close()
Â 
Â        # Calculate revenue protection score
Â        working_functions = sum(1 for result in test_results.values() if result.get("available", False))
Â        total_functions = len(test_results)
Â        revenue_protection_score = (working_functions / total_functions) * 100 if total_functions > 0 else 0
Â 
Â        # Check critical revenue functions
Â        critical_revenue_working = sum(1 for result in test_results.values()
Â                                     if result.get("revenue_impact") == "CRITICAL" and result.get("available", False))
Â        total_critical_revenue = sum(1 for result in test_results.values() if result.get("revenue_impact") == "CRITICAL")
Â 
Â        return {
Â            "status": "passed" if revenue_protection_score > 80 else "failed",
Â            "message": "Credit package service endpoints tested",
Â            "revenue_protection_score": revenue_protection_score,
Â            "working_functions": working_functions,
Â            "total_functions": total_functions,
Â            "critical_revenue_working": critical_revenue_working,
Â            "total_critical_revenue": total_critical_revenue,
Â            "function_results": test_results
Â        }
Â 
Â    except Exception as e:
Â        return {"status": "failed", "error": f"Credit package service test failed: {str(e)}"}
""",
Â                    "expected_result": "CreditPackageService protects revenue streams and optimizes costs",
Â                    "timeout_seconds": 35
Â                },
Â                {
Â                    "test_name": "test_payment_api_endpoints",
Â                    "description": "Test payment processing API endpoints (revenue critical)",
Â                    "test_type": "integration",
Â                    "priority": "critical",
Â                    "test_code": """
import httpx
import asyncpg
import uuid
import os
import time
import json

async def test_payment_api_endpoints():
Â    try:
Â        # Test revenue-critical payment endpoints (your 5 specified endpoints) - Dynamic, no hardcoded URLs
Â        api_base_url = os.environ.get("API_BASE_URL", "https://jyotiflow-ai.onrender.com")
Â        if not api_base_url:
Â            api_base_url = "https://jyotiflow-ai.onrender.com"
Â        test_session_id = f"credit_payment_{uuid.uuid4()}"
Â 
Â        endpoints_to_test = [
Â            {"url": "/api/credits/purchase", "method": "POST", "business_function": "Credit Purchase", "test_data": {"package_id": 1, "payment_method": "test"}},
Â            {"url": "/api/user/credits", "method": "GET", "business_function": "User Credit Balance", "test_data": None},
Â            {"url": "/api/admin/credit-packages", "method": "GET", "business_function": "Package Management", "test_data": None},
Â            {"url": "/api/admin/subscription-plans", "method": "GET", "business_function": "Subscription Management", "test_data": None},
Â            {"url": "/api/services/types", "method": "GET", "business_function": "Service Types", "test_data": None}
Â        ]
Â 
Â        endpoint_results = {}
Â 
Â        # Database connection for storing results
Â        conn = None
Â        try:
Â            conn = await asyncpg.connect(DATABASE_URL)
Â        except (asyncpg.PostgresError, asyncpg.PostgresConnectionError) as db_error:
Â            conn = None
Â            print(f"Database connection failed for payment API endpoints test: {db_error}")
Â        except Exception as connection_error:
Â            conn = None
Â            print(f"Unexpected database connection error: {connection_error}")
Â 
Â        async with httpx.AsyncClient(timeout=httpx.Timeout(connect=5.0, read=10.0, write=10.0, pool=5.0)) as client:
Â            for endpoint in endpoints_to_test:
Â                # Dynamic URL construction - moved out of try block to prevent UnboundLocalError
Â                url = f"{api_base_url}{endpoint['url']}"
Â 
Â                # Start timing before request attempt
Â                start_time = time.perf_counter()
Â                request_payload = endpoint['test_data'] or {} if endpoint['method'] != 'GET' else None
Â 
Â                try:
Â                    if endpoint['method'] == 'GET':
Â                        response = await client.get(url)
Â                    else:
Â                        response = await client.post(url, json=request_payload)
Â                    end_time = time.perf_counter()
Â                    response_time_ms = int((end_time - start_time) * 1000)  # Convert to milliseconds
Â 
Â                    # Revenue-critical endpoints should be accessible (even if auth required)
Â                    result = {
Â                        "endpoint_accessible": response.status_code in [200, 401, 403, 422],
Â                        "status_code": response.status_code,
Â                        "business_impact": "CRITICAL" if endpoint['business_function'] in ["Credit Purchase", "User Credit Balance"] else "HIGH",
Â                        "revenue_critical": endpoint['business_function'] in ["Credit Purchase", "Service Types"],
Â                        "endpoint_url": url,
Â                        "method": endpoint['method'],
Â                        "response_time_ms": response_time_ms
Â                    }
Â                    endpoint_results[endpoint['business_function']] = result
Â 
Â                    # Store in database (database-driven approach)
Â                    if conn:
Â                        try:
Â                            # Store session info and request payload properly in request_body as JSON
Â                            request_body_json = json.dumps({
Â                                "test_session_id": test_session_id,
Â                                "request_payload": request_payload
Â                            }) if request_payload is not None else json.dumps({"test_session_id": test_session_id})
Â 
Â                            await conn.execute('''
Â                                INSERT INTO monitoring_api_calls
Â                                (endpoint, method, status_code, response_time, user_id, request_body, error)
Â                                VALUES ($1, $2, $3, $4, $5, $6, $7)
Â                                ON CONFLICT DO NOTHING
Â                            ''', url, endpoint['method'], response.status_code,
Â                                response_time_ms, None, request_body_json, None)
Â                        except Exception as db_error:
Â                            result["db_storage_error"] = str(db_error)
Â                            print(f"Database storage error for payment API endpoints test: {db_error}")
Â 
Â                except Exception as endpoint_error:
Â                    # Calculate response time even in exception path
Â                    end_time = time.perf_counter()
Â                    response_time_ms = int((end_time - start_time) * 1000)
Â 
Â                    error_result = {
Â                        "endpoint_accessible": False,
Â                        "error": str(endpoint_error),
Â                        "business_impact": "CRITICAL",
Â                        "endpoint_url": url,
Â                        "method": endpoint['method'],
Â                        "response_time_ms": response_time_ms
Â                    }
Â                    endpoint_results[endpoint['business_function']] = error_result
Â 
Â                    # Store error in database
Â                    if conn:
Â                        try:
Â                            # Store session info and request payload properly in request_body as JSON
Â                            request_body_json = json.dumps({
Â                                "test_session_id": test_session_id,
Â                                "request_payload": request_payload
Â                            }) if request_payload is not None else json.dumps({"test_session_id": test_session_id})
Â 
Â                            await conn.execute('''
Â                                INSERT INTO monitoring_api_calls
Â                                (endpoint, method, status_code, response_time, user_id, request_body, error)
Â                                VALUES ($1, $2, $3, $4, $5, $6, $7)
Â                                ON CONFLICT DO NOTHING
Â                            ''', url, endpoint['method'], 500,
Â                                response_time_ms, None, request_body_json, str(endpoint_error))
Â                        except Exception as e:
Â                            db_storage_error = str(e)
Â                            error_result["db_storage_error"] = db_storage_error
Â                            print(f"Database error storing payment API endpoints test error: {db_storage_error}")
Â 
Â        # Calculate revenue continuity score
Â        accessible_endpoints = sum(1 for result in endpoint_results.values() if result.get("endpoint_accessible", False))
Â        total_endpoints = len(endpoints_to_test)
Â        revenue_continuity_score = (accessible_endpoints / total_endpoints) * 100
Â 
Â        # Check revenue-critical endpoints specifically
Â        revenue_critical_working = sum(1 for result in endpoint_results.values()
Â                                     if result.get("revenue_critical", False) and result.get("endpoint_accessible", False))
Â        total_revenue_critical = sum(1 for result in endpoint_results.values() if result.get("revenue_critical", False))
Â 
Â        # Close database connection
Â        if conn:
Â            await conn.close()
Â 
Â        return {
Â            "status": "passed" if revenue_continuity_score > 60 else "failed",
Â            "message": f"Credit/Payment endpoints tested - {accessible_endpoints}/{total_endpoints} accessible (database-driven)",
Â            "test_session_id": test_session_id,
Â            "revenue_continuity_score": revenue_continuity_score,
Â            "accessible_endpoints": accessible_endpoints,
Â            "total_endpoints": total_endpoints,
Â            "revenue_critical_working": revenue_critical_working,
Â            "total_revenue_critical": total_revenue_critical,
Â            "endpoint_results": endpoint_results,
Â            "database_storage": conn is not None,
Â            "endpoints_tested": [f"{e['method']} {e['url']}" for e in endpoints_to_test]
Â        }
Â 
Â    except Exception as e:
Â        return {"status": "failed", "error": f"Credit/Payment endpoints test failed: {str(e)}"}
""",
Â                    "expected_result": "Payment API endpoints operational for revenue generation",
Â                    "timeout_seconds": 30
Â                },
Â                {
Â                    "test_name": "test_credit_payment_database_schema",
Â                    "description": "Test credit and payment database tables (revenue data integrity)",
Â                    "test_type": "unit",
Â                    "priority": "critical",
Â                    "test_code": """
async def test_credit_payment_database_schema():
Â    conn = await asyncpg.connect(DATABASE_URL)
Â    try:
Â        # Test revenue-critical credit/payment tables
Â        payment_tables = [
Â            'credit_packages',
Â            'user_credits',
Â            'payment_transactions',
Â            'subscription_plans',
Â            'user_subscriptions',
Â            'service_types'
Â        ]
Â 
Â        table_validation_results = {}
Â 
Â        for table in payment_tables:
Â            try:
Â                # Check table exists
Â                table_exists = await conn.fetchrow('''
Â                    SELECT table_name FROM information_schema.tables
Â                    WHERE table_name = $1 AND table_schema = 'public'
Â                ''', table)
Â 
Â                if table_exists:
Â                    # Test table structure for revenue requirements
Â                    columns = await conn.fetch('''
Â                        SELECT column_name, data_type, is_nullable
Â                        FROM information_schema.columns
Â                        WHERE table_name = $1 AND table_schema = 'public'
Â                        ORDER BY ordinal_position
Â                    ''', table)
Â 
Â                    column_names = [col['column_name'] for col in columns]
Â 
Â                    # Validate revenue-critical columns exist
Â                    required_columns = {
Â                        'credit_packages': ['id', 'name', 'credits', 'price_usd', 'enabled'],
Â                        'user_credits': ['user_id', 'credits_balance', 'credits_used'],
Â                        'payment_transactions': ['id', 'user_id', 'amount', 'status', 'created_at'],
Â                        'subscription_plans': ['id', 'name', 'price_usd', 'features'],
Â                        'user_subscriptions': ['user_id', 'plan_id', 'status', 'start_date'],
Â                        'service_types': ['id', 'name', 'credits_required', 'price_usd']
Â                    }
Â 
Â                    missing_columns = [col for col in required_columns.get(table, []) if col not in column_names]
Â 
Â                    table_validation_results[table] = {
Â                        "exists": True,
Â                        "column_count": len(column_names),
Â                        "has_required_columns": len(missing_columns) == 0,
Â                        "missing_columns": missing_columns,
Â                        "revenue_ready": len(missing_columns) == 0
Â                    }
Â 
Â                    # Test revenue operations on critical tables
Â                    if table == 'credit_packages' and len(missing_columns) == 0:
Â                        # Test package operations (revenue critical)
Â                        package_count = await conn.fetchval(
Â                            "SELECT COUNT(*) FROM credit_packages WHERE enabled = true"
Â                        )
Â 
Â                        table_validation_results[table]["active_packages"] = package_count or 0
Â                        table_validation_results[table]["revenue_operations_tested"] = True
Â 
Â                else:
Â                    table_validation_results[table] = {
Â                        "exists": False,
Â                        "revenue_ready": False,
Â                        "business_impact": "CRITICAL - Revenue processing disabled"
Â                    }
Â 
Â            except Exception as table_error:
Â                table_validation_results[table] = {
Â                    "exists": False,
Â                    "error": str(table_error),
Â                    "business_impact": "CRITICAL - Revenue operations failed"
Â                }
Â 
Â        # Calculate revenue readiness score
Â        revenue_ready_tables = 0
Â        for result in table_validation_results.values():
Â            if result.get("revenue_ready", False):
Â                revenue_ready_tables += 1
Â        total_tables = len(payment_tables)
Â        revenue_readiness_score = (revenue_ready_tables / total_tables) * 100
Â 
Â        return {
Â            "status": "passed" if revenue_readiness_score > 80 else "failed",
Â            "message": "Credit payment database schema validated",
Â            "revenue_readiness_score": revenue_readiness_score,
Â            "revenue_ready_tables": revenue_ready_tables,
Â            "total_tables": total_tables,
Â            "table_results": table_validation_results
Â        }
Â 
Â    except Exception as e:
Â        return {"status": "failed", "error": f"Credit payment database schema test failed: {str(e)}"}
Â    finally:
Â        await conn.close()
""",
Â                    "expected_result": "Credit payment database schema protects revenue data integrity",
Â                    "timeout_seconds": 35
Â                }
Â            ]
Â        }
Â 
Â    async def store_test_suites(self, test_suites: Dict[str, Any]) -> None:
Â        """
Â        Store generated test suites in the database for execution tracking.
Â 
Â        Args:
Â            test_suites: Dictionary of test suites organized by category
Â 
Â        Raises:
Â            DatabaseConnectionError: If unable to connect to database
Â            TestStorageError: If test suite storage fails
Â        """
Â        if not self.database_url:
Â            logger.warning("No database URL provided, skipping test suite storage")
Â            return
Â 
Â        try:
Â            conn = await asyncpg.connect(self.database_url)
Â            try:
Â                # Store each test suite as a session with generated test cases
Â                for suite_name, suite_data in test_suites.items():
Â                    if isinstance(suite_data, dict) and 'test_category' in suite_data:
Â                        # Create a test execution session for this suite
Â                        session_id = str(uuid.uuid4())
Â                        await conn.execute("""
Â                            INSERT INTO test_execution_sessions (
Â                                session_id, test_type, test_category, environment,
Â                                started_at, status, triggered_by, created_at
Â                            ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
Â                            ON CONFLICT (session_id) DO NOTHING
Â                        """,
Â                        session_id,
Â                        suite_name,
Â                        suite_data['test_category'],
Â                        "production",
Â                        datetime.now(timezone.utc).replace(tzinfo=None),
Â                        "generated",
Â                        "test_suite_generator",
Â                        datetime.now(timezone.utc).replace(tzinfo=None)
Â                        )
Â 
Â                        # Store individual test cases for this suite
Â                        test_cases = suite_data.get('test_cases', [])
Â                        for test_case in test_cases:
Â                            if isinstance(test_case, dict):
Â                                await conn.execute("""
Â                                    INSERT INTO test_case_results (
Â                                        session_id, test_name, test_category, status,
Â                                        test_data, output_data, created_at
Â                                    ) VALUES ($1, $2, $3, $4, $5, $6, $7)
Â                                """,
Â                                session_id,
Â                                test_case.get('test_name', 'unnamed_test'),
Â                                suite_data['test_category'],  # FIXED: Always use suite category, no individual override
Â                                "generated",
Â                                json.dumps(test_case),
Â                                json.dumps(test_case),
Â                                datetime.now(timezone.utc).replace(tzinfo=None)
Â                                )
Â 
Â                logger.info("âœ… Test suites stored in database successfully")
Â 
Â            finally:
Â                await conn.close()
Â 
Â        except asyncpg.PostgresError as db_error:
Â            logger.error(f"Database error storing test suites: {db_error}")
Â            raise DatabaseConnectionError(f"Failed to connect to database: {db_error}") from db_error
Â        except Exception as storage_error:
Â            logger.error(f"Failed to store test suites: {storage_error}")
Â            raise TestStorageError(f"Test suite storage failed: {storage_error}") from storage_error

Â    async def generate_user_management_tests(self) -> Dict[str, Any]:
Â        """Generate user management service tests - USER EXPERIENCE CRITICAL"""
Â        return {
Â            "test_suite_name": "User Management Services",
Â            "test_category": "user_management_critical",
Â            "description": "Critical tests for user registration, authentication, profile management, and user services",
Â            "test_cases": [
Â                {
Â                    "test_name": "test_user_management_api_endpoints",
Â                    "description": "Test user management API endpoints",
Â                    "test_type": "integration",
Â                    "priority": "critical",
Â                    "test_code": """
import httpx

async def test_user_management_api_endpoints():
Â    try:
Â        endpoints_to_test = [
Â            {"url": "/api/auth/login", "method": "POST", "business_function": "Authentication"},
Â            {"url": "/register", "method": "POST", "business_function": "Registration"},
Â            {"url": "/api/user/profile", "method": "GET", "business_function": "Profile Management"},
Â            {"url": "/api/sessions/user", "method": "GET", "business_function": "Session History"}
Â        ]
Â 
Â        endpoint_results = {}
Â 
Â        async with httpx.AsyncClient() as client:
Â            for endpoint in endpoints_to_test:
Â                try:
Â                    url = f"https://jyotiflow-ai.onrender.com{endpoint['url']}"
Â 
Â                    if endpoint['method'] == 'GET':
Â                        response = await client.get(url)
Â                    else:
Â                        response = await client.post(url, json={})
Â 
Â                    endpoint_results[endpoint['business_function']] = {
Â                        "endpoint_accessible": response.status_code in [200, 401, 403, 422],
Â                        "status_code": response.status_code
Â                    }
Â 
Â                except Exception as endpoint_error:
Â                    endpoint_results[endpoint['business_function']] = {
Â                        "endpoint_accessible": False,
Â                        "error": str(endpoint_error)
Â                    }
Â 
Â        accessible_endpoints = sum(1 for result in endpoint_results.values() if result.get("endpoint_accessible", False))
Â        total_endpoints = len(endpoints_to_test)
Â        success_rate = (accessible_endpoints / total_endpoints) * 100
Â 
Â        return {
Â            "status": "passed" if success_rate > 75 else "failed",
Â            "message": "User management API endpoints tested",
Â            "success_rate": success_rate,
Â            "accessible_endpoints": accessible_endpoints,
Â            "total_endpoints": total_endpoints,
Â            "endpoint_results": endpoint_results
Â        }
Â 
Â    except Exception as e:
Â        return {"status": "failed", "error": f"User management API test failed: {str(e)}"}
""",
Â                    "expected_result": "User management API endpoints operational",
Â                    "timeout_seconds": 25
Â                }
Â            ]
Â        }
Â 

Â    async def generate_admin_services_tests(self) -> Dict[str, Any]:
Â        """Generate admin services tests - BUSINESS MANAGEMENT CRITICAL - Environment-configurable base URL with direct endpoint configuration"""
Â        return {
Â            "test_suite_name": "Admin Services",
Â            "test_category": "admin_services_critical",
Â            "description": "Critical tests for admin dashboard, analytics, settings, and management functions - Database Driven",
Â            "test_cases": [
Â                {
Â                    "test_name": "test_admin_authentication_endpoint",
Â                    "description": "Test admin authentication endpoint with environment-configurable base URL and direct endpoint configuration",
Â                    "test_type": "integration",
Â                    "priority": "high",
Â                    "test_code": """
import httpx
import asyncpg
import json
import os
import time
import uuid

async def test_admin_authentication_endpoint():
Â    \"\"\"Test admin authentication endpoint - environment-configurable base URL, direct endpoint configuration\"\"\"
Â    import httpx, time, os
Â    try:
Â        # Direct endpoint configuration (not from database)
Â        endpoint = "/api/auth/login"
Â        method = "POST"
Â        business_function = "Admin Authentication"
Â        test_data = {"email": "admin@jyotiflow.ai", "password": "Jyoti@2024!"}
Â        api_base_url = os.getenv('API_BASE_URL', 'https://jyotiflow-ai.onrender.com')
Â        expected_codes = [200, 401, 403, 422]
Â 
Â        # Execute HTTP request to actual endpoint
Â        url = api_base_url.rstrip('/') + '/' + endpoint.lstrip('/')
Â 
Â        try:
Â            async with httpx.AsyncClient(timeout=30.0) as client:
Â                start_time = time.time()
Â                print(f"ðŸŒ Making HTTP request to: {url}")
Â 
Â                if method == 'GET':
Â                    response = await client.get(url, params=test_data)
Â                elif method in ['POST', 'PUT', 'PATCH']:
Â                    response = await client.request(method, url, json=test_data)
Â                elif method == 'DELETE':
Â                    response = await client.delete(url)
Â                else:
Â                    response = await client.request(method, url)
Â 
Â                response_time_ms = int((time.time() - start_time) * 1000)
Â                status_code = response.status_code
Â                test_status = 'passed' if status_code in expected_codes else 'failed'
Â 
Â                print(f"ðŸ“Š Response: {status_code} ({response_time_ms}ms)")
Â 
Â        except Exception as http_error:
Â            print(f"âŒ HTTP request failed: {str(http_error)}")
Â            return {"status": "failed", "error": f"HTTP request failed: {str(http_error)}", "business_function": business_function}
Â 
Â        # Return test results (database storage handled by test execution engine)
Â        return {
Â            "status": test_status,
Â            "business_function": business_function,
Â            "execution_time_ms": response_time_ms,
Â            "details": {
Â                "status_code": status_code,
Â                "response_time_ms": response_time_ms,
Â                "url": url,
Â                "method": method,
Â                "endpoint": endpoint
Â            }
Â        }
Â    except Exception as e:
Â        return {"status": "failed", "error": f"Test failed: {str(e)}"}
""",
Â                    "expected_result": "Admin authentication endpoint operational (database-driven)",
Â                    "timeout_seconds": 30
Â                },
Â                {
Â                    "test_name": "test_admin_overview_endpoint",
Â                    "description": "Test admin overview endpoint with environment-configurable base URL and direct endpoint configuration",
Â                    "test_type": "integration",
Â                    "priority": "high",
Â                    "test_code": """
import httpx
import asyncpg
import json
import os
import time
import uuid


async def test_admin_overview_endpoint():
Â    \"\"\"Test admin overview endpoint - environment-configurable base URL, direct endpoint configuration\"\"\"
Â    import httpx, time, os
Â    try:
Â        # Direct endpoint configuration (not from database)
Â        endpoint = "/api/admin/analytics/overview"
Â        method = "GET"
Â        business_function = "Admin Optimization"
Â        test_data = {"timeframe": "7d", "metrics": ["users", "sessions", "revenue"]}
Â        api_base_url = os.getenv('API_BASE_URL', 'https://jyotiflow-ai.onrender.com')
Â        expected_codes = [200, 401, 403, 422]
Â 
Â        # Execute HTTP request to actual endpoint
Â        url = api_base_url.rstrip('/') + '/' + endpoint.lstrip('/')
Â 
Â        try:
Â            async with httpx.AsyncClient(timeout=30.0) as client:
Â                start_time = time.time()
Â                print(f"ðŸŒ Making HTTP request to: {url}")
Â 
Â                if method == 'GET':
Â                    response = await client.get(url, params=test_data)
Â                elif method in ['POST', 'PUT', 'PATCH']:
Â                    response = await client.request(method, url, json=test_data)
Â                elif method == 'DELETE':
Â                    response = await client.delete(url)
Â                else:
Â                    response = await client.request(method, url)
Â 
Â                response_time_ms = int((time.time() - start_time) * 1000)
Â                status_code = response.status_code
Â                test_status = 'passed' if status_code in expected_codes else 'failed'
Â 
Â                print(f"ðŸ“Š Response: {status_code} ({response_time_ms}ms)")
Â 
Â        except Exception as http_error:
Â            print(f"âŒ HTTP request failed: {str(http_error)}")
Â            return {"status": "failed", "error": f"HTTP request failed: {str(http_error)}", "business_function": business_function}
Â 
Â        # Return test results (database storage handled by test execution engine)
Â        return {
Â            "status": test_status,
Â            "business_function": business_function,
Â            "execution_time_ms": response_time_ms,
Â            "details": {
Â                "status_code": status_code,
Â                "response_time_ms": response_time_ms,
Â                "url": url,
Â                "method": method,
Â                "endpoint": endpoint
Â            }
Â        }
Â    except Exception as e:
Â        return {"status": "failed", "error": f"Test failed: {str(e)}"}
""",
Â                    "expected_result": "Admin overview endpoint operational (database-driven)",
Â                    "timeout_seconds": 30
Â                },
Â                {
Â                    "test_name": "test_admin_revenue_insights_endpoint",
Â                    "description": "Test admin revenue insights endpoint with environment-configurable base URL and direct endpoint configuration",
Â                    "test_type": "integration",
Â                    "priority": "high",
Â                    "test_code": """
import httpx
import asyncpg
import json
import os
import time
import uuid

async def test_admin_revenue_insights_endpoint():
Â    \"\"\"Test admin revenue insights endpoint - environment-configurable base URL, direct endpoint configuration\"\"\"
Â    import httpx, time, os
Â    try:
Â        # Direct endpoint configuration (not from database)
Â        endpoint = "/api/admin/analytics/revenue-insights"
Â        method = "GET"
Â        business_function = "Admin Monetization"
Â        test_data = {"period": "30d", "breakdown": ["daily", "source"]}
Â        api_base_url = os.getenv('API_BASE_URL', 'https://jyotiflow-ai.onrender.com')
Â        expected_codes = [200, 401, 403, 422]
Â 
Â        # Execute HTTP request to actual endpoint
Â        url = api_base_url.rstrip('/') + '/' + endpoint.lstrip('/')
Â 
Â        try:
Â            async with httpx.AsyncClient(timeout=30.0) as client:
Â                start_time = time.time()
Â                print(f"ðŸŒ Making HTTP request to: {url}")
Â 
Â                if method == 'GET':
Â                    response = await client.get(url, params=test_data)
Â                elif method in ['POST', 'PUT', 'PATCH']:
Â                    response = await client.request(method, url, json=test_data)
Â                elif method == 'DELETE':
Â                    response = await client.delete(url)
Â                else:
Â                    response = await client.request(method, url)
Â 
Â                response_time_ms = int((time.time() - start_time) * 1000)
Â                status_code = response.status_code
Â                test_status = 'passed' if status_code in expected_codes else 'failed'
Â 
Â                print(f"ðŸ“Š Response: {status_code} ({response_time_ms}ms)")
Â 
Â        except Exception as http_error:
Â            print(f"âŒ HTTP request failed: {str(http_error)}")
Â            return {"status": "failed", "error": f"HTTP request failed: {str(http_error)}", "business_function": business_function}
Â 
Â        # Return test results (database storage handled by test execution engine)
Â        return {
Â            "status": test_status,
Â            "business_function": business_function,
Â            "execution_time_ms": response_time_ms,
Â            "details": {
Â                "status_code": status_code,
Â                "response_time_ms": response_time_ms,
Â                "url": url,
Â                "method": method,
Â                "endpoint": endpoint
Â            }
Â        }
Â    except Exception as e:
Â        return {"status": "failed", "error": f"Test failed: {str(e)}"}
""",
Â                    "expected_result": "Admin revenue insights endpoint operational (database-driven)",
Â                    "timeout_seconds": 30
Â                },
Â                {
Â                    "test_name": "test_admin_analytics_endpoint",
Â                    "description": "Test admin analytics endpoint with environment-configurable base URL and direct endpoint configuration",
Â                    "test_type": "integration",
Â                    "priority": "high",
Â                    "test_code": """
import httpx
import asyncpg
import json
import os
import time
import uuid

async def test_admin_analytics_endpoint():
Â    \"\"\"Test admin analytics endpoint - environment-configurable base URL, direct endpoint configuration\"\"\"
Â    import httpx, time, os
Â    try:
Â        # Direct endpoint configuration (not from database)
Â        endpoint = "/api/admin/analytics/analytics"
Â        method = "GET"
Â        business_function = "Admin Stats"
Â        test_data = {"view": "dashboard", "filters": ["active_users", "revenue"]}
Â        api_base_url = os.getenv('API_BASE_URL', 'https://jyotiflow-ai.onrender.com')
Â        expected_codes = [200, 401, 403, 422]
Â 
Â        # Execute HTTP request to actual endpoint
Â        url = api_base_url.rstrip('/') + '/' + endpoint.lstrip('/')
Â 
Â        try:
Â            async with httpx.AsyncClient(timeout=30.0) as client:
Â                start_time = time.time()
Â                print(f"ðŸŒ Making HTTP request to: {url}")
Â 
Â                if method == 'GET':
Â                    response = await client.get(url, params=test_data)
Â                elif method in ['POST', 'PUT', 'PATCH']:
Â                    response = await client.request(method, url, json=test_data)
Â                elif method == 'DELETE':
Â                    response = await client.delete(url)
Â                else:
Â                    response = await client.request(method, url)
Â 
Â                response_time_ms = int((time.time() - start_time) * 1000)
Â                status_code = response.status_code
Â                test_status = 'passed' if status_code in expected_codes else 'failed'
Â 
Â                print(f"ðŸ“Š Response: {status_code} ({response_time_ms}ms)")
Â 
Â        except Exception as http_error:
Â            print(f"âŒ HTTP request failed: {str(http_error)}")
Â            return {"status": "failed", "error": f"HTTP request failed: {str(http_error)}", "business_function": business_function}
Â 
Â        # Return test results (database storage handled by test execution engine)
Â        return {
Â            "status": test_status,
Â            "business_function": business_function,
Â            "execution_time_ms": response_time_ms,
Â            "details": {
Â                "status_code": status_code,
Â                "response_time_ms": response_time_ms,
Â                "url": url,
Â                "method": method,
Â                "endpoint": endpoint
Â            }
Â        }
Â    except Exception as e:
Â        return {"status": "failed", "error": f"Test failed: {str(e)}"}
""",
Â                    "expected_result": "Admin analytics endpoint operational (database-driven)",
Â                    "timeout_seconds": 30
Â                }
Â            ]
Â        }

Â    async def generate_community_services_tests(self) -> Dict[str, Any]:
Â        """Generate community services tests - USER ENGAGEMENT CRITICAL"""
Â        return {
Â            "test_suite_name": "Community Services",
Â            "test_category": "community_services_critical",
Â            "description": "Tests for community features, follow-up systems, and user engagement services",
Â            "test_cases": [
Â                {
Â                    "test_name": "test_community_api_endpoints",
Â                    "description": "Test community and engagement API endpoints",
Â                    "test_type": "integration",
Â                    "priority": "medium",
Â                    "test_code": """
import httpx

async def test_community_api_endpoints():
Â    try:
Â        endpoints_to_test = [
Â            {"url": "/api/community", "method": "GET", "business_function": "Community Features"},
Â            {"url": "/api/followup/schedule", "method": "POST", "business_function": "Follow-up System"},
Â            {"url": "/api/donations", "method": "GET", "business_function": "Donations System"}
Â        ]
Â 
Â        endpoint_results = {}

Â 
Â        async with httpx.AsyncClient() as client:
Â            for endpoint in endpoints_to_test:
Â                try:
Â                    url = f"https://jyotiflow-ai.onrender.com{endpoint['url']}"
Â 
Â                    if endpoint['method'] == 'GET':
Â                        response = await client.get(url)
Â                    else:
Â                        response = await client.post(url, json={})
Â 
Â                    endpoint_results[endpoint['business_function']] = {
Â                        "endpoint_accessible": response.status_code in [200, 401, 403, 422],
Â                        "status_code": response.status_code
Â                    }
Â 
Â                except (httpx.TimeoutException, httpx.ReadTimeout, httpx.WriteTimeout, httpx.ConnectTimeout) as timeout_error:
Â                    # Handle timeout exceptions specifically with detailed context
Â                    endpoint_results[endpoint['business_function']] = {
Â                        "endpoint_accessible": False,
Â                        "error": f"Timeout: {str(timeout_error)}",
Â                        "error_type": "timeout",
Â                        "endpoint_url": endpoint['url'],
Â                        "method": endpoint['method']
Â                    }
Â                except Exception as endpoint_error:
Â                    # Handle generic exceptions with error type information
Â                    error_type = endpoint_error.__class__.__name__
Â                    endpoint_results[endpoint['business_function']] = {
Â                        "endpoint_accessible": False,
Â                        "error": str(endpoint_error),
Â                        "error_type": error_type,
Â                        "endpoint_url": endpoint['url'],
Â                        "method": endpoint['method']
Â                    }
Â 
Â        accessible_endpoints = sum(1 for result in endpoint_results.values() if result.get("endpoint_accessible", False))
Â        total_endpoints = len(endpoints_to_test)
Â        success_rate = (accessible_endpoints / total_endpoints) * 100
Â 
Â        return {
Â            "status": "passed" if success_rate > 60 else "failed",
Â            "message": "Community services API endpoints tested",
Â            "success_rate": success_rate,
Â            "accessible_endpoints": accessible_endpoints,
Â            "total_endpoints": total_endpoints,
Â            "endpoint_results": endpoint_results
Â        }
Â 
Â    except Exception as e:
Â        return {"status": "failed", "error": f"Community services API test failed: {str(e)}"}
""",
Â                    "expected_result": "Community services API endpoints operational",
Â                    "timeout_seconds": 20
Â                }
Â            ]
Â        }

Â    async def generate_notification_services_tests(self) -> Dict[str, Any]:
Â        """Generate notification services tests - USER COMMUNICATION CRITICAL"""
Â        return {
Â            "test_suite_name": "Notification Services",
Â            "test_category": "notification_services_critical",
Â            "description": "Tests for notification systems, alerts, and user communication services",
Â            "test_cases": [
Â                {
Â                    "test_name": "test_notification_api_endpoints",
Â                    "description": "Test notification API endpoints",
Â                    "test_type": "integration",
Â                    "priority": "medium",
Â                    "test_code": """
import httpx

async def test_notification_api_endpoints():
Â    try:
Â        endpoints_to_test = [
Â            {"url": "/api/notify", "method": "POST", "business_function": "Notification System"}
Â        ]
Â 
Â        endpoint_results = {}
Â 
Â        async with httpx.AsyncClient() as client:
Â            for endpoint in endpoints_to_test:
Â                try:
Â                    url = f"https://jyotiflow-ai.onrender.com{endpoint['url']}"
Â                    response = await client.post(url, json={})
Â 
Â                    endpoint_results[endpoint['business_function']] = {
Â                        "endpoint_accessible": response.status_code in [200, 401, 403, 422],
Â                        "status_code": response.status_code
Â                    }
Â 
Â                except (httpx.TimeoutException, httpx.ReadTimeout, httpx.WriteTimeout, httpx.ConnectTimeout) as timeout_error:
Â                    # Handle timeout exceptions specifically with detailed context
Â                    endpoint_results[endpoint['business_function']] = {
Â                        "endpoint_accessible": False,
Â                        "error": f"Timeout: {str(timeout_error)}",
Â                        "error_type": "timeout",
Â                        "endpoint_url": endpoint['url'],
Â                        "method": endpoint['method']
Â                    }
Â                except Exception as endpoint_error:
Â                    # Handle generic exceptions with error type information
Â                    error_type = endpoint_error.__class__.__name__
Â                    endpoint_results[endpoint['business_function']] = {
Â                        "endpoint_accessible": False,
Â                        "error": str(endpoint_error),
Â                        "error_type": error_type,
Â                        "endpoint_url": endpoint['url'],
Â                        "method": endpoint['method']
Â                    }
Â 
Â        accessible_endpoints = sum(1 for result in endpoint_results.values() if result.get("endpoint_accessible", False))
Â        total_endpoints = len(endpoints_to_test)
Â        success_rate = (accessible_endpoints / total_endpoints) * 100
Â 
Â        return {
Â            "status": "passed" if success_rate > 50 else "failed",
Â            "message": "Notification services API endpoints tested",
Â            "success_rate": success_rate,
Â            "accessible_endpoints": accessible_endpoints,
Â            "total_endpoints": total_endpoints,
Â            "endpoint_results": endpoint_results
Â        }
Â 
Â    except Exception as e:
Â        return {"status": "failed", "error": f"Notification services API test failed: {str(e)}"}
""",
Â                    "expected_result": "Notification services API endpoints operational",
Â                    "timeout_seconds": 15
Â                }
Â            ]
Â        }

Â    async def generate_analytics_monitoring_tests(self) -> Dict[str, Any]:
Â        """Generate analytics and monitoring services tests - BUSINESS INTELLIGENCE CRITICAL"""
Â        return {
Â            "test_suite_name": "Analytics & Monitoring Services",
Â            "test_category": "analytics_monitoring_critical",
Â            "description": "Tests for analytics, monitoring, session tracking, and business intelligence services",
Â            "test_cases": [
Â                {
Â                    "test_name": "test_analytics_monitoring_api_endpoints",
Â                    "description": "Test analytics and monitoring API endpoints",
Â                    "test_type": "integration",
Â                    "priority": "high",
Â                    "test_code": """
import httpx

async def test_analytics_monitoring_api_endpoints():
Â    try:
Â        endpoints_to_test = [
Â            {"url": "/api/monitoring/test-status", "method": "GET", "business_function": "Test Monitoring"},
Â            {"url": "/api/sessions/analytics", "method": "GET", "business_function": "Session Analytics"},
Â            {"url": "/api/admin/analytics/overview", "method": "GET", "business_function": "Business Analytics"}
Â        ]
Â 
Â        endpoint_results = {}
Â 
Â        async with httpx.AsyncClient() as client:
Â            for endpoint in endpoints_to_test:
Â                try:
Â                    url = f"https://jyotiflow-ai.onrender.com{endpoint['url']}"
Â                    response = await client.get(url)
Â 
Â                    endpoint_results[endpoint['business_function']] = {
Â                        "endpoint_accessible": response.status_code in [200, 401, 403, 422],
Â                        "status_code": response.status_code
Â                    }
Â 
Â                except Exception as endpoint_error:
Â                    endpoint_results[endpoint['business_function']] = {
Â                        "endpoint_accessible": False,
Â                        "error": str(endpoint_error)
Â                    }
Â 
Â        accessible_endpoints = sum(1 for result in endpoint_results.values() if result.get("endpoint_accessible", False))
Â        total_endpoints = len(endpoints_to_test)
Â        success_rate = (accessible_endpoints / total_endpoints) * 100
Â 
Â        return {
Â            "status": "passed" if success_rate > 70 else "failed",
Â            "message": "Analytics monitoring API endpoints tested",
Â            "success_rate": success_rate,
Â            "accessible_endpoints": accessible_endpoints,
Â            "total_endpoints": total_endpoints,
Â            "endpoint_results": endpoint_results
Â        }
Â 
Â    except Exception as e:
Â        return {"status": "failed", "error": f"Analytics monitoring API test failed: {str(e)}"}
""",
Â                    "expected_result": "Analytics monitoring API endpoints operational",
Â                    "timeout_seconds": 25
Â                }
Â            ]
Â        }

async def main() -> Dict[str, Any]:
Â    """
Â    Generate all test suites for the JyotiFlow AI platform.
Â 
Â    Returns:
Â        Dict containing all generated test suites
Â 
Â    Raises:
Â        TestGenerationError: If test suite generation fails
Â        SystemExit: If critical error occurs
Â    """
Â    try:
Â        generator = TestSuiteGenerator()
Â        test_suites = await generator.generate_all_test_suites()
Â 
Â        logger.info("ðŸ§ª COMPREHENSIVE TEST SUITE GENERATION COMPLETE")
Â        logger.info("=" * 60)
Â 
Â        total_tests = 0
Â        for suite_name, suite_data in test_suites.items():
Â            test_count = len(suite_data.get('test_cases', []))
Â            logger.info(f"âœ… {suite_data['test_suite_name']}: {test_count} tests")
Â            total_tests += test_count
Â 
Â        logger.info(f"ðŸŽ¯ Total Tests Generated: {total_tests}")
Â        logger.info("ðŸš€ Ready for execution via test runner")
Â 
Â        return test_suites
Â 
Â    except (TestGenerationError, DatabaseConnectionError) as known_error:
Â        logger.error(f"Known error during test generation: {known_error}")
Â        raise
Â    except Exception as unexpected_error:
Â        logger.critical(f"Unexpected error during test generation: {unexpected_error}")
Â        raise TestGenerationError(f"Unexpected error: {unexpected_error}") from unexpected_error

Â    async def generate_unit_tests(self) -> Dict[str, Any]:
Â        """Generate unit tests for individual components"""
Â        return {
Â            "test_suite_name": "Unit Tests",
Â            "test_category": "unit_tests",
Â            "description": "Unit tests for individual components and functions",
Â            "test_cases": [
Â                {
Â                    "test_name": "test_individual_components",
Â                    "description": "Test individual component functionality",
Â                    "test_type": "unit",
Â                    "priority": "medium",
Â                    "test_code": """
import httpx

async def test_individual_components():
Â    try:
Â        components_to_test = [
Â            {"url": "/api/health", "method": "GET", "component": "Health Check"},
Â            {"url": "/api/auth/status", "method": "GET", "component": "Auth Status"},
Â            {"url": "/api/services/status", "method": "GET", "component": "Services Status"}
Â        ]
Â 
Â        component_results = {}
Â 
Â        async with httpx.AsyncClient() as client:
Â            for component in components_to_test:
Â                try:
Â                    url = f"https://jyotiflow-ai.onrender.com{component['url']}"
Â                    response = await client.get(url)
Â 
Â                    component_results[component['component']] = {
Â                        "component_accessible": response.status_code in [200, 401, 403, 422],
Â                        "status_code": response.status_code
Â                    }
Â 
Â                except Exception as component_error:
Â                    component_results[component['component']] = {
Â                        "component_accessible": False,
Â                        "error": str(component_error)
Â                    }
Â 
Â        accessible_components = sum(1 for result in component_results.values() if result.get("component_accessible", False))
Â        total_components = len(components_to_test)
Â        success_rate = (accessible_components / total_components) * 100
Â 
Â        return {
Â            "status": "passed" if success_rate > 70 else "failed",
Â            "message": "Unit tests for individual components completed",
Â            "success_rate": success_rate,
Â            "accessible_components": accessible_components,
Â            "total_components": total_components,
Â            "component_results": component_results
Â        }
Â 
Â    except Exception as e:
Â        return {"status": "failed", "error": f"Unit tests failed: {str(e)}"}
""",
Â                    "expected_result": "Individual components functioning correctly",
Â                    "timeout_seconds": 20
Â                }
Â            ]
Â        }

Â    async def generate_end_to_end_tests(self) -> Dict[str, Any]:
Â        """Generate end-to-end tests for complete user workflows"""
Â        return {
Â            "test_suite_name": "End-to-End Tests",
Â            "test_category": "end_to_end_tests",
Â            "description": "End-to-end tests for complete user workflows",
Â            "test_cases": [
Â                {
Â                    "test_name": "test_complete_user_workflow",
Â                    "description": "Test complete user workflow from registration to service usage",
Â                    "test_type": "e2e",
Â                    "priority": "high",
Â                    "test_code": """
import httpx

async def test_complete_user_workflow():
Â    try:
Â        workflow_steps = [
Â            {"url": "/api/auth/register", "method": "POST", "step": "User Registration"},
Â            {"url": "/api/auth/login", "method": "POST", "step": "User Login"},
Â            {"url": "/api/services", "method": "GET", "step": "Service Discovery"},
Â            {"url": "/api/user/profile", "method": "GET", "step": "Profile Access"}
Â        ]
Â 
Â        workflow_results = {}
Â 
Â        async with httpx.AsyncClient() as client:
Â            for step in workflow_steps:
Â                try:
Â                    url = f"https://jyotiflow-ai.onrender.com{step['url']}"
Â 
Â                    if step['method'] == 'GET':
Â                        response = await client.get(url)
Â                    else:
Â                        response = await client.post(url, json={})
Â 
Â                    workflow_results[step['step']] = {
Â                        "step_accessible": response.status_code in [200, 401, 403, 422],
Â                        "status_code": response.status_code
Â                    }
Â 
Â                except Exception as step_error:
Â                    workflow_results[step['step']] = {
Â                        "step_accessible": False,
Â                        "error": str(step_error)
Â                    }
Â 
Â        accessible_steps = sum(1 for result in workflow_results.values() if result.get("step_accessible", False))
Â        total_steps = len(workflow_steps)
Â        success_rate = (accessible_steps / total_steps) * 100
Â 
Â        return {
Â            "status": "passed" if success_rate > 75 else "failed",
Â            "message": "End-to-end workflow tests completed",
Â            "success_rate": success_rate,
Â            "accessible_steps": accessible_steps,
Â            "total_steps": total_steps,
Â            "workflow_results": workflow_results
Â        }
Â 
Â    except Exception as e:
Â        return {"status": "failed", "error": f"End-to-end tests failed: {str(e)}"}
""",
Â                    "expected_result": "Complete user workflow functioning correctly",
Â                    "timeout_seconds": 30
Â                }
Â            ]
Â        }

Â    async def generate_load_tests(self) -> Dict[str, Any]:
Â        """Generate load tests for performance under stress"""
Â        return {
Â            "test_suite_name": "Load Tests",
Â            "test_category": "load_tests",
Â            "description": "Load tests for performance under stress conditions",
Â            "test_cases": [
Â                {
Â                    "test_name": "test_system_load_capacity",
Â                    "description": "Test system performance under load",
Â                    "test_type": "load",
Â                    "priority": "medium",
Â                    "test_code": """
import httpx
import asyncio

async def test_system_load_capacity():
Â    try:
Â        load_endpoints = [
Â            {"url": "/api/health", "concurrent_requests": 5},
Â            {"url": "/api/services", "concurrent_requests": 3},
Â            {"url": "/api/auth/status", "concurrent_requests": 3}
Â        ]
Â 
Â        load_results = {}
Â 
Â        async with httpx.AsyncClient() as client:
Â            for endpoint in load_endpoints:
Â                try:
Â                    url = f"https://jyotiflow-ai.onrender.com{endpoint['url']}"
Â 
Â                    # Create concurrent requests
Â                    tasks = []
Â                    for _ in range(endpoint['concurrent_requests']):
Â                        tasks.append(client.get(url))
Â 
Â                    responses = await asyncio.gather(*tasks, return_exceptions=True)
Â 
Â                    successful_requests = sum(1 for r in responses if not isinstance(r, Exception) and r.status_code in [200, 401, 403, 422])
Â                    total_requests = len(responses)
Â 
Â                    load_results[endpoint['url']] = {
Â                        "successful_requests": successful_requests,
Â                        "total_requests": total_requests,
Â                        "success_rate": (successful_requests / total_requests) * 100 if total_requests > 0 else 0
Â                    }
Â 
Â                except Exception as load_error:
Â                    load_results[endpoint['url']] = {
Â                        "successful_requests": 0,
Â                        "total_requests": endpoint['concurrent_requests'],
Â                        "success_rate": 0,
Â                        "error": str(load_error)
Â                    }
Â 
Â        overall_success_rate = sum(result.get("success_rate", 0) for result in load_results.values()) / len(load_results) if load_results else 0
Â 
Â        return {
Â            "status": "passed" if overall_success_rate > 70 else "failed",
Â            "message": "Load tests completed",
Â            "overall_success_rate": overall_success_rate,
Â            "load_results": load_results
Â        }
Â 
Â    except Exception as e:
Â        return {"status": "failed", "error": f"Load tests failed: {str(e)}"}
""",
Â                    "expected_result": "System handles load appropriately",
Â                    "timeout_seconds": 45
Â                }
Â            ]
Â        }

Â    async def generate_spiritual_tests(self) -> Dict[str, Any]:
Â        """Generate spiritual services tests - alias for generate_spiritual_services_tests"""
Â        return await self.generate_spiritual_services_tests()

if __name__ == "__main__":
Â    test_suites = asyncio.run(main())