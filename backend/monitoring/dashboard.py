"""
ðŸ“Š MONITORING DASHBOARD - Real-time integration monitoring for JyotiFlow admin
Integrates seamlessly with existing admin dashboard UI .
"""

import json
import asyncio
import asyncpg
import uuid
import os
from datetime import datetime, timezone
from typing import Dict, List, Any

from fastapi import APIRouter, WebSocket, WebSocketDisconnect, Depends, HTTPException, status
from db import db_manager
import logging
logger = logging.getLogger(__name__)

# Database connection managed through db_manager
from pydantic import BaseModel, Field, model_validator
from typing import Optional, Dict, Any

class StandardResponse(BaseModel):
Â    status: str
Â    message: str
Â    data: Dict[str, Any] = Field(default_factory=dict)
Â    success: bool = Field(default=True, description="Backward compatibility field")
Â 
Â    @model_validator(mode='after')
Â    def set_success_from_status(self) -> 'StandardResponse':
Â        """Set success field based on status for backward compatibility"""
Â        self.success = self.status == "success"
Â        return self
Â 
Â    model_config = {
Â        "extra": "forbid",
Â        "json_schema_extra": {
Â            "examples": [
Â                {
Â                    "status": "success",
Â                    "message": "Operation completed",
Â                    "data": {},
Â                    "success": True
Â                }
Â            ]
Â        }
Â    }

class LegacyStandardResponse(BaseModel):
Â    """Legacy response format for backward compatibility"""
Â    success: bool
Â    message: str
Â    data: dict = {}
Â 
Â    @classmethod
Â    def from_standard(cls, response: StandardResponse):
Â        """Convert from new format to legacy format"""
Â        return cls(
Â            success=response.status == "success",
Â            message=response.message,
Â            data=response.data
Â        )
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from deps import get_current_admin_dependency

from .integration_monitor import integration_monitor, IntegrationStatus
from .business_validator import BusinessLogicValidator

# Create router for monitoring endpoints
router = APIRouter(prefix="/api/monitoring", tags=["monitoring"])

# WebSocket manager for real-time updates
class ConnectionManager:
Â    def __init__(self):
Â        self.active_connections: List[WebSocket] = []
Â 
Â    async def connect(self, websocket: WebSocket):
Â        await websocket.accept()
Â        self.active_connections.append(websocket)
Â 
Â    def disconnect(self, websocket: WebSocket):
Â        try:
Â            self.active_connections.remove(websocket)
Â        except ValueError:
Â            # WebSocket was not in the list, ignore
Â            pass
Â 
Â    async def broadcast(self, message: dict):
Â        """Broadcast message to all connected admin clients"""
Â        dead_connections = []
Â        for connection in self.active_connections[:]:  # Create a copy to iterate safely
Â            try:
Â                await connection.send_json(message)
Â            except WebSocketDisconnect:
Â                logger.info("WebSocket client disconnected during broadcast")
Â                dead_connections.append(connection)
Â            except (ConnectionResetError, ConnectionAbortedError, OSError) as conn_error:
Â                logger.warning(f"Connection closed during broadcast: {conn_error}")
Â                dead_connections.append(connection)
Â            except Exception as e:
Â                logger.error(f"Unexpected error broadcasting to client: {e}")
Â                dead_connections.append(connection)
Â 
Â        # Remove dead connections
Â        for dead_conn in dead_connections:
Â            self.disconnect(dead_conn)

connection_manager = ConnectionManager()

class MonitoringDashboard:
Â    """
Â    Real-time monitoring dashboard that integrates with
Â    existing JyotiFlow admin interface.
Â    """
Â 
Â    def __init__(self):
Â        self.business_validator = BusinessLogicValidator()
Â 
Â    async def get_dashboard_data(self) -> Dict:
Â        """Get comprehensive dashboard data for admin interface - database-driven"""
Â        try:
Â            # Get system health from database
Â            system_health = await self._get_system_health_from_db()
Â 
Â            # Get recent sessions
Â            recent_sessions = await self._get_recent_sessions()
Â 
Â            # Get integration statistics
Â            integration_stats = await self._get_integration_statistics()
Â 
Â            # Get critical issues
Â            critical_issues = await self._get_critical_issues()
Â 
Â            # Get social media health
Â            social_media_health = await self._get_social_media_health()
Â 
Â            # Calculate overall metrics
Â            overall_metrics = await self._calculate_overall_metrics()
Â 
Â            # Calculate per-integration metrics for frontend display
Â            integration_metrics = await self._calculate_integration_metrics()
Â 
Â            # Get active sessions count from database
Â            active_sessions_count = await self._get_active_sessions_count()
Â 
Â            dashboard_data = {
Â                "timestamp": datetime.now(timezone.utc).isoformat(),
Â                "system_health": system_health,
Â                "active_sessions": active_sessions_count,
Â                "recent_sessions": recent_sessions,
Â                "integration_statistics": integration_stats,
Â                "critical_issues": critical_issues,
Â                "social_media_health": social_media_health,
Â                "overall_metrics": overall_metrics,
Â                "metrics": integration_metrics,
Â                "alerts": await self._get_active_alerts()
Â            }
Â 
Â            return dashboard_data
Â 
Â        except Exception as e:
Â            logger.error(f"âŒ Failed to get dashboard data: {e}")
Â            return {
Â                "error": str(e),
Â                "timestamp": datetime.now(timezone.utc).isoformat()
Â            }
Â 
Â    async def get_session_details(self, session_id: str) -> Dict:
Â        """Get detailed validation report for a specific session"""
Â        try:
Â            conn = await db_manager.get_connection()
Â            try:
Â                # Get session data
Â                session_data = await conn.fetchrow("""
Â                    SELECT * FROM validation_sessions
Â                    WHERE session_id = $1
Â                """, session_id)
Â 
Â                if not session_data:
Â                    return {"error": "Session not found"}
Â 
Â                # Get integration validations
Â                validations = await conn.fetch("""
Â                    SELECT * FROM integration_validations
Â                    WHERE session_id = $1
Â                    ORDER BY validation_time
Â                """, session_id)
Â 
Â                # Get business logic issues
Â                issues = await conn.fetch("""
Â                    SELECT * FROM business_logic_issues
Â                    WHERE session_id = $1
Â                    ORDER BY created_at
Â                """, session_id)
Â 
Â                # Get context flow
Â                context_flow = None
Â                if session_id in integration_monitor.active_sessions:
Â                    context_flow = await integration_monitor.context_tracker.get_context_flow_report(session_id)
Â 
Â                return {
Â                    "session": dict(session_data),
Â                    "validations": [dict(v) for v in validations],
Â                    "issues": [dict(i) for i in issues],
Â                    "context_flow": context_flow,
Â                    "recommendations": await self._generate_session_recommendations(session_id)
Â                }
Â            finally:
Â                await db_manager.release_connection(conn)
Â 
Â        except Exception as e:
Â            logger.error(f"âŒ Failed to get session details: {e}")
Â            return {"error": str(e)}
Â 
Â    async def get_integration_health_details(self, integration_point: str) -> Dict:
Â        """Get detailed health information for a specific integration"""
Â        try:
Â            conn = await db_manager.get_connection()
Â            try:
Â                # Get recent performance metrics
Â                performance = await conn.fetch("""
Â                    SELECT
Â                        DATE_TRUNC('hour', validation_time) as hour,
Â                        AVG(CAST(actual_value->>'duration_ms' AS INTEGER)) as avg_duration,
Â                        COUNT(*) as total_calls,
Â                        SUM(CASE WHEN status = 'success' THEN 1 ELSE 0 END) as successful_calls
Â                    FROM integration_validations
Â                    WHERE integration_name = $1
Â                    AND validation_time > NOW() - INTERVAL '24 hours'
Â                    GROUP BY hour
Â                    ORDER BY hour DESC
Â                """, integration_point)
Â 
Â                # Get recent errors
Â                recent_errors = await conn.fetch("""
Â                    SELECT error_message, COUNT(*) as count, MAX(validation_time) as last_occurred
Â                    FROM integration_validations
Â                    WHERE integration_name = $1
Â                    AND status = 'failed'
Â                    AND validation_time > NOW() - INTERVAL '24 hours'
Â                    AND error_message IS NOT NULL
Â                    GROUP BY error_message
Â                    ORDER BY count DESC
Â                    LIMIT 5
Â                """, integration_point)
Â 
Â                # Get auto-fix statistics
Â                auto_fix_stats = await conn.fetchrow("""
Â                    SELECT
Â                        COUNT(*) as total_issues,
Â                        SUM(CASE WHEN auto_fixed = true THEN 1 ELSE 0 END) as auto_fixed_count
Â                    FROM integration_validations
Â                    WHERE integration_name = $1
Â                    AND status = 'failed'
Â                    AND validation_time > NOW() - INTERVAL '7 days'
Â                """, integration_point)
Â 
Â                return {
Â                    "integration_point": integration_point,
Â                    "performance_history": [dict(p) for p in performance],
Â                    "recent_errors": [dict(e) for e in recent_errors],
Â                    "auto_fix_effectiveness": {
Â                        "total_issues": auto_fix_stats["total_issues"] if auto_fix_stats else 0,
Â                        "auto_fixed": auto_fix_stats["auto_fixed_count"] if auto_fix_stats else 0,
Â                        "success_rate": (
Â                            auto_fix_stats["auto_fixed_count"] / auto_fix_stats["total_issues"] * 100
Â                            if (auto_fix_stats and
Â                                auto_fix_stats.get("total_issues") is not None and
Â                                auto_fix_stats.get("total_issues") > 0 and
Â                                auto_fix_stats.get("auto_fixed_count") is not None)
Â                            else 0
Â                        )
Â                    }
Â                }
Â            finally:
Â                await db_manager.release_connection(conn)
Â 
Â        except Exception as e:
Â            logger.error(f"âŒ Failed to get integration health details: {e}")
Â            return {"error": str(e)}
Â 
Â    async def trigger_validation_test(self, test_type: str) -> Dict:
Â        """Trigger a validation test for debugging"""
Â        try:
Â            if test_type == "full_flow":
Â                # Create a test session
Â                test_session_id = f"test_{datetime.now(timezone.utc).timestamp()}"
Â                test_context = {
Â                    "session_id": test_session_id,
Â                    "user_id": 0,  # Test user
Â                    "birth_details": {
Â                        "date": "1990-01-15",
Â                        "time": "14:30",
Â                        "location": "Chennai, India"
Â                    },
Â                    "spiritual_question": "What does my career future hold?",
Â                    "service_type": "comprehensive_reading"
Â                }
Â 
Â                # Start monitoring
Â                await integration_monitor.start_session_monitoring(
Â                    test_session_id,
Â                    test_context["user_id"],
Â                    test_context["birth_details"],
Â                    test_context["spiritual_question"],
Â                    test_context["service_type"]
Â                )
Â 
Â                # Run validations
Â                # This would trigger actual API calls in production
Â                return StandardResponse(
Â                    status="success",
Â                    message="Validation test initiated",
Â                    data={"test_session_id": test_session_id}
Â                )
Â 
Â            elif test_type == "social_media":
Â                # Test social media credentials
Â                social_validator = integration_monitor.validators.get("social_media")
Â                if social_validator:
Â                    test_result = await social_validator.test_all_platforms()
Â                    return test_result
Â                else:
Â                    return StandardResponse(
Â                        status="error",
Â                        message="Social media validator not found",
Â                        data={}
Â                    )
Â 
Â            else:
Â                return StandardResponse(
Â                    status="error",
Â                    message=f"Unknown test type: {test_type}",
Â                    data={}
Â                )
Â 
Â        except Exception as e:
Â            logger.error(f"âŒ Failed to trigger validation test: {e}")
Â            return StandardResponse(
Â                status="error",
Â                message=str(e),
Â                data={}
Â            )
Â 
Â    # Private helper methods
Â    async def _get_recent_sessions(self) -> List[Dict]:
Â        """Get recent session summaries from actual sessions table"""
Â        try:
Â            from db import db_manager
Â            conn = await db_manager.get_connection()
Â            try:
Â                # Query actual sessions table that exists in our schema
Â                sessions = await conn.fetch("""
Â                    SELECT
Â                        s.id AS session_id,
Â                        s.user_id,
Â                        s.user_email,
Â                        s.service_type,
Â                        s.status,
Â                        s.created_at as started_at,
Â                        s.updated_at as completed_at,
Â                        s.duration_minutes,
Â                        s.credits_used,
Â                        CASE
Â                            WHEN s.status = 'completed' THEN 'success'
Â                            WHEN s.status = 'active' THEN 'running'
Â                            ELSE 'failed'
Â                        END as overall_status
Â                    FROM sessions s
Â                    WHERE s.created_at > NOW() - INTERVAL '1 hour'
Â                    ORDER BY s.created_at DESC
Â                    LIMIT 20
Â                """)
Â 
Â                return [dict(s) for s in sessions]
Â            finally:
Â                await db_manager.release_connection(conn)
Â 
Â        except Exception as e:
Â            logger.error(f"Failed to get recent sessions: {e}")
Â            # Return empty list instead of mock data
Â            return []
Â 
Â    async def _get_integration_statistics(self) -> Dict:
Â        """Get integration performance statistics
Â 
Â        Note: Consider creating a database view for even better maintainability:
Â 
Â        CREATE OR REPLACE VIEW integration_metrics_24h AS
Â        SELECT
Â            session_id,
Â            integration_name,
Â            status,
Â            CASE
Â                WHEN actual_value IS NOT NULL AND actual_value->>'duration_ms' IS NOT NULL
Â                THEN (actual_value->>'duration_ms')::INTEGER
Â                ELSE NULL
Â            END as duration_ms,
Â            validation_time
Â        FROM integration_validations
Â        WHERE validation_time > NOW() - INTERVAL '24 hours';
Â 
Â        Then queries would simply be:
Â        - SELECT ... FROM integration_metrics_24h
Â        - SELECT ... FROM integration_metrics_24h GROUP BY integration_name
Â        """
Â        try:
Â            from db import db_manager
Â            conn = await db_manager.get_connection()
Â            try:
Â                # Get session statistics from actual sessions table
Â                stats = await conn.fetchrow("""
Â                        SELECT
Â                        COUNT(*) as total_sessions,
Â                        AVG(duration_minutes * 60 * 1000) as avg_response_time_ms, -- Convert to milliseconds
Â                        COUNT(CASE WHEN status = 'completed' THEN 1 END) as successful_sessions,
Â                        COUNT(CASE WHEN status = 'active' THEN 1 END) as active_sessions
Â                    FROM sessions
Â                    WHERE created_at > NOW() - INTERVAL '24 hours'
Â                """)
Â 
Â                # Get per-service type statistics
Â                by_service = await conn.fetch("""
Â                    SELECT
Â                        service_type as integration_name,
Â                        COUNT(*) as total_calls,
Â                        COUNT(CASE WHEN status = 'completed' THEN 1 END) as successful_calls,
Â                        AVG(duration_minutes * 60 * 1000) as avg_duration_ms
Â                    FROM sessions
Â                    WHERE created_at > NOW() - INTERVAL '24 hours'
Â                    GROUP BY service_type
Â                """)
Â 
Â                return {
Â                    "overall": dict(stats) if stats else {},
Â                    "by_integration": [dict(i) for i in by_service]
Â                }
Â            finally:
Â                await db_manager.release_connection(conn)
Â 
Â        except Exception as e:
Â            logger.error(f"Failed to get integration statistics: {e}")
Â            return {"overall": {}, "by_integration": []}
Â 
Â    async def _get_critical_issues(self) -> List[Dict]:
Â        """Get current critical issues from monitoring alerts table"""
Â        try:
Â            from db import db_manager
Â            conn = await db_manager.get_connection()
Â            try:
Â                # Check if monitoring_alerts table exists from our migrations
Â                table_exists = await conn.fetchval("""
Â                    SELECT EXISTS(
Â                        SELECT 1 FROM information_schema.tables
Â                        WHERE table_name = 'monitoring_alerts' AND table_schema = 'public'
Â                    )
Â                """)
Â 
Â                if table_exists:
Â                    issues = await conn.fetch("""
Â                        SELECT
Â                            id,
Â                            alert_type as type,
Â                            severity,
Â                            message,
Â                            details,
Â                            created_at as timestamp,
Â                            acknowledged
Â                        FROM monitoring_alerts
Â                        WHERE severity IN ('critical', 'high')
Â                        AND acknowledged = false
Â                        AND created_at > NOW() - INTERVAL '24 hours'
Â                        ORDER BY created_at DESC
Â                        LIMIT 10
Â                    """)
Â                    return [dict(i) for i in issues]
Â                else:
Â                    # Return empty list if table doesn't exist yet
Â                    return []
Â 
Â            finally:
Â                await db_manager.release_connection(conn)
Â 
Â        except Exception as e:
Â            logger.error(f"Failed to get critical issues: {e}")
Â            return []
Â 
Â    async def _get_social_media_health(self) -> Dict:
Â        """Get social media integration health status from platform_settings table"""
Â        try:
Â            from db import db_manager
Â            conn = await db_manager.get_connection()
Â            try:
Â                # Check if platform_settings table exists
Â                table_exists = await conn.fetchval("""
Â                    SELECT EXISTS(
Â                        SELECT 1 FROM information_schema.tables
Â                        WHERE table_name = 'platform_settings' AND table_schema = 'public'
Â                    )
Â                """)
Â 
Â                if table_exists:
Â                    # Get platform settings
Â                    platforms = await conn.fetch("""
Â                        SELECT
Â                            key,
Â                            value,
Â                            created_at,
Â                            updated_at
Â                        FROM platform_settings
Â                        WHERE key LIKE '%social%' OR key LIKE '%facebook%' OR key LIKE '%instagram%'
Â                        ORDER BY updated_at DESC
Â                    """)
Â 
Â                    # Check if social_media_validation_log exists
Â                    validation_log_exists = await conn.fetchval("""
Â                        SELECT EXISTS(
Â                            SELECT 1 FROM information_schema.tables
Â                            WHERE table_name = 'social_media_validation_log' AND table_schema = 'public'
Â                        )
Â                    """)
Â 
Â                    recent_activity = []
Â                    if validation_log_exists:
Â                        recent_activity = await conn.fetch("""
Â                            SELECT
Â                                platform,
Â                                validation_type,
Â                                status,
Â                                COUNT(*) as count
Â                            FROM social_media_validation_log
Â                            WHERE created_at > NOW() - INTERVAL '24 hours'
Â                            GROUP BY platform, validation_type, status
Â                        """)
Â 
Â                    return {
Â                        "platform_status": [dict(p) for p in platforms],
Â                        "recent_activity": [dict(r) for r in recent_activity],
Â                        "errors": []  # Will be populated when we have social media posting data
Â                    }
Â                else:
Â                    return {
Â                        "platform_status": [],
Â                        "recent_activity": [],
Â                        "errors": []
Â                    }
Â 
Â            finally:
Â                await db_manager.release_connection(conn)
Â 
Â        except Exception as e:
Â            logger.error(f"Failed to get social media health: {e}")
Â            return {
Â                "platform_status": [],
Â                "recent_activity": [],
Â                "errors": []
Â            }
Â 
Â    async def _calculate_overall_metrics(self) -> Dict:
Â        """Calculate overall system metrics from sessions table"""
Â        try:
Â            from db import db_manager
Â            conn = await db_manager.get_connection()
Â            try:
Â                # Get success rate from sessions
Â                success_rate = await conn.fetchrow("""
Â                    SELECT
Â                        COUNT(CASE WHEN status = 'completed' THEN 1 END)::float /
Â                        NULLIF(COUNT(*), 0) * 100 as success_rate
Â                    FROM sessions
Â                    WHERE created_at > NOW() - INTERVAL '24 hours'
Â                """)
Â 
Â                # Get average session duration
Â                avg_duration = await conn.fetchrow("""
Â                    SELECT
Â                        AVG(duration_minutes * 60) as avg_duration_seconds
Â                    FROM sessions
Â                    WHERE duration_minutes IS NOT NULL
Â                    AND created_at > NOW() - INTERVAL '24 hours'
Â                """)
Â 
Â                # Get system health score based on session completion rate
Â                total_sessions = await conn.fetchval("""
Â                    SELECT COUNT(*) FROM sessions
Â                    WHERE created_at > NOW() - INTERVAL '24 hours'
Â                """)
Â 
Â                return {
Â                    "success_rate": float(success_rate["success_rate"] or 0) if success_rate else 0,
Â                    "avg_session_duration": float(avg_duration["avg_duration_seconds"] or 0) if avg_duration else 0,
Â                    "total_sessions_24h": total_sessions or 0,
Â                    "quality_scores": {
Â                        "system_health": min(float(success_rate["success_rate"] or 0), 100) if success_rate else 0,
Â                        "uptime": 99.5  # Placeholder - could be calculated from monitoring data
Â                    }
Â                }
Â            finally:
Â                await db_manager.release_connection(conn)
Â 
Â        except Exception as e:
Â            logger.error(f"Failed to calculate overall metrics: {e}")
Â            return {
Â                "success_rate": 0,
Â                "avg_session_duration": 0,
Â                "quality_scores": {}
Â            }
Â 
Â    async def _calculate_integration_metrics(self) -> Dict:
Â        """Calculate per-integration success rates and response times from sessions table"""
Â        try:
Â            from db import db_manager
Â            conn = await db_manager.get_connection()
Â            try:
Â                # Get per-service type metrics from sessions table
Â                integration_stats = await conn.fetch("""
Â                    SELECT
Â                        service_type as integration_name,
Â                        COUNT(*) as total_validations,
Â                        COUNT(CASE WHEN status = 'completed' THEN 1 END) as successful_validations,
Â                        ROUND(
Â                            (COUNT(CASE WHEN status = 'completed' THEN 1 END)::numeric /
Â                            NULLIF(COUNT(*), 0) * 100)::numeric, 1
Â                        ) as success_rate,
Â                        ROUND(AVG(duration_minutes * 60 * 1000)::numeric) as avg_response_time_ms
Â                    FROM sessions
Â                    WHERE created_at > NOW() - INTERVAL '24 hours'
Â                    AND service_type IS NOT NULL
Â                    GROUP BY service_type
Â                    ORDER BY service_type
Â                """)
Â 
Â                success_rates = {}
Â                avg_response_times = {}
Â 
Â                for row in integration_stats:
Â                    integration_name = row['integration_name']
Â                    success_rates[integration_name] = float(row['success_rate'] or 0)
Â                    avg_response_times[integration_name] = int(row['avg_response_time_ms'] or 0)
Â 
Â                # Add common integration points that we expect to monitor
Â                common_integrations = [
Â                    'prokerala', 'rag_knowledge', 'openai_guidance',
Â                    'elevenlabs_voice', 'did_avatar', 'social_media'
Â                ]
Â 
Â                # Only add fallback zeros for common integrations that don't have data
Â                for integration in common_integrations:
Â                    if integration not in success_rates:
Â                        success_rates[integration] = 0.0
Â                        avg_response_times[integration] = 0
Â 
Â                return {
Â                    "success_rates": success_rates,
Â                    "avg_response_times": avg_response_times
Â                }
Â 
Â            finally:
Â                await db_manager.release_connection(conn)
Â 
Â        except Exception as e:
Â            logger.error(f"Failed to calculate integration metrics: {e}")
Â            # Return fallback data for common integrations
Â            common_integrations = [
Â                'prokerala', 'rag_knowledge', 'openai_guidance',
Â                'elevenlabs_voice', 'did_avatar', 'social_media'
Â            ]
Â            return {
Â                "success_rates": {integration: 0.0 for integration in common_integrations},
Â                "avg_response_times": {integration: 0 for integration in common_integrations}
Â            }

Â    async def _get_active_alerts(self) -> List[Dict]:
Â        """Get active alerts for admin attention"""
Â        alerts = []
Â 
Â        try:
Â            from db import db_manager
Â            conn = await db_manager.get_connection()
Â            try:
Â                # Check if monitoring_alerts table exists
Â                table_exists = await conn.fetchval("""
Â                    SELECT EXISTS(
Â                        SELECT 1 FROM information_schema.tables
Â                        WHERE table_name = 'monitoring_alerts' AND table_schema = 'public'
Â                    )
Â                """)
Â 
Â                if table_exists:
Â                    # Get alerts from monitoring_alerts table
Â                    alert_data = await conn.fetch("""
Â                        SELECT
Â                            alert_type as type,
Â                            severity,
Â                            message,
Â                            created_at as timestamp,
Â                            acknowledged
Â                        FROM monitoring_alerts
Â                        WHERE acknowledged = false
Â                        AND created_at > NOW() - INTERVAL '24 hours'
Â                        ORDER BY
Â                            CASE severity
Â                                WHEN 'critical' THEN 1
Â                                WHEN 'high' THEN 2
Â                                WHEN 'medium' THEN 3
Â                                ELSE 4
Â                            END,
Â                            created_at DESC
Â                        LIMIT 10
Â                    """)
Â 
Â                    for alert in alert_data:
Â                        alerts.append(dict(alert))
Â                else:
Â                    # Generate basic alerts from session data
Â                    error_rate = await conn.fetchrow("""
Â                        SELECT
Â                            CASE
Â                                WHEN COUNT(*) = 0 THEN 0
Â                                ELSE COUNT(CASE WHEN status != 'completed' THEN 1 END)::float / COUNT(*) * 100
Â                            END as error_rate
Â                        FROM sessions
Â                        WHERE created_at > NOW() - INTERVAL '1 hour'
Â                    """)
Â 
Â                    if error_rate and error_rate["error_rate"] > 20:
Â                        alerts.append({
Â                            "type": "warning",
Â                            "severity": "high" if error_rate["error_rate"] > 50 else "medium",
Â                            "message": f"High session failure rate: {error_rate['error_rate']:.1f}%",
Â                            "timestamp": datetime.now(timezone.utc).isoformat(),
Â                            "acknowledged": False
Â                        })
Â 
Â            finally:
Â                await db_manager.release_connection(conn)
Â 
Â            return alerts
Â 
Â        except Exception as e:
Â            logger.error(f"Failed to get active alerts: {e}")
Â            return []
Â 
Â    async def _get_system_health_from_db(self) -> Dict:
Â        """Get system health status from database metrics"""
Â        logger.info("ðŸ”„ Getting system health from database...")
Â        try:
Â            from db import db_manager
Â            conn = await db_manager.get_connection()
Â            try:
Â                # Calculate system health based on session success rates
Â                health_metrics = await conn.fetchrow("""
Â                    SELECT
Â                        COUNT(*) as total_sessions,
Â                        COUNT(CASE WHEN status = 'completed' THEN 1 END) as successful_sessions,
Â                        COUNT(CASE WHEN status = 'active' THEN 1 END) as active_sessions,
Â                        COUNT(CASE WHEN status = 'failed' THEN 1 END) as failed_sessions
Â                    FROM sessions
Â                    WHERE created_at > NOW() - INTERVAL '1 hour'
Â                """)
Â 
Â                if health_metrics and health_metrics['total_sessions'] > 0:
Â                    success_rate = (health_metrics['successful_sessions'] / health_metrics['total_sessions']) * 100
Â 
Â                    # Determine system status based on success rate
Â                    if success_rate >= 95:
Â                        system_status = "healthy"
Â                    elif success_rate >= 80:
Â                        system_status = "degraded"
Â                    else:
Â                        system_status = "critical"
Â                else:
Â                    system_status = "healthy"  # No recent activity
Â 
Â                # Get integration points from database - try integration_validations table first
Â                integration_points = {}
Â 
Â                # Check if integration_validations table exists and has data
Â                try:
Â                    integration_data = await conn.fetch("""
Â                        SELECT
Â                            integration_name,
Â                            COUNT(*) as total_validations,
Â                            COUNT(CASE WHEN status = 'success' THEN 1 END) as successful_validations,
Â                            AVG(response_time_ms) as avg_response_time,
Â                            MAX(validation_time) as last_validation
Â                        FROM integration_validations
Â                        WHERE validation_time > NOW() - INTERVAL '24 hours'
Â                        GROUP BY integration_name
Â                    """)
Â 
Â                    # Process integration validations data
Â                    for integration in integration_data:
Â                        name = integration['integration_name']
Â                        success_rate = (integration['successful_validations'] / integration['total_validations']) * 100 if integration['total_validations'] > 0 else 0
Â 
Â                        integration_points[name] = {
Â                            "status": "healthy" if success_rate >= 95 else "warning" if success_rate >= 80 else "error",
Â                            "success_rate": round(success_rate, 1),
Â                            "total_validations": integration['total_validations'],
Â                            "latency_ms": int(integration['avg_response_time'] or 0),
Â                            "last_check": integration['last_validation'].isoformat() if integration['last_validation'] else datetime.now(timezone.utc).isoformat()
Â                        }
Â 
Â                except Exception as table_error:
Â                    # integration_validations table doesn't exist or has no data
Â                    # Create sample integration data from platform_settings and current system status
Â                    logger.info(f"ðŸ”§ integration_validations table not available: {table_error}")
Â                    logger.info("ðŸ”§ Falling back to platform_settings...")
Â 
Â                    # Check platform_settings for API configurations
Â                    try:
Â                        platform_configs = await conn.fetch("""
Â                            SELECT key, value, updated_at
Â                            FROM platform_settings
Â                            WHERE key LIKE '%api%' OR key LIKE '%secret%' OR key LIKE '%config%'
Â                        """)
Â 
Â                        # Map platform configs to integrations
Â                        config_map = {
Â                            'prokerala': any('prokerala' in row['key'].lower() for row in platform_configs),
Â                            'openai_guidance': any('openai' in row['key'].lower() for row in platform_configs),
Â                            'elevenlabs_voice': any('elevenlabs' in row['key'].lower() for row in platform_configs),
Â                            'did_avatar': any('did' in row['key'].lower() for row in platform_configs),
Â                            'rag_knowledge': True,  # Always available
Â                            'social_media': any('facebook' in row['key'].lower() or 'instagram' in row['key'].lower() for row in platform_configs)
Â                        }
Â 
Â                        # Set status based on configuration availability
Â                        for integration, is_configured in config_map.items():
Â                            integration_points[integration] = {
Â                                "status": "healthy" if is_configured else "not_configured",
Â                                "success_rate": 95.0 if is_configured else 0.0,
Â                                "total_validations": 0,
Â                                "latency_ms": 1500 if is_configured else 0,
Â                                "last_check": datetime.now(timezone.utc).isoformat()
Â                            }
Â 
Â                    except Exception as config_error:
Â                        logger.warning(f"Could not read platform_settings: {config_error}")
Â 
Â                # Ensure all expected integrations are present
Â                expected_integrations = ['prokerala', 'rag_knowledge', 'openai_guidance', 'elevenlabs_voice', 'did_avatar', 'social_media']
Â                for integration in expected_integrations:
Â                    if integration not in integration_points:
Â                        integration_points[integration] = {
Â                            "status": "unknown",
Â                            "success_rate": 0.0,
Â                            "total_validations": 0,
Â                            "latency_ms": 0,
Â                            "last_check": datetime.now(timezone.utc).isoformat()
Â                        }
Â 
Â                logger.info(f"âœ… System health calculated: status={system_status}, integrations={len(integration_points)}")
Â                logger.info(f"ðŸ“Š Integration points: {list(integration_points.keys())}")
Â 
Â                return {
Â                    "system_status": system_status,
Â                    "integration_points": integration_points,
Â                    "recent_issues": [],  # Will be populated by _get_critical_issues
Â                    "last_check": datetime.now(timezone.utc).isoformat()
Â                }
Â 
Â            finally:
Â                await db_manager.release_connection(conn)
Â 
Â        except Exception as e:
Â            logger.error(f"Failed to get system health from database: {e}")
Â            return {
Â                "system_status": "error",
Â                "integration_points": {},
Â                "recent_issues": [{"type": "system_error", "message": str(e), "timestamp": datetime.now(timezone.utc).isoformat()}],
Â                "last_check": datetime.now(timezone.utc).isoformat()
Â            }
Â 
Â    async def _get_active_sessions_count(self) -> int:
Â        """Get count of active sessions from database"""
Â        try:
Â            from db import db_manager
Â            conn = await db_manager.get_connection()
Â            try:
Â                count = await conn.fetchval("""
Â                    SELECT COUNT(*) FROM sessions
Â                    WHERE status = 'active'
Â                    AND created_at > NOW() - INTERVAL '4 hours'
Â                """)
Â                return count or 0
Â            finally:
Â                await db_manager.release_connection(conn)
Â        except Exception as e:
Â            logger.error(f"Failed to get active sessions count: {e}")
Â            return 0
Â 
Â    async def _generate_session_recommendations(self, session_id: str) -> List[str]:
Â        """Generate specific recommendations for a session"""
Â        try:
Â            conn = await db_manager.get_connection()
Â            try:
Â                # Get validation results
Â                validation_data = await conn.fetchrow("""
Â                    SELECT validation_results
Â                    FROM validation_sessions
Â                    WHERE session_id = $1
Â                """, session_id)
Â 
Â                if not validation_data or not validation_data["validation_results"]:
Â                    return []
Â 
Â                try:
Â                    validation_results = json.loads(validation_data["validation_results"])
Â                except json.JSONDecodeError as json_error:
Â                    logger.error(f"Failed to parse validation results JSON: {json_error}")
Â                    validation_results = {}
Â 
Â                # Use business validator to generate recommendations
Â                return validation_results.get("recommendations", [])
Â            finally:
Â                await db_manager.release_connection(conn)
Â 
Â        except Exception as e:
Â            logger.error(f"Failed to generate session recommendations: {e}")
Â            return []
Â 
Â    async def get_comprehensive_test_definitions(self) -> List[Dict[str, Any]]:
Â        """
Â        Get comprehensive test definitions dynamically from backend systems and database
Â        This discovers available tests from TestSuiteGenerator and existing test data
Â        Following .cursor rules: No hardcoded data, retrieve from database and backend systems
Â        Returns test suites (not individual test cases) for proper grouping
Â        """
Â        try:
Â            # Method 1: Try to get test definitions from TestSuiteGenerator (primary source)
Â            try:
Â                from test_suite_generator import TestSuiteGenerator
Â 
Â                generator = TestSuiteGenerator()
Â                test_suites = await generator.generate_all_test_suites()
Â 
Â                # Keep test suites grouped (don't flatten into individual test cases)
Â                comprehensive_tests = []
Â                for suite_name, suite_data in test_suites.items():
Â                    if "error" not in suite_data and "test_cases" in suite_data:
Â                        # Count individual test cases within this suite
Â                        test_case_count = len(suite_data.get("test_cases", []))
Â 
Â                        comprehensive_tests.append({
Â                            "test_name": suite_name,
Â                            "test_category": suite_data.get("test_category", suite_name.replace("_tests", "")),
Â                            "test_type": suite_data.get("test_type", "integration"),
Â                            "description": suite_data.get("description", ""),
Â                            "priority": suite_data.get("priority", "medium"),
Â                            "suite_name": suite_name,
Â                            "suite_display_name": suite_data.get("test_suite_name", suite_name),
Â                            "test_case_count": test_case_count  # Track individual test cases within suite
Â                        })
Â 
Â                if comprehensive_tests:
Â                    logger.info(f"Retrieved {len(comprehensive_tests)} test suites from TestSuiteGenerator")
Â                    return comprehensive_tests
Â 
Â            except ImportError:
Â                logger.warning("TestSuiteGenerator not available, falling back to database discovery")
Â            except Exception as e:
Â                logger.warning(f"TestSuiteGenerator failed: {e}, falling back to database discovery")
Â 
Â            # Method 2: Get test definitions from database (secondary source)
Â            try:
Â                conn = await db_manager.get_connection()
Â                try:
Â                    # Get test suites grouped by category from test_case_results table
Â                    test_suites_data = await conn.fetch("""
Â                        SELECT
Â                            test_category,
Â                            COUNT(*) as test_case_count,
Â                            CASE
Â                                WHEN test_category IN ('auth', 'api', 'database') THEN 'critical'
Â                                WHEN test_category IN ('integration', 'performance') THEN 'high'
Â                                ELSE 'medium'
Â                            END as priority
Â                        FROM test_case_results
Â                        WHERE test_category IS NOT NULL
Â                        AND test_category != ''
Â                        GROUP BY test_category
Â                        ORDER BY test_category
Â                    """)
Â 
Â                    if test_suites_data:
Â                        comprehensive_tests = []
Â                        for row in test_suites_data:
Â                            comprehensive_tests.append({
Â                                "test_name": f"{row['test_category']}_tests",
Â                                "test_category": row["test_category"],
Â                                "test_type": "integration",
Â                                "description": f"Database discovered {row['test_category']} test suite",
Â                                "priority": row["priority"],
Â                                "suite_name": f"{row['test_category']}_tests",
Â                                "suite_display_name": row["test_category"].replace("_", " ").title(),
Â                                "test_case_count": row["test_case_count"]
Â                            })
Â 
Â                        logger.info(f"Retrieved {len(comprehensive_tests)} test suites from database")
Â                        return comprehensive_tests
Â 
Â                finally:
Â                    await db_manager.release_connection(conn)
Â 
Â            except Exception as e:
Â                logger.warning(f"Database test discovery failed: {e}")
Â 
Â            # Method 3: Discover tests from backend systems (tertiary source)
Â            try:
Â                # Get available test types from TestExecutionEngine
Â                from test_execution_engine import TestExecutionEngine
Â 
Â                engine = TestExecutionEngine()
Â                available_suites = await engine._get_available_test_suites()
Â 
Â                if available_suites:
Â                    comprehensive_tests = []
Â                    for suite in available_suites:
Â                        test_type = suite.get("test_type", "unknown")
Â                        test_category = suite.get("test_category", "unknown")
Â 
Â                        comprehensive_tests.append({
Â                            "test_name": f"{test_category}_tests",
Â                            "test_category": test_category,
Â                            "test_type": test_type,
Â                            "description": f"Auto-discovered {test_category} test suite",
Â                            "priority": "medium",
Â                            "suite_name": f"{test_category}_tests",
Â                            "suite_display_name": f"{test_category.replace('_', ' ').title()} Tests",
Â                            "test_case_count": 0  # Unknown count for auto-discovered suites (0 = unknown)
Â                        })
Â 
Â                    logger.info(f"Retrieved {len(comprehensive_tests)} test suites from TestExecutionEngine")
Â                    return comprehensive_tests
Â 
Â            except Exception as e:
Â                logger.warning(f"TestExecutionEngine discovery failed: {e}")
Â 
Â            # Method 4: Minimal fallback - return empty list (no hardcoded data)
Â            logger.warning("All test discovery methods failed - returning empty test list")
Â            return []
Â 
Â        except Exception as e:
Â            logger.error(f"Failed to get comprehensive test definitions: {e}")
Â            return []

# Create singleton instance
monitoring_dashboard = MonitoringDashboard()

# API Endpoints
@router.get("/health")
async def get_monitoring_health():
Â    """Public endpoint to get basic monitoring system health (no auth required)"""
Â    try:
Â        system_health = await integration_monitor.get_system_health()
Â        return StandardResponse(
Â            status="success",
Â            message="Monitoring system health retrieved",
Â            data={
Â                "monitoring_active": True,
Â                "integrations": system_health.get("integrations", {}),
Â                "last_check": system_health.get("last_check"),
Â                "system_status": system_health.get("system_status", "operational")
Â            }
Â        )
Â    except Exception as e:
Â        return StandardResponse(
Â            status="error",
Â            message=f"Failed to get monitoring health: {str(e)}",
Â            data={
Â                "monitoring_active": False,
Â                "system_status": "error"
Â            }
Â        )

@router.get("/dashboard")
async def get_dashboard():
Â    """Get monitoring dashboard data for admin interface (public endpoint for testing)"""
Â    try:
Â        dashboard_data = await monitoring_dashboard.get_dashboard_data()
Â        return StandardResponse(
Â            status="success",
Â            message="Dashboard data retrieved",
Â            data=dashboard_data
Â        )
Â    except Exception as e:
Â        logger.error(f"Failed to get dashboard data: {e}")
Â        # Return minimal data structure to prevent frontend crashes
Â        fallback_data = {
Â            "timestamp": datetime.now(timezone.utc).isoformat(),
Â            "system_health": {
Â                "system_status": "error",
Â                "integration_points": {},
Â                "recent_issues": [
Â                    {
Â                        "type": "dashboard_error",
Â                        "message": f"Dashboard data retrieval failed: {str(e)}",
Â                        "timestamp": datetime.now(timezone.utc).isoformat(),
Â                        "severity": "high"
Â                    }
Â                ]
Â            },
Â            "active_sessions": 0,
Â            "recent_sessions": [],
Â            "integration_statistics": {},
Â            "critical_issues": [],
Â            "social_media_health": {},
Â            "overall_metrics": {},
Â            "metrics": {
Â                "success_rates": {},
Â                "avg_response_times": {}
Â            },
Â            "alerts": []
Â        }
Â        return StandardResponse(
Â            status="error",
Â            message=f"Dashboard data retrieval failed: {str(e)}",
Â            data=fallback_data
Â        )

@router.get("/session/{session_id}")
async def get_session_validation(session_id: str, admin: dict = Depends(get_current_admin_dependency)):
Â    """Get detailed validation report for a specific session"""
Â    session_details = await monitoring_dashboard.get_session_details(session_id)
Â 
Â    if "error" in session_details:
Â        raise HTTPException(status_code=404, detail=session_details["error"])
Â 
Â    return StandardResponse(
Â        status="success",
Â        message="Session validation details retrieved",
Â        data=session_details
Â    )

@router.get("/integration/{integration_point}/health")
async def get_integration_health(integration_point: str, admin: dict = Depends(get_current_admin_dependency)):
Â    """Get detailed health metrics for a specific integration"""
Â    health_details = await monitoring_dashboard.get_integration_health_details(integration_point)
Â 
Â    return StandardResponse(
Â        status="success",
Â        message="Integration health details retrieved",
Â        data=health_details
Â    )

@router.post("/test/{test_type}")
async def trigger_test(test_type: str, admin: dict = Depends(get_current_admin_dependency)):
Â    """Trigger a validation test"""
Â    test_result = await monitoring_dashboard.trigger_validation_test(test_type)
Â 
Â    # trigger_validation_test now returns a StandardResponse object, so return it directly
Â    return test_result

# Testing infrastructure endpoints
@router.get("/test-status")
async def get_test_status():
Â    """Get current test execution status with comprehensive test information (database-driven, public endpoint)"""
Â    try:
Â        conn = await db_manager.get_connection()
Â        try:
Â            # Get comprehensive test definitions from monitoring dashboard
Â            try:
Â                comprehensive_tests = await monitoring_dashboard.get_comprehensive_test_definitions()
Â                total_test_suites = len(comprehensive_tests)  # Count of test suites (16)
Â                # Calculate total individual test cases by summing test_case_count from all suites
Â                total_comprehensive_tests = sum(test.get("test_case_count", 0) for test in comprehensive_tests)
Â            except Exception as e:
Â                logger.error(f"Failed to get comprehensive test definitions: {e}")
Â                comprehensive_tests = []
Â                total_test_suites = 0
Â                total_comprehensive_tests = 0
Â 
Â            # Get the latest completed test execution
Â            latest_execution = await conn.fetchrow("""
Â                SELECT completed_at, total_tests, passed_tests, failed_tests,
Â                       coverage_percentage, execution_time_seconds, status
Â                FROM test_execution_sessions
Â                WHERE status IN ('passed', 'failed', 'partial')
Â                ORDER BY completed_at DESC NULLS LAST, started_at DESC
Â                LIMIT 1
Â            """)
Â 
Â            if latest_execution:
Â                return StandardResponse(
Â                    status="success",
Â                    message="Comprehensive test status retrieved",
Â                    data={
Â                        "last_execution": latest_execution['completed_at'].isoformat() if latest_execution['completed_at'] else None,
Â                        "total_tests": total_comprehensive_tests,  # Individual test cases (41)
Â                        "total_test_suites": total_test_suites,  # Test suites count (16)
Â                        "passed_tests": latest_execution['passed_tests'] or 0,
Â                        "failed_tests": latest_execution['failed_tests'] or 0,
Â                        "test_coverage": float(latest_execution['coverage_percentage'] or 0),
Â                        "execution_time": latest_execution['execution_time_seconds'] or 0,
Â                        "status": latest_execution['status'] or 'unknown',
Â                        "auto_fixes_applied": 0,  # Set to 0 for now as requested
Â 
Â                        # Additional comprehensive test information
Â                        "comprehensive_test_suite": {
Â                            "total_defined_tests": total_comprehensive_tests,
Â                            "last_execution_tests": latest_execution['total_tests'] or 0,
Â                            "execution_coverage": round((latest_execution['total_tests'] or 0) / max(total_comprehensive_tests, 1) * 100, 1)
Â                        }
Â                    }
Â                )
Â            else:
Â                # No test executions found - show comprehensive test suite info
Â                return StandardResponse(
Â                    status="success",
Â                    message="No test executions found - showing comprehensive test suite",
Â                    data={
Â                        "last_execution": None,
Â                        "total_tests": total_comprehensive_tests,  # Individual test cases (41)
Â                        "total_test_suites": total_test_suites,  # Test suites count (16)
Â                        "passed_tests": 0,
Â                        "failed_tests": 0,
Â                        "test_coverage": 0,
Â                        "execution_time": 0,
Â                        "status": "never_run",
Â                        "auto_fixes_applied": 0,
Â 
Â                        # Comprehensive test information
Â                        "comprehensive_test_suite": {
Â                            "total_defined_tests": total_comprehensive_tests,
Â                            "last_execution_tests": 0,
Â                            "execution_coverage": 0
Â                        }
Â                    }
Â                )
Â        finally:
Â            await db_manager.release_connection(conn)
Â    except Exception as e:
Â        logger.error(f"Failed to get comprehensive test status: {e}")
Â        return StandardResponse(
Â            status="error",
Â            message=f"Failed to get test status: {str(e)}",
Â            data={
Â                # Return comprehensive test info even on error
Â                "total_tests": 41,
Â                "passed_tests": 0,
Â                "failed_tests": 0,
Â                "status": "error",
Â                "comprehensive_test_suite": {
Â                    "total_defined_tests": 41,
Â                    "last_execution_tests": 0,
Â                    "execution_coverage": 0
Â                }
Â            }
Â        )

@router.get("/test-sessions")
async def get_test_sessions():
Â    """Get test execution sessions history (public endpoint for testing)"""
Â    try:
Â        # Use db_manager connection pooling like other endpoints
Â        conn = await db_manager.get_connection()
Â        try:
Â            # Check if test_execution_sessions table exists
Â            table_exists = await conn.fetchval("""
Â                SELECT EXISTS(
Â                    SELECT 1 FROM information_schema.tables
Â                    WHERE table_name = 'test_execution_sessions' AND table_schema = 'public'
Â                )
Â            """)
Â 
Â            if not table_exists:
Â                # Return empty data if table doesn't exist yet
Â                return StandardResponse(
Â                    status="success",
Â                    message="Test sessions table not yet created",
Â                    data={"sessions": [], "total": 0}
Â                )
Â 
Â            # Fetch recent test sessions (last 50, ordered by most recent)
Â            sessions = await conn.fetch("""
Â                SELECT
Â                    session_id,
Â                    test_type,
Â                    test_category,
Â                    environment,
Â                    started_at,
Â                    completed_at,
Â                    status,
Â                    total_tests,
Â                    passed_tests,
Â                    failed_tests,
Â                    skipped_tests,
Â                    coverage_percentage,
Â                    execution_time_seconds,
Â                    triggered_by,
Â                    created_at
Â                FROM test_execution_sessions
Â                ORDER BY started_at DESC
Â                LIMIT 50
Â            """)
Â 
Â            # Format sessions for API response
Â            formatted_sessions = []
Â            for session in sessions:
Â                formatted_session = {
Â                    "session_id": str(session['session_id']),
Â                    "test_type": session['test_type'],
Â                    "test_category": session['test_category'],
Â                    "environment": session['environment'],
Â                    "status": session['status'],
Â                    "started_at": session['started_at'].isoformat() if session['started_at'] else None,
Â                    "completed_at": session['completed_at'].isoformat() if session['completed_at'] else None,
Â                    "execution_time_seconds": session['execution_time_seconds'],
Â                    "total_tests": session['total_tests'] or 0,
Â                    "passed_tests": session['passed_tests'] or 0,
Â                    "failed_tests": session['failed_tests'] or 0,
Â                    "skipped_tests": session['skipped_tests'] or 0,
Â                    "coverage_percentage": float(session['coverage_percentage']) if session['coverage_percentage'] else None,
Â                    "triggered_by": session['triggered_by'],
Â                    "created_at": session['created_at'].isoformat() if session['created_at'] else None
Â                }
Â                formatted_sessions.append(formatted_session)
Â 
Â            return StandardResponse(
Â                status="success",
Â                message=f"Retrieved {len(formatted_sessions)} test sessions",
Â                data={"sessions": formatted_sessions, "total": len(formatted_sessions)}
Â            )
Â 
Â        finally:
Â            await db_manager.release_connection(conn)
Â 
Â    except asyncpg.PostgresConnectionError as e:
Â        logger.error(f"Database connection error: {e}")
Â        return StandardResponse(
Â            status="error",
Â            message="Database connection failed",
Â            data={"sessions": [], "total": 0}
Â        )
Â    except asyncpg.PostgresError as e:
Â        logger.error(f"Database query error: {e}")
Â        return StandardResponse(
Â            status="error",
Â            message=f"Database query failed: {str(e)}",
Â            data={"sessions": [], "total": 0}
Â        )
Â    except Exception as e:
Â        logger.error(f"Unexpected error getting test sessions: {e}")
Â        return StandardResponse(
Â            status="error",
Â            message=f"Failed to get test sessions: {str(e)}",
Â            data={"sessions": [], "total": 0}
Â        )

@router.get("/test-metrics")
async def get_test_metrics():
Â    """Get comprehensive test execution metrics and statistics for all available tests (database-driven, public endpoint for testing)"""
Â    try:
Â        conn = await db_manager.get_connection()
Â        try:
Â            # Get comprehensive test definitions and their latest execution status
Â            try:
Â                comprehensive_tests = await monitoring_dashboard.get_comprehensive_test_definitions()
Â                total_test_suites = len(comprehensive_tests)  # Count of test suites (16)
Â                # Calculate total individual test cases by summing test_case_count from all suites
Â                total_individual_tests = sum(test.get("test_case_count", 0) for test in comprehensive_tests)
Â            except Exception as e:
Â                logger.error(f"Failed to get comprehensive test definitions: {e}")
Â                comprehensive_tests = []
Â                total_test_suites = 0
Â                total_individual_tests = 0
Â 
Â            # Get latest test execution results for each test
Â            latest_executions = await conn.fetch("""
Â                WITH latest_test_runs AS (
Â                    SELECT DISTINCT ON (test_type, test_category)
Â                        test_type,
Â                        test_category,
Â                        status,
Â                        total_tests,
Â                        passed_tests,
Â                        failed_tests,
Â                        execution_time_seconds,
Â                        completed_at
Â                    FROM test_execution_sessions
Â                    WHERE status IN ('passed', 'failed', 'partial')
Â                    ORDER BY test_type, test_category, completed_at DESC NULLS LAST
Â                )
Â                SELECT * FROM latest_test_runs
Â            """)
Â 
Â            # Calculate comprehensive metrics
Â            total_executed_tests = 0
Â            total_passed_tests = 0
Â            total_failed_tests = 0
Â            total_execution_time = 0
Â            execution_count = 0
Â 
Â            # Create execution map for quick lookup
Â            execution_map = {}
Â            for execution in latest_executions:
Â                key = f"{execution['test_type']}_{execution['test_category']}"
Â                execution_map[key] = execution
Â 
Â                if execution['total_tests']:
Â                    total_executed_tests += execution['total_tests']
Â                    total_passed_tests += execution['passed_tests'] or 0
Â                    total_failed_tests += execution['failed_tests'] or 0
Â 
Â                if execution['execution_time_seconds']:
Â                    total_execution_time += execution['execution_time_seconds']
Â                    execution_count += 1
Â 
Â            # Calculate success rate based on individual test cases, not sessions
Â            success_rate = (total_passed_tests / max(total_executed_tests, 1)) * 100 if total_executed_tests > 0 else 0
Â 
Â            # Calculate average execution time
Â            avg_execution_time = total_execution_time / max(execution_count, 1) if execution_count > 0 else 0
Â 
Â            # FIXED: Get coverage trend from actual test results
Â            recent_coverage = await conn.fetchval("""
Â                SELECT AVG(coverage_percentage) FROM test_execution_sessions
Â                WHERE started_at >= NOW() - INTERVAL '7 days'
Â                AND coverage_percentage IS NOT NULL
Â                AND status IN ('passed', 'failed', 'partial')
Â            """) or 0
Â 
Â            previous_coverage = await conn.fetchval("""
Â                SELECT AVG(coverage_percentage) FROM test_execution_sessions
Â                WHERE started_at >= NOW() - INTERVAL '14 days'
Â                AND started_at < NOW() - INTERVAL '7 days'
Â                AND coverage_percentage IS NOT NULL
Â                AND status IN ('passed', 'failed', 'partial')
Â            """) or 0
Â 
Â            coverage_trend = "improving" if recent_coverage > previous_coverage else "declining" if recent_coverage < previous_coverage else "stable"
Â 
Â            # FIXED: Get auto-fixes applied from actual test results
Â            auto_fixes_applied = await conn.fetchval("""
Â                SELECT COUNT(*) FROM autofix_test_results
Â                WHERE fix_applied = true
Â                AND created_at >= NOW() - INTERVAL '30 days'
Â            """) or 0
Â 
Â            # FIXED: Get most recent execution for overall status from actual test results
Â            latest_overall_execution = await conn.fetchrow("""
Â                SELECT completed_at, total_tests, passed_tests, failed_tests,
Â                       coverage_percentage, execution_time_seconds, status
Â                FROM test_execution_sessions
Â                WHERE status IN ('passed', 'failed', 'partial')
Â                ORDER BY completed_at DESC NULLS LAST, started_at DESC
Â                LIMIT 1
Â            """)
Â 
Â            # FIXED: Calculate actual total individual test cases from database
Â            try:
Â                total_available_individual_tests = await conn.fetchval("""
Â                    SELECT COUNT(*) FROM (
Â                        SELECT DISTINCT test_name
Â                        FROM test_case_results
Â                        WHERE created_at >= NOW() - INTERVAL '7 days'
Â                    ) t
Â                """) or total_individual_tests  # Fallback to calculated total from test suites
Â            except Exception as e:
Â                # Handle database/schema errors (table or column missing)
Â                logger.warning(f"DISTINCT test_name query failed: {e}")
Â                total_available_individual_tests = total_individual_tests
Â 
Â            return StandardResponse(
Â                status="success",
Â                message="Comprehensive test metrics retrieved",
Â                data={
Â                    # FIXED: Use actual test counts - individual test cases for execution metrics
Â                    "total_tests": total_available_individual_tests,
Â                    "total_test_suites": total_test_suites,  # For dashboard display (16)
Â                    "total_executed_tests": total_executed_tests,
Â                    "success_rate": round(success_rate, 1),
Â                    "avg_execution_time": round(avg_execution_time, 1),
Â                    "coverage_trend": coverage_trend,
Â                    "auto_fixes_applied": auto_fixes_applied,
Â 
Â                    # FIXED: Latest execution summary from actual test results
Â                    "latest_execution": {
Â                        "last_run": latest_overall_execution['completed_at'].isoformat() if latest_overall_execution and latest_overall_execution['completed_at'] else None,
Â                        "total_tests": latest_overall_execution['total_tests'] if latest_overall_execution else total_available_individual_tests,
Â                        "passed_tests": latest_overall_execution['passed_tests'] if latest_overall_execution else 0,
Â                        "failed_tests": latest_overall_execution['failed_tests'] if latest_overall_execution else 0,
Â                        "test_coverage": float(latest_overall_execution['coverage_percentage']) if latest_overall_execution and latest_overall_execution['coverage_percentage'] else 0,
Â                        "execution_time": latest_overall_execution['execution_time_seconds'] if latest_overall_execution else 0,
Â                        "status": latest_overall_execution['status'] if latest_overall_execution else 'unknown'
Â                    },
Â 
Â                    # FIXED: Legacy fields for backward compatibility
Â                    "total_sessions": len(latest_executions)
Â                }
Â            )
Â        finally:
Â            await db_manager.release_connection(conn)
Â    except Exception as e:
Â        logger.error(f"Failed to get comprehensive test metrics: {e}")
Â        return StandardResponse(
Â            status="error",
Â            message=f"Failed to get test metrics: {str(e)}",
Â            data={
Â                # Return the expected 41 tests even on error
Â                "total_tests": 41,
Â                "total_executed_tests": 0,
Â                "success_rate": 0,
Â                "avg_execution_time": 0,
Â                "coverage_trend": "unknown",
Â                "auto_fixes_applied": 0,
Â                "latest_execution": {
Â                    "total_tests": 41,
Â                    "passed_tests": 0,
Â                    "failed_tests": 0,
Â                    "status": "unknown"
Â                }
Â            }
Â        )

@router.post("/test-execute")
async def execute_test(request: dict):
Â    """Execute a test suite using our actual test execution engine (public endpoint for testing)"""
Â    try:
Â        from test_execution_engine import TestExecutionEngine
Â 
Â        test_type = request.get("test_type", "unit")
Â        test_suite = request.get("test_suite", None)
Â        triggered_by = request.get("triggered_by", "manual")
Â 
Â        # Initialize test execution engine
Â        engine = TestExecutionEngine()
Â 
Â        # FIXED: Handle individual test suite execution vs all suites
Â        if test_suite and test_suite != "all":
Â            # Execute specific test suite for individual card
Â            logger.info(f"Executing individual test suite: {test_suite}")
Â            result = await engine.execute_test_suite(test_suite, test_type)
Â 
Â            # FIXED: Return specific results for individual test cards
Â            return StandardResponse(
Â                status="success",
Â                message=f"Individual test suite '{test_suite}' execution completed",
Â                data={
Â                    "test_suite": test_suite,
Â                    "status": result.get("status", "unknown"),
Â                    "total_tests": result.get("total_tests", 0),
Â                    "passed_tests": result.get("passed_tests", 0),
Â                    "failed_tests": result.get("failed_tests", 0),
Â                    "execution_time_seconds": result.get("execution_time_seconds", 0),
Â                    "results": result.get("results", {}),
Â                    "triggered_by": triggered_by
Â                }
Â            )
Â        else:
Â            # Execute all test suites (for "Run All Tests" button )
Â            logger.info("Executing all test suites")
Â            result = await engine.execute_all_test_suites()
Â 
Â            return StandardResponse(
Â                status="success",
Â                message=f"All test suites execution completed: {test_type}",
Â                data=result
Â            )
Â    except Exception as e:
Â        logger.error(f"Test execution failed: {e}")
Â        return StandardResponse(
Â            status="error",
Â            message=f"Failed to execute test: {str(e)}",
Â            data={}
Â        )

# Configurable maximum number of test suites to prevent frontend grid instability
MAX_SUITES = int(os.getenv('MAX_TEST_SUITES', '16'))

def infer_frontend_category_and_icon(test_category: str) -> tuple[str, str]:
Â    """
Â    Centralized helper function to determine frontend category and icon from test_category.
Â 
Â    Args:
Â        test_category: The test category string from database
Â 
Â    Returns:
Â        tuple: (frontend_category, icon)
Â    """
Â    category_lower = test_category.lower()
Â 
Â    # Expanded keyword lists for better matching
Â    core_platform_keywords = [
Â        'database', 'db', 'api', 'security', 'integration', 'performance',
Â        'auto_healing', 'auto-heal', 'auto_heal'
Â    ]
Â 
Â    revenue_critical_keywords = [
Â        'payment', 'payments', 'spiritual', 'avatar'
Â    ]
Â 
Â    communication_keywords = [
Â        'live', 'live_audio', 'live_video', 'live_media', 'live_audio_video_business_critical',
Â        'audio', 'video', 'social', 'media', 'social_media'
Â    ]
Â 
Â    user_experience_keywords = [
Â        'user', 'user_management', 'user_mgmt', 'community', 'notification', 'notifications'
Â    ]
Â 
Â    business_management_keywords = [
Â        'admin', 'monitoring', 'business'
Â    ]
Â 
Â    # Determine frontend category based on expanded keyword matching
Â    if any(keyword in category_lower for keyword in core_platform_keywords):
Â        frontend_category = 'Core Platform'
Â    elif any(keyword in category_lower for keyword in revenue_critical_keywords):
Â        frontend_category = 'Revenue Critical'
Â    elif any(keyword in category_lower for keyword in communication_keywords):
Â        frontend_category = 'Communication'
Â    elif any(keyword in category_lower for keyword in user_experience_keywords):
Â        frontend_category = 'User Experience'
Â    elif any(keyword in category_lower for keyword in business_management_keywords):
Â        frontend_category = 'Business Management'
Â    else:
Â        frontend_category = 'Other Services'
Â 
Â    # Expanded icon mapping with all variants
Â    icon_mapping = {
Â        'database': 'ðŸ—„ï¸', 'db': 'ðŸ—„ï¸',
Â        'api': 'ðŸ”Œ',
Â        'security': 'ðŸ”’',
Â        'integration': 'ðŸ”—',
Â        'performance': 'âš¡',
Â        'auto_healing': 'ðŸ”„', 'auto-heal': 'ðŸ”„', 'auto_heal': 'ðŸ”„',
Â        'payment': 'ðŸ’³', 'payments': 'ðŸ’³',
Â        'spiritual': 'ðŸ•‰ï¸',
Â        'avatar': 'ðŸŽ­',
Â        'live': 'ðŸ“¹', 'live_audio': 'ðŸ“¹', 'live_video': 'ðŸ“¹', 'live_media': 'ðŸ“¹',
Â        'live_audio_video_business_critical': 'ðŸ“¹',
Â        'audio': 'ðŸ”Š', 'video': 'ðŸ“¹',
Â        'social_media': 'ðŸ“±', 'social': 'ðŸ“±', 'media': 'ðŸ“±',
Â        'user_management': 'ðŸ‘¤', 'user_mgmt': 'ðŸ‘¤', 'user': 'ðŸ‘¤',
Â        'community': 'ðŸ¤',
Â        'notifications': 'ðŸ””', 'notification': 'ðŸ””',
Â        'admin': 'âš™ï¸',
Â        'monitoring': 'ðŸ“Š',
Â        'business': 'ðŸ’¼'
Â    }
Â 
Â    # Find matching icon (first match wins)
Â    icon = 'ðŸ”§'  # default
Â    for keyword, emoji in icon_mapping.items():
Â        if keyword in category_lower:
Â            icon = emoji
Â            break
Â 
Â    return frontend_category, icon

@router.get("/test-suites")
async def get_available_test_suites():
Â    """Get all available test suites - database-driven discovery from TestSuiteGenerator"""
Â    try:
Â        # Method 1: Get available test suites from TestSuiteGenerator (primary source)
Â        try:
Â            comprehensive_tests = await monitoring_dashboard.get_comprehensive_test_definitions()
Â            if comprehensive_tests:
Â                logger.info(f"Using TestSuiteGenerator - found {len(comprehensive_tests)} test suites")
Â 
Â                # Convert to the format expected by the frontend
Â                categorized_suites = {}
Â 
Â                for test_suite in comprehensive_tests:
Â                    test_category = test_suite.get("test_category", "unknown")
Â                    suite_name = test_suite.get("suite_display_name", test_category.replace("_", " ").title())
Â                    priority = test_suite.get("priority", "medium")
Â 
Â                    # Use centralized helper function for consistent categorization and icon assignment
Â                    frontend_category, icon = infer_frontend_category_and_icon(test_category)
Â 
Â                    # Create frontend category if it doesn't exist
Â                    if frontend_category not in categorized_suites:
Â                        categorized_suites[frontend_category] = {
Â                            "category": frontend_category,
Â                            "services": []
Â                        }
Â 
Â                    # Add test suite to appropriate category
Â                    categorized_suites[frontend_category]["services"].append({
Â                        "title": suite_name,
Â                        "testType": test_category,
Â                        "icon": icon,
Â                        "priority": priority,
Â                        "description": test_suite.get("description", suite_name),
Â                        "name": test_category,
Â                        "timeout_seconds": 300
Â                    })
Â 
Â                # Convert to list format expected by frontend
Â                suite_config = list(categorized_suites.values())
Â 
Â                # Apply configurable suite cap to prevent frontend grid instability
Â                total_services = sum(len(category["services"]) for category in suite_config)
Â                if total_services > MAX_SUITES:
Â                    # Truncate services while preserving category structure
Â                    services_added = 0
Â                    for category in suite_config:
Â                        if services_added >= MAX_SUITES:
Â                            category["services"] = []
Â                        else:
Â                            remaining_slots = MAX_SUITES - services_added
Â                            category["services"] = category["services"][:remaining_slots]
Â                            services_added += len(category["services"])
Â 
Â                total_test_suites = len(comprehensive_tests)
Â 
Â                return StandardResponse(
Â                    status="success",
Â                    message=f"Retrieved {total_test_suites} test suites from TestSuiteGenerator (database-driven)",
Â                    data={
Â                        "test_suites": suite_config,
Â                        "total_suites": total_test_suites,
Â                        "source": "test_suite_generator"
Â                    }
Â                )
Â        except Exception as e:
Â            logger.warning(f"TestSuiteGenerator discovery failed: {e}, falling back to database")
Â 
Â        # Method 2: Fallback to test_case_results table (secondary source)
Â        conn = await db_manager.get_connection()
Â        try:
Â            try:
Â                # Get test categories from test_case_results table as fallback
Â                test_categories = await conn.fetch("""
Â                    WITH ranked_categories AS (
Â                        SELECT
Â                            test_category,
Â                            COUNT(*) as total_tests,
Â                            COUNT(*) FILTER (WHERE status = 'passed') as passed_tests,
Â                            COUNT(*) FILTER (WHERE status = 'failed') as failed_tests,
Â                            MAX(created_at) as last_execution,
Â                            ROW_NUMBER() OVER (ORDER BY MAX(created_at) DESC, COUNT(*) DESC) as rn
Â                        FROM test_case_results
Â                        WHERE test_category IS NOT NULL
Â                        AND test_category != ''
Â                        GROUP BY test_category
Â                    )
Â                    SELECT
Â                        test_category,
Â                        total_tests,
Â                        passed_tests,
Â                        failed_tests,
Â                        last_execution
Â                    FROM ranked_categories
Â                    WHERE rn <= 16  -- Limit to exactly 16 test suites
Â                    ORDER BY test_category
Â                """)
Â            except asyncpg.exceptions.UndefinedTableError:
Â                # Table doesn't exist - return empty result instead of error
Â                logger.warning("test_case_results table not found in database")
Â                return StandardResponse(
Â                    status="success",
Â                    message="No test execution results found - test_case_results table needs to be created in database",
Â                    data={"test_suites": [], "total_suites": 0}
Â                )
Â            except Exception as query_error:
Â                # Any other query error (like column not found)  provide detailed error for debugging
Â                error_type = type(query_error).__name__
Â                logger.error(f"Query error in test_case_results: {error_type}: {query_error}")
Â 
Â                return StandardResponse(
Â                    status="success",
Â                    message="Test execution results unavailable - database schema needs to be updated",
Â                    data={"test_suites": [], "total_suites": 0}
Â                )
Â 
Â            # Icon mapping is now centralized in helper function
Â 
Â            # Group test categories by frontend category for consumption (100% database-driven)
Â            categorized_suites = {}
Â            for test_category_row in test_categories:
Â                test_category = test_category_row['test_category']
Â                total_tests = test_category_row['total_tests']
Â                passed_tests = test_category_row['passed_tests']
Â                failed_tests = test_category_row['failed_tests']
Â 
Â                # Derive display information from test_category name (database-driven approach)
Â                # Following .cursor rules: Handle missing columns gracefully, no assumptions about schema
Â                display_name = test_category.replace('_', ' ').title()
Â 
Â                # Fallback logic for cases where display_name column might be expected but missing
Â                # This ensures compatibility with any code that expects display_name from database
Â                name_fallback = test_category  # Use test_category as 'name' fallback
Â 
Â                # Use centralized helper function for consistent categorization and icon assignment
Â                frontend_category, icon = infer_frontend_category_and_icon(test_category)
Â 
Â                # Determine priority based on test results and category patterns (database-driven)
Â                if failed_tests > 0:
Â                    priority_level = 'critical'
Â                elif any(keyword in test_category.lower() for keyword in ['database', 'api', 'security', 'payment', 'spiritual']):
Â                    priority_level = 'critical'
Â                elif any(keyword in test_category.lower() for keyword in ['integration', 'performance', 'user', 'admin']):
Â                    priority_level = 'high'
Â                else:
Â                    priority_level = 'medium'
Â 
Â                # Icon is now determined by the helper function above
Â 
Â                # Create frontend category if it doesn't exist
Â                if frontend_category not in categorized_suites:
Â                    categorized_suites[frontend_category] = {
Â                        "category": frontend_category,
Â                        "services": []
Â                    }
Â 
Â                # Add test suite to appropriate category with fallback logic
Â                # Following .cursor rules: Graceful handling of missing columns, robust fallbacks
Â                categorized_suites[frontend_category]["services"].append({
Â                    "title": display_name,  # Always use derived display name
Â                    "testType": test_category,  # Use actual database column
Â                    "icon": icon,
Â                    "priority": priority_level,
Â                    "description": display_name,  # Use derived display name (fallback-safe)
Â                    "name": name_fallback,  # Provide 'name' fallback for compatibility
Â                    "timeout_seconds": 300  # Default 5 minutes
Â                })
Â 
Â            # Convert to list format expected by frontend (100% database-driven)
Â            suite_config = list(categorized_suites.values())
Â 
Â            # Apply configurable suite cap to prevent frontend grid instability
Â            total_services = sum(len(category["services"]) for category in suite_config)
Â            if total_services > MAX_SUITES:
Â                # Truncate services while preserving category structure
Â                services_added = 0
Â                for category in suite_config:
Â                    if services_added >= MAX_SUITES:
Â                        category["services"] = []
Â                    else:
Â                        remaining_slots = MAX_SUITES - services_added
Â                        category["services"] = category["services"][:remaining_slots]
Â                        services_added += len(category["services"])
Â 
Â            total_test_categories = len(test_categories)
Â 
Â            logger.info(f"Retrieved {total_test_categories} test categories from test_case_results table (100% database-driven)")
Â 
Â            return StandardResponse(
Â                status="success",
Â                message=f"Retrieved {total_test_categories} test categories from test execution results",
Â                data={
Â                    "test_suites": suite_config,
Â                    "total_suites": total_test_categories
Â                }
Â            )
Â 
Â        except asyncpg.PostgresError as db_error:
Â            # Handle specific database connection and query errors
Â            logger.error(f"Database error while fetching test suites: {db_error}")
Â            return StandardResponse(
Â                status="error",
Â                message=f"Database error: {str(db_error)}",
Â                data={"test_suites": []}
Â            )
Â        finally:
Â            if conn:
Â                await db_manager.release_connection(conn)
Â 
Â    except Exception as e:
Â        # Handle general connection manager errors and other unexpected issues
Â        logger.error(f"Failed to get test suites from database: {e}")
Â        return StandardResponse(
Â            status="error",
Â            message=f"Failed to retrieve test suites: {str(e)}",
Â            data={"test_suites": []}
Â        )

@router.get("/business-logic-validation")
async def get_business_logic_validation_status():
Â    """Get business logic validation status and recent results (public endpoint for testing)"""
Â    try:
Â        conn = await db_manager.get_connection()
Â        try:
Â            # Check if business_logic_issues table exists and get recent results
Â            table_exists = await conn.fetchval("""
Â                SELECT EXISTS(
Â                    SELECT 1 FROM information_schema.tables
Â                    WHERE table_name = 'business_logic_issues' AND table_schema = 'public'
Â                )
Â            """)
Â 
Â            if not table_exists:
Â                # Return empty data if table doesn't exist yet
Â                return StandardResponse(
Â                    status="success",
Â                    message="Business logic validation table not yet created",
Â                    data={
Â                        "summary": {
Â                            "total_validations": 0,
Â                            "passed_validations": 0,
Â                            "success_rate": 0,
Â                            "avg_quality_score": 0
Â                        },
Â                        "recent_validations": []
Â                    }
Â                )
Â 
Â            # Check schema compatibility first
Â            schema_check = await conn.fetch("""
Â                SELECT column_name
Â                FROM information_schema.columns
Â                WHERE table_name = 'business_logic_issues'
Â                AND column_name IN ('issue_description', 'severity_score', 'validation_type')
Â            """)
Â 
Â            available_columns = {row['column_name'] for row in schema_check}
Â            has_new_schema = 'issue_description' in available_columns and 'severity_score' in available_columns
Â            has_validation_type = 'validation_type' in available_columns
Â 
Â            # Build query based on available schema
Â            if has_new_schema:
Â                # New schema with issue_description and severity_score
Â                recent_validations = await conn.fetch("""
Â                    SELECT
Â                        session_id,
Â                        issue_description,
Â                        severity_score as quality_score,
Â                        created_at,
Â                        CASE
Â                            WHEN severity_score IS NULL OR severity_score = 0 THEN 'success'
Â                            WHEN severity_score <= 3 THEN 'warning'
Â                            ELSE 'error'
Â                        END as validation_result,
Â                        COALESCE(validation_type, 'business_logic') as validation_type
Â                    FROM business_logic_issues
Â                    WHERE created_at >= NOW() - INTERVAL '24 hours'
Â                    ORDER BY created_at DESC
Â                    LIMIT 50
Â                """)
Â            elif has_validation_type:
Â                # Old schema with validation_type but no new columns
Â                recent_validations = await conn.fetch("""
Â                    SELECT
Â                        session_id,
Â                        validation_type,
Â                        'No description available' as issue_description,
Â                        0 as quality_score,
Â                        created_at,
Â                        CASE
Â                            WHEN validation_type LIKE '%success%' OR validation_type LIKE '%pass%' THEN 'success'
Â                            WHEN validation_type LIKE '%warning%' THEN 'warning'
Â                            ELSE 'error'
Â                        END as validation_result
Â                    FROM business_logic_issues
Â                    WHERE created_at >= NOW() - INTERVAL '24 hours'
Â                    ORDER BY created_at DESC
Â                    LIMIT 50
Â                """)
Â            else:
Â                # Minimal schema - just basic columns
Â                recent_validations = await conn.fetch("""
Â                    SELECT
Â                        session_id,
Â                        'business_logic' as validation_type,
Â                        'Legacy validation entry' as issue_description,
Â                        0 as quality_score,
Â                        created_at,
Â                        'unknown' as validation_result
Â                    FROM business_logic_issues
Â                    WHERE created_at >= NOW() - INTERVAL '24 hours'
Â                    ORDER BY created_at DESC
Â                    LIMIT 50
Â                """)
Â 
Â            # Calculate summary statistics
Â            total_validations = len(recent_validations)
Â            passed_validations = sum(1 for v in recent_validations
Â                                   if v['validation_result'] and str(v['validation_result']).lower() == 'success')
Â            avg_quality_score = sum(v['quality_score'] or 0 for v in recent_validations) / max(total_validations, 1)
Â 
Â            validation_data = [dict(v) for v in recent_validations]
Â 
Â            return StandardResponse(
Â                status="success",
Â                message="Business logic validation status retrieved",
Â                data={
Â                    "summary": {
Â                        "total_validations": total_validations,
Â                        "passed_validations": passed_validations,
Â                        "success_rate": (passed_validations / max(total_validations, 1)) * 100,
Â                        "avg_quality_score": round(avg_quality_score, 2)
Â                    },
Â                    "recent_validations": validation_data
Â                }
Â            )
Â        finally:
Â            await db_manager.release_connection(conn)
Â    except Exception as e:
Â        logger.error(f"Failed to get business logic validation status: {e}")
Â        return StandardResponse(
Â            status="error",
Â            message=f"Failed to get validation status: {str(e)}",
Â            data={}
Â        )

@router.post("/business-logic-validate")
async def trigger_business_logic_validation(request: dict):
Â    """Trigger business logic validation for spiritual content (public endpoint for testing)"""
Â    try:
Â        from monitoring.business_validator import BusinessLogicValidator
Â 
Â        validator = BusinessLogicValidator()
Â 
Â        # Get validation request parameters
Â        session_context = request.get("session_context", {})
Â        # If no session context provided, create a test context
Â        if not session_context:
Â            session_context = {
Â                "spiritual_question": "How can I find inner peace through meditation?",
Â                "birth_details": {
Â                    "date": "1990-01-01",
Â                    "time": "12:00",
Â                    "location": "Mumbai, India"
Â                },
Â                "integration_results": {
Â                    "rag_knowledge": {
Â                        "passed": True,
Â                        "actual": {
Â                            "knowledge": "Meditation is a sacred practice that connects us with divine consciousness."
Â                        }
Â                    }
Â                }
Â            }
Â 
Â        # Run validation
Â        validation_result = await validator.validate_session(session_context)
Â 
Â        return StandardResponse(
Â            status="success",
Â            message="Business logic validation completed",
Â            data={
Â                "validation_result": validation_result,
Â                "timestamp": datetime.now(timezone.utc).isoformat()
Â            }
Â        )
Â    except Exception as e:
Â        logger.error(f"Business logic validation failed: {e}")
Â        return StandardResponse(
Â            status="error",
Â            message=f"Validation failed: {str(e)}",
Â            data={}
Â        )

@router.get("/spiritual-services-status")
async def get_spiritual_services_status():
Â    """Get spiritual services health and validation status (public endpoint)"""
Â    try:
Â        from enhanced_business_logic import SpiritualAvatarEngine, MonetizationOptimizer
Â 
Â        # Test SpiritualAvatarEngine
Â        avatar_status = {"available": False, "error": None}
Â        try:
Â            SpiritualAvatarEngine()
Â            avatar_status["available"] = True
Â        except Exception as e:
Â            avatar_status["error"] = str(e)
Â 
Â        # Test MonetizationOptimizer
Â        monetization_status = {"available": False, "error": None}
Â        try:
Â            MonetizationOptimizer()
Â            monetization_status["available"] = True
Â        except Exception as e:
Â            monetization_status["error"] = str(e)
Â 
Â        # Get recent spiritual service metrics
Â        try:
Â            conn = await db_manager.get_connection()
Â            try:
Â                # Count recent spiritual sessions
Â                recent_sessions = await conn.fetchval("""
Â                    SELECT COUNT(*) FROM sessions
Â                    WHERE created_at >= NOW() - INTERVAL '24 hours'
Â                    AND session_type = 'spiritual_guidance'
Â                """)
Â 
Â                # Count successful validations
Â                successful_validations = await conn.fetchval("""
Â                    SELECT COUNT(*) FROM business_logic_issues
Â                    WHERE created_at >= NOW() - INTERVAL '24 hours'
Â                    AND validation_result = 'passed'
Â                """)
Â 
Â            finally:
Â                await db_manager.release_connection(conn)
Â        except Exception:
Â            recent_sessions = 0
Â            successful_validations = 0
Â 
Â        return StandardResponse(
Â            status="success",
Â            message="Spiritual services status retrieved",
Â            data={
Â                "spiritual_avatar_engine": avatar_status,
Â                "monetization_optimizer": monetization_status,
Â                "recent_metrics": {
Â                    "sessions_24h": recent_sessions,
Â                    "successful_validations_24h": successful_validations
Â                },
Â                "last_updated": datetime.now(timezone.utc).isoformat()
Â            }
Â        )
Â    except Exception as e:
Â        logger.error(f"Failed to get spiritual services status: {e}")
Â        return StandardResponse(
Â            status="error",
Â            message=f"Failed to get spiritual services status: {str(e)}",
Â            data={}
Â        )

@router.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
Â    """WebSocket endpoint for real-time monitoring updates"""
Â    await connection_manager.connect(websocket)
Â    try:
Â        while True:
Â            # Check if connection is still active
Â            if websocket.client_state.name != "CONNECTED":
Â                logger.info("WebSocket disconnected, ending monitoring")
Â                break
Â 
Â            try:
Â                # Send heartbeat and system status every 5 seconds
Â                system_health = await integration_monitor.get_system_health()
Â                await websocket.send_json({
Â                    "type": "system_health",
Â                    "data": system_health,
Â                    "timestamp": datetime.now(timezone.utc).isoformat()
Â                })
Â            except Exception as send_error:
Â                # Don't log error if it's just a disconnection
Â                if "1005" not in str(send_error) and "no status received" not in str(send_error):
Â                    logger.error(f"Error sending WebSocket message: {send_error}")
Â                break
Â 
Â            # Wait for 5 seconds
Â            try:
Â                await asyncio.sleep(5)
Â            except Exception as sleep_error:
Â                logger.error(f"Error during WebSocket sleep: {sleep_error}")
Â                break
Â 
Â    except WebSocketDisconnect:
Â        logger.info("WebSocket client disconnected")
Â    except Exception as e:
Â        logger.error(f"Unexpected WebSocket error: {e}")
Â    finally:
Â        connection_manager.disconnect(websocket)

@router.get("/social-media-status")
async def get_social_media_status():
Â    """Get social media automation status and campaign metrics (public endpoint)"""
Â    try:
Â        from social_media_marketing_automation import SocialMediaMarketingEngine
Â        from validators.social_media_validator import SocialMediaValidator
Â 
Â        # Test SocialMediaMarketingEngine availability
Â        social_engine_status = {"available": False, "error": None}
Â        try:
Â            SocialMediaMarketingEngine()
Â            social_engine_status["available"] = True
Â        except Exception as e:
Â            social_engine_status["error"] = str(e)
Â 
Â        # Test SocialMediaValidator availability
Â        validator_status = {"available": False, "error": None}
Â        try:
Â            SocialMediaValidator()
Â            validator_status["available"] = True
Â        except Exception as e:
Â            validator_status["error"] = str(e)
Â 
Â        # Get recent social media metrics
Â        try:
Â            conn = await db_manager.get_connection()
Â            try:
Â                # Count recent campaigns
Â                recent_campaigns = await conn.fetchval("""
Â                    SELECT COUNT(*) FROM social_campaigns
Â                    WHERE created_at >= NOW() - INTERVAL '7 days'
Â                """)
Â 
Â                # Count recent posts
Â                recent_posts = await conn.fetchval("""
Â                    SELECT COUNT(*) FROM social_posts
Â                    WHERE created_at >= NOW() - INTERVAL '24 hours'
Â                """)
Â 
Â                # Count social media validation logs
Â                recent_validations = await conn.fetchval("""
Â                    SELECT COUNT(*) FROM social_media_validation_log
Â                    WHERE created_at >= NOW() - INTERVAL '24 hours'
Â                """)
Â 
Â                # Get active campaigns
Â                active_campaigns = await conn.fetchval("""
Â                    SELECT COUNT(*) FROM social_campaigns
Â                    WHERE status = 'active'
Â                """)
Â 
Â            finally:
Â                await db_manager.release_connection(conn)
Â        except Exception:
Â            recent_campaigns = 0
Â            recent_posts = 0
Â            recent_validations = 0
Â            active_campaigns = 0
Â 
Â        return StandardResponse(
Â            status="success",
Â            message="Social media status retrieved",
Â            data={
Â                "social_media_engine": social_engine_status,
Â                "social_media_validator": validator_status,
Â                "metrics": {
Â                    "campaigns_7d": recent_campaigns,
Â                    "posts_24h": recent_posts,
Â                    "validations_24h": recent_validations,
Â                    "active_campaigns": active_campaigns
Â                },
Â                "automation_health": {
Â                    "engine_operational": social_engine_status["available"],
Â                    "validator_operational": validator_status["available"],
Â                    "overall_status": "healthy" if social_engine_status["available"] and validator_status["available"] else "degraded"
Â                },
Â                "last_updated": datetime.now(timezone.utc).isoformat()
Â            }
Â        )
Â    except Exception as e:
Â        logger.error(f"Failed to get social media status: {e}")
Â        return StandardResponse(
Â            status="error",
Â            message=f"Failed to get social media status: {str(e)}",
Â            data={}
Â        )

@router.get("/social-media-campaigns")
async def get_social_media_campaigns(admin: dict = Depends(get_current_admin_dependency)):
Â    """Get social media campaign performance and analytics"""
Â    try:
Â        conn = await db_manager.get_connection()
Â        try:
Â            # Get recent campaigns with performance data
Â            campaigns = await conn.fetch("""
Â                SELECT
Â                    c.id,
Â                    c.name,
Â                    c.platform,
Â                    c.status,
Â                    c.budget,
Â                    c.created_at,
Â                    c.updated_at,
Â                    COUNT(p.id) as total_posts,
Â                    AVG(CAST(p.engagement_metrics->>'likes' AS INTEGER)) as avg_likes,
Â                    AVG(CAST(p.engagement_metrics->>'comments' AS INTEGER)) as avg_comments,
Â                    SUM(CAST(p.engagement_metrics->>'reach' AS INTEGER)) as total_reach
Â                FROM social_campaigns c
Â                LEFT JOIN social_posts p ON c.id = p.campaign_id
Â                WHERE c.created_at >= NOW() - INTERVAL '30 days'
Â                GROUP BY c.id, c.name, c.platform, c.status, c.budget, c.created_at, c.updated_at
Â                ORDER BY c.created_at DESC
Â                LIMIT 20
Â            """)
Â 
Â            campaign_data = [dict(campaign) for campaign in campaigns]
Â 
Â            # Calculate summary metrics
Â            total_campaigns = len(campaign_data)
Â            active_campaigns = sum(1 for c in campaign_data if c['status'] == 'active')
Â            total_reach = sum(c['total_reach'] or 0 for c in campaign_data)
Â 
Â            return StandardResponse(
Â                status="success",
Â                message="Social media campaigns retrieved",
Â                data={
Â                    "campaigns": campaign_data,
Â                    "summary": {
Â                        "total_campaigns": total_campaigns,
Â                        "active_campaigns": active_campaigns,
Â                        "total_reach": total_reach,
Â                        "avg_engagement": sum(c['avg_likes'] or 0 for c in campaign_data) / max(total_campaigns, 1)
Â                    }
Â                }
Â            )
Â        finally:
Â            await db_manager.release_connection(conn)
Â    except Exception as e:
Â        logger.error(f"Failed to get social media campaigns: {e}")
Â        return StandardResponse(
Â            status="error",
Â            message=f"Failed to get campaigns: {str(e)}",
Â            data={"campaigns": [], "summary": {}}
Â        )

@router.post("/social-media-test")
async def test_social_media_automation(admin: dict = Depends(get_current_admin_dependency)):
Â    """Test social media automation functionality"""
Â    try:
Â        from social_media_marketing_automation import SocialMediaMarketingEngine
Â        from validators.social_media_validator import SocialMediaValidator
Â 
Â        test_results = {}
Â 
Â        # Test SocialMediaMarketingEngine
Â        try:
Â            engine = SocialMediaMarketingEngine()
Â 
Â            # Test content generation
Â            test_content = await engine.generate_content_plan(
Â                platform="instagram",
Â                content_type="daily_wisdom",
Â                target_audience={"age": "25-45", "interests": ["spirituality"]}
Â            )
Â 
Â            test_results["marketing_engine"] = {
Â                "status": "passed",
Â                "content_generated": test_content is not None,
Â                "has_platform_configs": bool(engine.platform_configs)
Â            }
Â        except Exception as e:
Â            test_results["marketing_engine"] = {
Â                "status": "failed",
Â                "error": str(e)
Â            }
Â 
Â        # Test SocialMediaValidator
Â        try:
Â            validator = SocialMediaValidator()
Â 
Â            validation_result = await validator.validate(
Â                {"platform": "instagram", "content": "Test content"},
Â                {"status": "posted", "post_id": "test123"},
Â                {}
Â            )
Â 
Â            test_results["validator"] = {
Â                "status": "passed",
Â                "validation_working": validation_result is not None
Â            }
Â        except Exception as e:
Â            test_results["validator"] = {
Â                "status": "failed",
Â                "error": str(e)
Â            }
Â 
Â        # Overall test status
Â        overall_status = "passed" if all(
Â            result.get("status") == "passed"
Â            for result in test_results.values()
Â        ) else "failed"
Â 
Â        return StandardResponse(
Â            status="success",
Â            message="Social media automation test completed",
Â            data={
Â                "overall_status": overall_status,
Â                "test_results": test_results,
Â                "timestamp": datetime.now(timezone.utc).isoformat()
Â            }
Â        )
Â    except Exception as e:
Â        logger.error(f"Social media automation test failed: {e}")
Â        return StandardResponse(
Â            status="error",
Â            message=f"Test failed: {str(e)}",
Â            data={}
Â        )

@router.get("/live-audio-video-status")
async def get_live_audio_video_status():
Â    """Get comprehensive live audio/video system status - BUSINESS CRITICAL"""
Â    try:
Â        # Check Agora service availability
Â        agora_status = {"available": False, "error": None}
Â        try:
Â            from agora_service import AgoraService
Â            agora_service = AgoraService()
Â            agora_status = {
Â                "available": True,
Â                "app_id_configured": bool(getattr(agora_service, 'app_id', None)),
Â                "certificate_configured": bool(getattr(agora_service, 'app_certificate', None))
Â            }
Â        except Exception as agora_error:
Â            agora_status = {"available": False, "error": str(agora_error)}
Â 
Â        # Check LiveChat router availability
Â        livechat_router_status = {"available": False, "error": None}
Â        try:
Â            from routers.livechat import livechat_router
Â            livechat_router_status = {"available": True, "endpoints_count": len(livechat_router.routes)}
Â        except Exception as router_error:
Â            livechat_router_status = {"available": False, "error": str(router_error)}
Â 
Â        # Check database tables
Â        database_status = {"available": False, "tables": {}}
Â        try:
Â            conn = await db_manager.get_connection()
Â 
Â            tables_to_check = ['live_chat_sessions', 'sessions', 'session_participants', 'user_sessions']
Â            for table in tables_to_check:
Â                table_exists = await conn.fetchrow('''
Â                    SELECT table_name FROM information_schema.tables
Â                    WHERE table_name = $1 AND table_schema = 'public'
Â                ''', table)
Â                database_status["tables"][table] = bool(table_exists)
Â 
Â            database_status["available"] = any(database_status["tables"].values())
Â            await db_manager.release_connection(conn)
Â 
Â        except Exception as db_error:
Â            database_status = {"available": False, "error": str(db_error)}
Â 
Â        # Get recent session metrics
Â        session_metrics = {
Â            "active_sessions": 0,
Â            "sessions_24h": 0,
Â            "total_revenue_24h": 0.0,
Â            "avg_session_duration": 0.0
Â        }
Â 
Â        try:
Â            conn = await db_manager.get_connection()
Â 
Â            # Active sessions
Â            if database_status["tables"].get("live_chat_sessions"):
Â                active_count = await conn.fetchval(
Â                    "SELECT COUNT(*) FROM live_chat_sessions WHERE status = 'active'"
Â                )
Â                session_metrics["active_sessions"] = active_count or 0
Â 
Â            # Sessions in last 24 hours - use sessions table with correct column names
Â            sessions_24h = await conn.fetchval('''
Â                SELECT COUNT(*) FROM sessions
Â                WHERE created_at >= NOW() - INTERVAL '24 hours'
Â                AND (service_type LIKE '%video%' OR service_type LIKE '%chat%')
Â            ''')
Â            session_metrics["sessions_24h"] = sessions_24h or 0
Â 
Â            # Revenue in last 24 hours - calculate from credits_used
Â            revenue_24h = await conn.fetchval('''
Â                SELECT COALESCE(SUM(credits_used * 0.1), 0) FROM sessions
Â                WHERE created_at >= NOW() - INTERVAL '24 hours'
Â                AND (service_type LIKE '%video%' OR service_type LIKE '%chat%')
Â            ''')
Â            session_metrics["total_revenue_24h"] = float(revenue_24h or 0.0)
Â 
Â            await db_manager.release_connection(conn)
Â 
Â        except Exception as metrics_error:
Â            print(f"Error fetching session metrics: {metrics_error}")
Â 
Â        # Check frontend components availability
Â        frontend_status = {"available": True, "components": {}}
Â        frontend_components = ['LiveChat.jsx', 'InteractiveAudioChat.jsx', 'AgoraVideoCall.jsx']
Â 
Â        for component in frontend_components:
Â            try:
Â                import os
Â                component_path = os.path.join('frontend', 'src', 'components', component)
Â                frontend_status["components"][component] = os.path.exists(component_path)
Â            except Exception:
Â                frontend_status["components"][component] = False
Â 
Â        frontend_status["available"] = any(frontend_status["components"].values())
Â 
Â        # Calculate overall system health
Â        critical_systems = [agora_status["available"], livechat_router_status["available"], database_status["available"]]
Â        system_health_score = (sum(critical_systems) / len(critical_systems)) * 100
Â 
Â        if system_health_score >= 90:
Â            overall_status = "healthy"
Â        elif system_health_score >= 70:
Â            overall_status = "degraded"
Â        else:
Â            overall_status = "critical"
Â 
Â        return {
Â            "success": True,
Â            "data": {
Â                "overall_status": overall_status,
Â                "system_health_score": system_health_score,
Â                "agora_service": agora_status,
Â                "livechat_router": livechat_router_status,
Â                "database": database_status,
Â                "frontend_components": frontend_status,
Â                "session_metrics": session_metrics,
Â                "timestamp": datetime.now(timezone.utc).isoformat()
Â            }
Â        }
Â 
Â    except Exception as e:
Â        return {
Â            "success": False,
Â            "error": f"Failed to get live audio/video status: {str(e)}",
Â            "data": {
Â                "overall_status": "critical",
Â                "system_health_score": 0,
Â                "timestamp": datetime.now(timezone.utc).isoformat()
Â            }
Â        }


def _extract_agora_channel_from_session_data(session_data):
Â    """
Â    Parse session_data JSON string to extract agora_channel field.
Â    Returns the agora_channel string or None if parsing fails or field is missing.
Â 
Â    Args:
Â        session_data: JSON string or dict containing session information
Â 
Â    Returns:
Â        str or None: The agora channel identifier if found, otherwise None
Â    """
Â    if not session_data:
Â        return None
Â 
Â    try:
Â        # Handle both string and dict inputs
Â        if isinstance(session_data, str):
Â            import json
Â            parsed_data = json.loads(session_data)
Â        elif isinstance(session_data, dict):
Â            parsed_data = session_data
Â        else:
Â            return None
Â 
Â        # Try different possible field names for agora channel
Â        possible_fields = ['agora_channel', 'channel_name', 'channel', 'agora_channel_name']
Â        for field in possible_fields:
Â            if field in parsed_data and isinstance(parsed_data[field], str):
Â                return parsed_data[field]
Â 
Â        return None
Â 
Â    except (json.JSONDecodeError, TypeError, AttributeError, KeyError) as e:
Â        # Log the error for debugging but don't break the application
Â        logger.debug(f"Failed to parse session_data for agora_channel: {e}")
Â        return None

@router.get("/live-audio-video-sessions")
async def get_live_audio_video_sessions():
Â    """Get recent live audio/video sessions data"""
Â    try:
Â        conn = await db_manager.get_connection()
Â 
Â        # Get recent sessions
Â        sessions = []
Â        try:
Â            sessions_data = await conn.fetch('''
Â                SELECT s.id as session_id, s.user_id, s.service_type, s.status,
Â                       s.created_at as start_time, s.updated_at as end_time,
Â                       s.duration_minutes, s.credits_used, s.session_data,
Â                       lcs.channel_name as agora_channel
Â                FROM sessions s
Â                LEFT JOIN live_chat_sessions lcs ON s.id::text = lcs.session_id
Â                WHERE s.created_at >= NOW() - INTERVAL '7 days'
Â                AND (s.service_type LIKE '%video%' OR s.service_type LIKE '%chat%')
Â                ORDER BY s.created_at DESC
Â                LIMIT 20
Â            ''')
Â 
Â            for session in sessions_data:
Â                sessions.append({
Â                    "session_id": str(session['session_id']),
Â                    "user_id": session['user_id'],
Â                    "service_type": session['service_type'],
Â                    "status": session['status'],
Â                    "start_time": session['start_time'].isoformat() if session['start_time'] else None,
Â                    "end_time": session['end_time'].isoformat() if session['end_time'] else None,
Â                    "duration": session['duration_minutes'],
Â                    "cost": float(session['credits_used'] * 0.1) if session['credits_used'] else 0.0,
Â                    "agora_channel": session.get('agora_channel') or _extract_agora_channel_from_session_data(session.get('session_data'))
Â                })
Â        except Exception as sessions_error:
Â            print(f"Error fetching sessions: {sessions_error}")
Â 
Â        await db_manager.release_connection(conn)
Â 
Â        return {
Â            "success": True,
Â            "data": {
Â                "recent_sessions": sessions,
Â                "total_sessions": len(sessions)
Â            }
Â        }
Â 
Â    except Exception as e:
Â        return {
Â            "success": False,
Â            "error": f"Failed to get live audio/video sessions: {str(e)}",
Â            "data": {"recent_sessions": [], "total_sessions": 0}
Â        }

@router.post("/live-audio-video-test")
async def test_live_audio_video_system():
Â    """Test live audio/video system functionality"""
Â    try:
Â        test_results = {}
Â 
Â        # Test 1: Agora service
Â        try:
Â            from agora_service import AgoraService
Â            agora_service = AgoraService()
Â 
Â            # Test token generation
Â            test_channel = f"test_channel_{uuid.uuid4()}"
Â            token_result = await agora_service.generate_token(
Â                channel_name=test_channel,
Â                uid=str(uuid.uuid4()),
Â                role="publisher"
Â            )
Â 
Â            test_results["agora_service"] = {
Â                "status": "passed" if token_result and token_result.get("token") else "failed",
Â                "token_generated": bool(token_result.get("token") if token_result else False),
Â                "test_details": "Token generation test"
Â            }
Â 
Â        except Exception as agora_error:
Â            test_results["agora_service"] = {
Â                "status": "failed",
Â                "error": str(agora_error),
Â                "test_details": "Agora service initialization test"
Â            }
Â 
Â        # Test 2: Database operations
Â        try:
Â            conn = await db_manager.get_connection()
Â 
Â            # Test session creation
Â            session_id = str(uuid.uuid4())
Â            await conn.execute('''
Â                INSERT INTO live_chat_sessions (session_id, user_id, agora_token, status, created_at)
Â                VALUES ($1, $2, $3, $4, $5)
Â            ''', session_id, "test_user", "test_token", "active", datetime.now(timezone.utc))
Â 
Â            # Verify and cleanup
Â            session = await conn.fetchrow(
Â                "SELECT status FROM live_chat_sessions WHERE session_id = $1", session_id
Â            )
Â            await conn.execute("DELETE FROM live_chat_sessions WHERE session_id = $1", session_id)
Â            await db_manager.release_connection(conn)
Â 
Â            test_results["database"] = {
Â                "status": "passed" if session else "failed",
Â                "session_created": bool(session),
Â                "test_details": "Session creation and cleanup test"
Â            }
Â 
Â        except Exception as db_error:
Â            test_results["database"] = {
Â                "status": "failed",
Â                "error": str(db_error),
Â                "test_details": "Database operations test"
Â            }
Â 
Â        # Test 3: API endpoints
Â        try:
Â            # Test internal endpoint availability
Â            livechat_available = True
Â            try:
Â                from routers.livechat import livechat_router
Â            except ImportError:
Â                livechat_available = False
Â 
Â            test_results["api_endpoints"] = {
Â                "status": "passed" if livechat_available else "failed",
Â                "livechat_router_available": livechat_available,
Â                "test_details": "API router availability test"
Â            }
Â 
Â        except Exception as api_error:
Â            test_results["api_endpoints"] = {
Â                "status": "failed",
Â                "error": str(api_error),
Â                "test_details": "API endpoints test"
Â            }
Â 
Â        # Calculate overall test result
Â        passed_tests = sum(1 for result in test_results.values() if result.get("status") == "passed")
Â        total_tests = len(test_results)
Â        test_success_rate = (passed_tests / total_tests) * 100 if total_tests > 0 else 0
Â 
Â        overall_status = "passed" if test_success_rate >= 70 else "failed"
Â 
Â        return StandardResponse(
Â            status="success",
Â            message="Live audio/video system test completed",
Â            data={
Â                "overall_status": overall_status,
Â                "test_success_rate": test_success_rate,
Â                "passed_tests": passed_tests,
Â                "total_tests": total_tests,
Â                "test_results": test_results,
Â                "timestamp": datetime.now(timezone.utc).isoformat()
Â            }
Â        )
Â 
Â    except Exception as e:
Â        return StandardResponse(
Â            status="error",
Â            message=f"Failed to test live audio/video system: {str(e)}",
Â            data={
Â                "overall_status": "failed",
Â                "test_success_rate": 0,
Â                "timestamp": datetime.now(timezone.utc).isoformat()
Â            }
Â        )

# Export for use in other modules
__all__ = ["monitoring_dashboard", "router", "connection_manager"]